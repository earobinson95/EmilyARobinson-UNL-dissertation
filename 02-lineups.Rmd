# Perception through lineups {#lineups}

## Introduction

Previously, we saw how a data plot can be evaluated and treated as a visual statistic, a numerical function which summarizes the data.
To evaluate a graph, we have to run our statistic through a visual evaluation - a person. 
If two different methods of presenting data result in qualitatively different results when evaluated visually, then we can conclude that the visual statistics are significantly different. 
Recent graphical experiments have utilized statistical lineups to quantify the perception of graphical design choices 
[@hofmann_graphical_2012; @loy_model_2017; @loy_variations_2016; @vanderplas_clusters_2017]. 
Statistical lineups provide an elegant way of combining perception and statistical hypothesis testing using graphical experiments [@majumder_validation_2013; @vanderplas_testing_2020; @wickham_graphical_2010].
'Lineups' are named after the 'police lineup' of criminal investigations where witnesses are asked to identify the criminal from a set of individuals. 
Similarly, a statistical lineup is a plot consisting of smaller panels; the viewer is asked to identify the plot of the real data from among a set of decoy null plots. 
A statistical lineup typically consists of 20 panels - 1 target panel and 19 null panels. Figure 
If the viewer can identify the target panel embedded within the set of null panels, this suggests that the real data is visually distinct from data generated under the null model. 
\cref{fig:lineup-example} provides example's of a statistical lineup's. The lineup plot on the left displays increasing exponential data on a linear scale with panel (2 x 5) + 3 as the target.
The lineup plot on the right displays increasing exponential data on the log scale with panel 2 x 2 as the
target.
Crowd sourcing websites such as Amazon Mechanical Turk, Reddit, and Prolifc allow us to collect responses from multiple viewers. 

```{r lineup-example, fig.height = 2.75, fig.width = 5.75, fig.cap = "Lineup example"}
lineupData_linear <- read.csv(file = "data/lineup-example-data-linear.csv")
linearPlot <- ggplot(lineupData_linear, aes(x=x, y=y)) +
  facet_wrap(~.sample, ncol=5) +
  geom_point(size = .05) +
  theme(aspect.ratio = 1) +
  theme_bw(base_size = 14) +
  theme(axis.title.y = element_blank(),
        axis.title.x = element_blank(),
        axis.text.y  = element_blank(),
        axis.text.x  = element_blank(),
        strip.text = element_text(size = 5, margin = margin(0.05,0,0.05,0, "cm")),
        strip.background = element_rect(size = 0.5)
  )

lineupData_log <- read.csv(file = "data/lineup-example-data-log.csv")
logPlot <- ggplot(lineupData_log, aes(x=x, y=y)) +
  facet_wrap(~.sample, ncol=5) +
  geom_point(size = .05) +
  theme(aspect.ratio = 1) +
  theme_bw(base_size = 14) +
  theme(axis.title.y = element_blank(),
        axis.title.x = element_blank(),
        axis.text.y  = element_blank(),
        axis.text.x  = element_blank(),
        strip.text = element_text(size = 5, margin = margin(0.05,0,0.05,0, "cm")),
        strip.background = element_rect(size = 0.5)
  ) +
  scale_y_continuous(trans = "log10")

grid.arrange(linearPlot, logPlot, ncol = 2)
```

While explicit graphical tests direct the participant to a specific feature of a plot to answer a specific question, implicit graphical tests require the user to identify both the purpose and function of the plot in order to evaluate the plots shown [@vanderplas_testing_2020]. 
Implicit graphical tests, such as lineups, have the advantage of simultaneously visually testing for multiple visual features including outliers, clusters, linear and nonlinear relationships. 
To lay a foundation for future exploration of the use of log scales, we begin with the most fundamental ability to identify differences in charts; this does not require that participants understand exponential growth, identify log scales, or have any mathematical training. 
Instead, we are simply testing the change in perceptual sensitivity resulting from visualization choices.
The study in this chapter is conducted through visual inference and the use of statistical lineups to differentiate between exponentially increasing curves with differing levels of curvature, using linear and log scales.

## Data Generation

In this study, both the target and null data sets were generated by simulating data from an exponential model; the models differ in the parameters selected for the null and target panels. 
In order to guarantee the simulated data spans the same domain and range of values, we implemented a domain constraint of $x\in [0,20]$ and a range constraint of $y\in [10,100]$ with $N = 50$ points randomly assigned throughout the domain and mapped to the y-axis using the exponential model with the selected parameters. 
These constraints provide some assurance that participants who select the target plot are doing so because of their visual perception differentiating between curvature or growth rate rather than different starting or ending values.

Data was simulated based on a three-parameter exponential model with multiplicative errors: 
\begin{align}
y_i & = \alpha\cdot e^{\beta\cdot x_i + \epsilon_i} + \theta \\
\text{with } \epsilon_i & \sim N(0, \sigma^2). \nonumber
\end{align} 
The parameters $\alpha$ and $\theta$ are adjusted based on $\beta$ and $\sigma^2$ to guarantee the range and domain constraints are met. 
The model generated $N = 50$ points $(x_i, y_i), i = 1,...,N$ where $x$ and $y$ have an increasing exponential relationship. 
The heuristic data generation procedure is described below:

\noindent \textit{Algorithm 2.1.1: Paremeter Estimation}

\noindent \textbf{Input Parameters:} domain $x\in[0,20]$, range $y\in[10,100]$, midpoint $x_{mid}$.

\noindent \textbf{Output:} estimated model parameters $\hat\alpha, \hat\beta, \hat\theta$

1. Determine the $y=-x$ line scaled to fit the assigned domain and range.

2. Map the values $x_{mid} - 0.1$ and $x_{mid} + 0.1$ to the $y=-x$ line for two additional points.

3. From the set points $(x_k, y_k)$ for $k = 1,2,3,4$, obtain the coefficients from the linear model $\ln(y_k) = b_0 +b_1x_k$ to obtain starting values - $\alpha_0 = e^{b_0}, \beta_0 =  b_1, \theta_0 = 0.5\cdot \min(y)$

4. Using the `nls()` function from the `stats` package in Rstudio and the starting parameter values - $\alpha_0, \beta_0, \theta_0$ - fit the nonlinear model, $y_k = \alpha\cdot e^{\beta\cdot x_k}+\theta$ to obtain estimated parameter values - $\hat\alpha, \hat\beta, \hat\theta.$

\noindent\textit{Algorithm 2.1.2: Exponential Simulation}

\noindent \textbf{Input Paremeters:} sample size $N = 50$, estimated parameters $\hat\alpha$, $\hat\beta$, and $\hat\theta$, $\sigma$ standard deviation from the exponential curve.

\noindent \textbf{Output Parameters:} $N$ points, in the form of vectors $\mathbf{x}$ and $\mathbf{y}$.

1. Generate $\tilde x_j, j = 1,..., N\cdot \frac{3}{4}$ as a sequence of evenly spaced points in $[0,20]$. This ensures the full domain of $x$ is used, fulfilling the constraints of spanning the same domain and range for each parameter combination.

2. Obtain $\tilde x_i, i = 1,...N$ by sampling $N = 50$ values from the set of $\tilde x_j$ values. This gaurantees some variability and potential clustring in the exponential growth curve disrupting the perception due to continuity of points.

3. Obtain the final $x_i$ values by jittering $\tilde x_i$.

4. Calculate $\tilde\alpha = \frac{\hat\alpha}{e^{\sigma^2/2}}.$ This ensures that the range of simulated values for different standard devaition parameters has an equal expected value for a given rate of change due to the non-constant variance across the domain.

5. Generate $y_i = \tilde\alpha\cdot e^{\hat\beta x_i + e_i}+\hat\theta$ where $e_i\sim N(0,\sigma^2).$

## Parameter Selection

For each level of difficulty, we simulated 1000 data sets of $(x_{ij}, y_{ij})$ points for $i = 1,...,50$ and $j = 1...10$.
Each generated $x_i$ point from \textit{Algorithm 2.1.2} was replicated 10 times. 
Then the lack of fit statistic (LOF) was computed for each simulated data set by calculating the deviation of the data from a linear line.
Plotting the density curves of the LOF statistics for each level of difficulty choice allows us to evaluate the ability of differentiating between the difficulty levels and thus detecting the target plot.
In \cref{fig:lof-density-curves}, we can see the densities of each of the three difficulty levels.
While the LOF statistic provides us a numerical value for discriminating between the difficulty levels, we cannot directly relate this to the perceptual discriminability; it serves primarily as an approximation to ensure that we are testing parameters at several distinct levels of difficulty. 
Final parameter estimates are shown in \cref{tab:parameter-data}.

```{r lof-density-curves, fig.height = 2.5, fig.width = 5, fig.cap = "Lack of fit statistic density curves"}
lofData <- read.csv(file = "data/lineup-lof-data.csv")
lofPlot_curvature <- lofData %>%
  mutate(Curvature = factor(Curvature, levels = c("Obvious Curvature", "Noticeable Curvature", "Almost Linear"), labels = c("Easy (Lots of curvature)", "Medium (Noticable curvature)", "Hard (Almost Linear)"))) %>%
  mutate(Variability = factor(Variability, levels = c("Low"))) %>%
  ggplot(aes(x = statistic, fill = Curvature, color = Curvature)) +
  geom_density(alpha = 0.7) +
  scale_fill_manual("Curvature Difficulty", values = c("#004400", "#116611", "#55aa55")) +
  scale_color_manual("Curvature Difficulty", values = c("#004400", "#116611", "#55aa55")) +
  theme_bw(base_size = 14) +
  theme(legend.position = "bottom",
        axis.text    = element_text(size = 6),
        axis.title   = element_text(size = 8),
        legend.title = element_text(size = 8),
        legend.text  = element_text(size = 6),
        legend.key.size = unit(0.5, "line")
        ) +
  scale_x_continuous("Lack of Fit Statistic")
lofPlot_curvature
```

```{r parameter-data}
parameter_data <- read.csv(file = "data/lineup-parameter-data.csv")
parameter_data %>%
  mutate(difficulty = ifelse(difficulty == "Obvious Curvature", "Easy",
                             ifelse(difficulty == "Noticable Curvature", "Medium", "Hard"))
         ) %>%
  select(difficulty, xMid, alphahat, alphatilde, betahat, thetahat, sigma_vals) %>%
  knitr::kable("latex", digits = 2, escape = F, booktabs = T, linesep = "", align = "c", label = "parameter-data",
        col.names = c("",   "$x_{mid}$", "$\\hat\\alpha$", "$\\tilde\\alpha$", "$\\hat\\beta$", "$\\hat\\theta$", "$\\hat\\sigma$"),
        caption = "Lineup data simulation final parameters")
```

## Lineup Setup 

Lineup plots were generated by mapping one simulated data set corresponding to difficulty level A to a scatter plot to be identified as the target plot while multiple simulated data sets corresponding to difficulty level B were individually mapped to scatter plots for the null plots. 
For example, a target plot with simulated data following an increasing exponential curve with obvious curvature is embedded within null plots with simulated data following an increasing exponential trend that is almost linear (i.e. Hard Null - Easy Target). 
By our constraints, the target plot and null plots will span a similar domain and range. 
There are a total of six (i.e. $3!\cdot 2!$) lineup parameter combinations.
Two sets of each lineup parameter combination were simulated (total of 12 test data sets) and plotted on both the linear scale and the log scale (total of 24 test lineup plots).
In addition, there are three parameter combinations which generate homogeneous "Rorschach" lineups, where all panels are from the same distribution. Each participant evaluated one of these lineups, but for simplicity, these evaluations are not described in this paper.

## Study Design

Each participant was shown a total of thirteen lineup plots (twelve test lineup plots and one Rorschach lineup plot). Participants were randomly assigned one of the two replicate data sets for each of the six unique lineup parameter combinations. 
For each assigned test data set, the participant was shown the lineup plot corresponding to both the linear scale and the log scale. For the additional Rorschach lineup plot, participants were randomly assigned one data set shown on either the linear or the log scale. 
The order of the thirteen lineup plots shown was randomized for each participant. 

Participants above the age of majority were recruited from Reddit's Visualization and Sample Size communities.
Since participants recruited on Reddit were not compensated for their time, most participants have an interest in data visualization research. 
Previous literature suggests that prior mathematical knowledge or experience with exponential data is not associated with the outcome of graphical experiments [@vanderplas2015spatial]. 
Participants completed the experiment using a Shiny applet (https://shiny.srvanderplas.com/log-study/).

Participants were shown a series of lineup plots and asked to identify the plot that was most different from the others. 
On each plot, participants were asked to justify their choice and provide their level of confidence in their choice.
The goal of this experimental task is to test an individuals ability to perceptually differentiate exponentially increasing trends with differing levels of curvature on both the linear and log scale. 

## Results

Participant recruitment through Reddit occurred over the course of two weeks during which 58 individuals completed 518 unique test lineup evaluations. 
Previous studies have found that results do not differ on lineup-related tasks between Reddit and e.g. Amazon Mechanical Turk [@vanderplas_clusters_2017].
Participants who completed fewer than 6 lineup evaluations were removed from the study (17 participants, 41 evaluations).
The final data set included a total of 41 participants and 477 lineup evaluations. 
Each plot was evaluated by between 18 and 28 individuals (Mean: 21.77, SD: 2.29). 
In 67\% of the 477 lineup evaluations, participants correctly identified the target panel. 

Target plot identification was analyzed using the lme4 R package and glmer function [@lme4]. Estimates and odds ratio comparisons were calculated using the emmeans R package [@emmeans].
Each lineup plot evaluated was assigned a value based on the participant response (correct = 1, not correct = 0).
Define $Y_{ijkl}$ to be the event that participant $l$ correctly identifies the target plot for data set $k$ with curvature $j$ plotted on scale $i$.
The binary response was analyzed using generalized linear mixed model following a binomial distribution with a logit link function following a row-column blocking design accounting for the variation due to participant and data set respectively as 
\begin{equation}
\text{logit }P(Y_{ijk}) = \eta + \delta_i + \gamma_j + \delta \gamma_{ij} + s_l + d_k
\end{equation}
where
\begin{itemize}
\item $\eta$ is the baseline average probability of selecting the target plot
\item $\delta_i$ is the effect of the log/linear scale
\item $\gamma_j$ is the effect of the curvature combination
\item $\delta\gamma_{ij}$ is the two-way interaction effect of the scale and curvature
\item $s_l \sim N(0,\sigma^2_\text{participant})$, random effect for participant characteristics
\item $d_k \sim N(0,\sigma^2_{\text{data}})$, random effect for data specific characteristics. 
\end{itemize}
\noindent We assume that random effects for data set and participant are independent.

The analysis of variance table shown in \cref{tab:lineup-anova-table} indicate a significant interaction between the curvature combination and scale. Variance due to participant and data set were estimated to be $\sigma^2_{\text{participant}} = 2.79$ (s.e. 1.67) and $\sigma^2_{\text{data}} = 0.44$ (s.e. 0.66) respectively.

```{r anova}
read.csv("data/lineup-anova-table.csv") %>%
  knitr::kable("latex", escape = F, booktabs = T, linesep = "", align = "c", 
        label = "lineup-anova-table",
        caption = "Lineup ANOVA table for fixed effects.")
```

On both the log and linear scales, the highest accuracy occurred in lineup plots where the target model and null model had large curvature differences (Easy Null - Hard Target; Hard Null - Easy Target).
<!-- When comparing models that have slight curvature differences \er{(e.g. Medium Null - Hard Target, Medium Null - Easy Target, Easy Null - Medium Target)}, there is a sacrifice in accuracy when displayed on the linear scale.  -->
There is a decrease in accuracy on the linear scale when comparing a target plot with less curvature to null plots with more curvature (Easy Null - Medium Target; Medium Null - Hard Target). 
@best_perception_2007 found that accuracy of identifying the correct curve type was higher when nonlinear trends were presented indicating that it is hard to say something is linear (i.e. something has less curvature), but easy to say that it is not linear; our results concur with this observation.
Overall, there are no significant differences in accuracy between curvature combinations when data is presented on a log scale indicating participants were consistent in their success of identifying the target panel on the log scale.
\cref{fig:odds-ratio-plot} displays the estimated (log) odds ratio of successfully identifying the target panel on the log scale compared to the linear scale. 
The choice of scale has no impact if curvature differences are large (Hard Null - Easy Target; Easy Null - Hard Target). 
However, presenting data on the log scale makes us more sensitive to slight changes in curvature (Medium Null - Easy Target; Medium Null - Hard Target; Easy Null - Medium Target). 
An exception occurs when identifying a plot with curvature embedded in null plots close to a linear trend (Hard Null - Medium Target), again supporting the claim that it is easy to identify a curve in a bunch of lines but much harder to identify a line in a bunch of curves [@best_perception_2007].

```{r odds-ratio-plot, echo = F, eval = T, fig.width = 8, fig.height = 5, fig.align='center', fig.cap = "Lineups log(odds) results", message = F, warning = F}
odds_ratios <- read.csv("data/lineup-odds-ratios.csv")
dodge <- position_dodge(width=0.9)
odds_ratio_plot <- odds_ratios %>%
  mutate(target = factor(target, levels = c("Easy", "Medium", "Hard")),
         null = factor(null, levels = c("Easy", "Medium", "Hard"))) %>%
  ggplot(aes(x = odds.ratio, y = null, color = target, shape = target)) + 
  geom_point(position = dodge, size = 3) + 
  geom_errorbar(aes(xmin = asymp.LCL, xmax = asymp.UCL), position = dodge, width = .1) +
  geom_vline(xintercept = 1) +
  theme_bw()  +
  theme(axis.title = element_text(size = 8),
        axis.text = element_text(size = 8),
        legend.title = element_text(size = 8),
        legend.text  = element_text(size = 8),
        legend.key.size = unit(0.7, "line"),
        legend.position = "bottom"
  ) +
  scale_y_discrete("Null plot type") +
  scale_x_continuous("Odds ratio (on log scale) \n (Log vs Linear)", trans = "log10") + 
  scale_color_manual("Target Plot Type", values = c("#004400", "#116611", "#55aa55")) + 
  scale_shape_discrete("Target Plot Type")

picsList <- c("images/tM_nH.png", "images/tE_nH.png", 
              "images/tH_nM.png", "images/tE_nM.png", 
              "images/tH_nE.png", "images/tM_nE.png"
)

library(cowplot)
pimage <- axis_canvas(odds_ratio_plot, axis = 'y') + 
  draw_image(picsList[1], y = 2.75, scale = 0.6) +
  draw_image(picsList[2], y = 2.25, scale = 0.6) +
  draw_image(picsList[3], y = 1.75, scale = 0.6) +
  draw_image(picsList[4], y = 1.25, scale = 0.6) +
  draw_image(picsList[5], y = 0.75, scale = 0.6) +
  draw_image(picsList[6], y = 0.25, scale = 0.6)

# insert the image strip into the plot
ggdraw(insert_yaxis_grob(odds_ratio_plot, pimage, position = "right", clip = "on"))
```

## Discussion and Conclusion

The overall goal of this chapter is to provide basic research to support the principles used to guide design decisions in scientific visualizations of exponential data. 
In this study, we explore the use of linear and log scales to determine whether our ability to notice differences in exponentially increasing trends is impacted by the choice of scale. 
Our results indicated that when there was a large difference in curvature between the target plot and null plots, the choice of scale had no impact and participants accurately differentiated between the two curves on both the linear and log scale. 
However, displaying exponentially increasing data on a log scale improved the accuracy of differentiating between models with slight curvature differences.
An exception occurred when identifying a plot with curvature embedded in surrounding plots closely relating to a linear trend, indicating that it is easy to identify a curve in a group of lines but much harder to identify a line in a group of curves.
The use of visual inference to identify these guidelines suggests that there are \emph{perceptual} advantages to log scales when differences are subtle. 
What remains to be seen is whether there are cognitive disadvantages to log scales: do log scales make it harder to make use of graphical information?