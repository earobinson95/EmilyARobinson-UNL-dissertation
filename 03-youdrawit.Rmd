# Prediction with you draw it {#youdrawit}

```{r eyefitting-plots}

eyefitting_example_sim <- read.csv("data/youdrawit/youdrawit-eyefitting-simdata-example.csv")
eyefitting_example_simplot <- eyefitting_example_sim %>%
  filter(data == "point_data") %>%
  filter(dataset %in% c("F", "N", "S") | (x < 16 & x > 4)) %>%
  rename(`Parameter Choice` = dataset) %>%
  ggplot(aes(x = x, y = y)) +
  geom_point(size = 0.5) +
  facet_grid(~`Parameter Choice`, labeller = labeller(`Parameter Choice` = label_both)) +
  theme_bw(base_size = 14) +
  theme(aspect.ratio = 1,
        legend.position = "none",
        plot.title   = element_text(size = 12, hjust = 0),
        axis.text    = element_text(size = 12),
        axis.title   = element_text(size = 12),
        legend.title = element_text(size = 12),
        legend.text  = element_text(size = 12),
        # strip.text = element_text(size = 5, margin = margin(0.05,0,0.05,0, "cm")),
        # strip.background = element_rect(size = 0.5),
        legend.key.size = unit(1, "line")
        ) 

eyefitting_model_data <- read.csv("data/youdrawit/youdrawit-eyefitting-model-data.csv")

eyefitting.preds.lmer <- read.csv("data/youdrawit/youdrawit-eyefitting-lmerpred-data.csv")
# Plot Predictions
eyefitting.lmer.plot <- eyefitting.preds.lmer %>%
  filter((parm_id %in% c("F", "N", "S") | (x <= 16 & x >= 4))) %>%
  rename(`Parameter Choice` = parm_id) %>%
  ggplot(aes(x = x)) +
  geom_line(data = eyefitting_model_data, aes(x = x, y = residualols, group = plotID, color = "OLS"), alpha = 0.05) +
  geom_line(data = eyefitting_model_data, aes(x = x, y = residualpca, group = plotID, color = "PCA"), alpha = 0.05) +
  geom_ribbon(aes(ymin = asymp.LCL.ols, ymax = asymp.UCL.ols, fill = "OLS"), color = NA, alpha = 0.7) +
  geom_line(aes(y = emmean.ols, color = "OLS")) +
  geom_ribbon(aes(ymin = asymp.LCL.pca, ymax = asymp.UCL.pca, fill = "PCA"), color = NA, alpha = 0.7) +
  geom_line(aes(y = emmean.pca, color = "PCA")) +
  geom_hline(yintercept = 0, linetype = "dashed", alpha = 0.5) +
  facet_grid(~`Parameter Choice`, labeller = labeller(`Parameter Choice` = label_both)) +
  theme_bw(base_size = 14) +
  theme(aspect.ratio = 1,
        legend.position = "right",
        plot.title   = element_text(size = 12, hjust = 0),
        axis.text    = element_text(size = 12),
        axis.title   = element_text(size = 12),
        legend.title = element_text(size = 12),
        legend.text  = element_text(size = 12),
        # strip.text = element_text(size = 5, margin = margin(0.05,0,0.05,0, "cm")),
        # strip.background = element_rect(size = 0.5),
        legend.key.size = unit(1, "line")
        ) +
  scale_y_continuous("Residual") +
  scale_color_manual("Estimates", values = c("steelblue", "orange")) +
  scale_fill_manual("Estimates", values = c("steelblue", "orange")) 

eyefitting.grid.gamm <- read.csv("data/youdrawit/youdrawit-eyefitting-gammpred-data.csv")
eyefitting.gamm.plot <- eyefitting.grid.gamm %>%
  filter((parm_id %in% c("F", "N", "S") | (x <= 16 & x >= 4))) %>%
  rename(`Parameter Choice` = parm_id) %>%
  ggplot(aes(x = x)) +
  geom_line(data = eyefitting_model_data, aes(x = x, y = residualols, group = plotID, color = "OLS"), alpha = 0.05) +
  geom_line(data = eyefitting_model_data, aes(x = x, y = residualpca, group = plotID, color = "PCA"), alpha = 0.05) +
  geom_ribbon(aes(ymin = ols.lower, ymax = ols.upper, fill = "OLS"), color = NA, alpha = 0.5) +
  geom_line(aes(y = ols.pred, color = "OLS")) +
  geom_ribbon(aes(ymin = pca.lower, ymax = pca.upper, fill = "PCA"), color = NA, alpha = 0.5) +
  geom_line(aes(y = pca.pred, color = "PCA")) +
  geom_hline(yintercept = 0, linetype = "dashed", alpha = 0.5) +
  facet_grid(~`Parameter Choice`, labeller = labeller(`Parameter Choice` = label_both)) +
  theme_bw(base_size = 14) +
  theme(aspect.ratio = 1,
        legend.position = "right",
        plot.title   = element_text(size = 12, hjust = 0),
        axis.text    = element_text(size = 12),
        axis.title   = element_text(size = 12),
        legend.title = element_text(size = 12),
        legend.text  = element_text(size = 12),
        # strip.text = element_text(size = 5, margin = margin(0.05,0,0.05,0, "cm")),
        # strip.background = element_rect(size = 0.5),
        legend.key.size = unit(1, "line")
        ) +
  scale_y_continuous("Residual") +
  scale_color_manual("Estimates", values = c("steelblue", "orange")) +
  scale_fill_manual("Estimates", values = c("steelblue", "orange"))
```

## Introduction

In [Chapter 2](#lineups), a base foundation for future exploration of the use of log scales was established by evaluating participants ability to identify differences in charts through the use of lineups. 
This did not require that participants were able to understand exponential growth, identify log scales, or have any mathematical training; instead, it simply tested the change in perceptual sensitivity resulting from visualization choices. 
In order to determine whether there are cognitive disadvantages to log scales, we utilize interactive graphics to test an individual's ability to make predictions for exponentially increasing data. In this study, participants are asked to draw a line using their computer mouse through the exponentially increasing trend shown on both the log and linear scale. 

<!-- + Early studies explored the estimation and prediction of exponential growth, finding that growth is underestimated when presented both numerically and graphically but that numerical estimation is more accurate than graphical estimation for exponential curves [@wagenaar_misperception_1975].  -->

### Past Methodology

Initial studies in the 20th century explored the use of fitting lines by eye through a set of points [@finney_subjective_1951; @mosteller_eye_1981]. 
Common methods of fitting trends by eye involve maneuvering a string, black thread, or ruler until the fit is suitable, then drawing the line. 
In @finney_subjective_1951, it was of interest to determine the effect of stopping iterative maximum likelihood calculations after one iteration. Twenty-one scientists were recruited via postal mail and asked to "rule two lines" in order to judge by eye the positions for a pair of parallel probit regression lines in a biological assay \cref{fig:subjective-judgement}.
Results indicated that one cycle of iterations was sufficient based on the starting values provided by eye from the participants.

```{r subjective-judgement, fig.cap = "Subjective Judgement in Statistical Analysis (1951) Parallel Probits", out.width="50%"}
knitr::include_graphics("images/subjective-judgement-plot.png")
```

Thirty years later, @mosteller_eye_1981, sought to understand the properties of least squares and other computed lines by establishing one systematic method of fitting lines by eye. 
The authors recruited 153 graduate students and post doctoral researchers in Introductory Biostatistics. 
Participants were asked to fit lines by eye to four sets of points \cref{fig:mosteller-eyefitting-plot} using an 8.5 x 11 inch transparency with a straight line etched completely across the middle. 
A latin square design with packets of the set of points stapled together in four different orders was used in order to determine if there is an effect of order of presentation.
It was found that order of presentation had no effect and that participants tended to fit the slope of the first principal component over the slope of the least squares regression line. 

```{r mosteller-eyefitting-plot, fig.cap = "Eye Fitting Straight Lines (1981) Data Sets", out.width="75%"}
knitr::include_graphics("images/eyefitting-straight-lines-plots.png")
```

In 2015, the New York Times introduced an interactive feature, called You Draw It [@aisch_cox_quealy_2015; @buchanan_park_pearce_2017; @katz_2017] 
Readers are asked to input their own assumptions about various metrics and learning how these assumptions relate to reality.
The NY Times team utilizes Data Driven Documents (D3) that allows readers to predict these metrics through the use of drawing a line on their computer screen with their computer mouse. 
\cref{fig:nyt-caraccidents} is one such example in which readers are asked to draw the line for the missing years providing what they estimate to be the number of Americans who have died every year from car accidents, since 1990.
After the reader has completed drawing the line, the actual observed values are revealed and the reader may check their estimated knowledge against the actual reported data [@katz_2017].

```{r nyt-caraccidents, fig.cap = "New York Times You Draw It Feature", out.width="75%"}
knitr::include_graphics("images/nyt-caraccidents-frame1.png")
```

### Data Driven Documents

Major news and research organizations such as the NY Times, FiveThirtyEight, Washing Post, and the Pew Research Center create and customize graphics with Data Driven Documents.
In June 2020, the NY Times released an front page displaying figures that represent each of the 100,000 lives lost from the coronavirus pandemic at this point in time [@NYTrememberinglives]. 
During 2021 March Maddness, FiveThirtyEight created a roster-shuffling machine which allowed readers to build their own NBA contender through interactivity [@ryanabest_2021].
Data Driven Documents (D3) is an open-source JavaScript based graphing framework created by Mike Bostock during his time working on graphics at the NY Times.
For readers familiar with R, it is notable to consider D3 in JavaScript equivalent to the ggplot2 package in R. 
Similar to geometric objects and style choices in ggplot2, the grammar of D3 also includes elements such as circles, paths, and rectangles with choices of attributes and styles such as color and size.
Data Driven Documents depend on Extensible Markup Langage (XML) to generate graphics and images by binding objects and layers to the plotting area as Scalable Vector Graphics (SVG) in order to preserve the shapes rather than the pixels \pcref{fig:raster-vs-vector} \ear{CITE: https://martech.zone/vecteezy-svg-editor-online/}. 
Advantages of using D3 include animation and allowing for movement and user interaction such as hovering, clicking, and brushing. 

```{r raster-vs-vector, fig.cap = "SVG vs Raster", out.width="75%"}
knitr::include_graphics("images/raster-vs-vector.png")
```

A challenge of working with D3 is the environment necessary to display the graphics and images. 
The r2d3 package in R provides an efficient integration of D3 visuals and R by displaying them in familiar HTML output formats such as RMarkdown or Shiny applications [@r2d3].
The creator of the graphic applies D3.js code to visualize data which has previously been processed within an R setting. 

```{r r2d3-example, echo = T, eval = F}
r2d3(data = data,
     script = "d3-source-code.js",
     d3_version= "5")
```

The example R code illustrates the structure of the r2d3 function in R which includes specification of a data frame in R (converted to a JSON file), the D3.js source code file, and the D3 version that accompanies the source code.
A default SVG container for layering elements is then generated by the r2d3 function which renders the plot using the source code. 
[Appendix A](#youdrawit-with-shiny) outlines the development of the you draw it study interactive plots through the use of r2d3 and R shiny applications. 
\cref{fig:youdrawit-example} provides an example of a you draw it interactive plot. 
The first frame shows what the participant sees along with the following prompt: \textit{Use your mouse to fill in the trend in the yellow box region}. 
Next, the yellow box region moves along as the participant draws their trend-line until the yellow region disappears indicating the participant has filled in the entire domain.

```{r youdrawit-example, fig.cap = "You Draw It Example", out.width="110%"}
knitr::include_graphics("images/ydiExample-0.10-10-linear.png")
```

## Study Design

This chapter contains two sub studies which aim to establish you draw it as a tool for measuring predictions of trends and then apply this as a method of testing graphics.
@mosteller_eye_1981 was replicated as part of the study data collection in order to validate you draw it as a method for testing graphics, referred to as Eye Fitting Straight Lines in the Modern Era. 
This method is then used to test an individual's ability to make predictions for exponentially increasing data on both the log and linear scales, referred to as Prediction of Exponential Trends.
Data for both sub-studies were collected in conjunction with one another over the same study participant sample.

A total of 6 data sets - 4 Eye Fitting Straight Lines in the Modern Era and 2 Prediction of Exponential Trends - are generated for each individual the start of the experiment. 
The 2 data sets corresponding to to the data used in the Prediction of Exponential Trends are then plotted a total of 4 times each by truncating the points at both 50% and 75% of the domain as well as on both the log and linear scales for a total of 8 task plots. 
Participants in the study are first shown 2 you draw it plot practice plots followed by 12 you draw it task plots. 
The order of all 12 task plots were randomly assigned for each individual in a completely randomized design where users saw the 4 task plots from the Eye Fitting Straight Lines in the Modern Era simulated data interspersed with the 8 task plots from the Prediction of Exponential Trends simulated data.

## Eye Fitting Straight Lines in the Modern Era

@finney_subjective_1951 and @mosteller_eye_1981 use methods such as using a ruler, string, or transparency sheet to fit straight lines through a set of points.
This section replicates the study found in @mosteller_eye_1981 in order to establish you draw it as a tool and method for testing graphics.

### Data Simulation

All data processing was conducted in R before being passed to the D3.js code. 
A total of $N = 30$ points $(x_i, y_i), i = 1,...N$ were generated for $x\in [x_{min}, x_{max}]$ where $x$ and $y$ have a linear relationship.
Data was simulated based on linear model with additive errors: 
\begin{align}
y_i & = \beta_0 + \beta_1 \cdot x_i + e_i \\
\text{with } e_i & \sim N(0, \sigma^2). \nonumber
\end{align} 
The parameters $\beta_0$ and $\beta_1$ are selected to replicate [@mosteller_eye_1981] with $e_i$ generated by rejection sampling in order to guarantee the points shown align with that of the fitted line. 
An ordinary least squares regression is then fit to the simulated points in order to obtain the best fit line and fitted values in 0.25 increments across the domain, $(x_k, \hat y_{k,OLS}), k = 1, ..., 4\cdot x_{max} +1$.
The function then outputs a list of point data and line data both indicating the parameter identification, x value, and corresponding simulated or fitted y value.
The data simulation procedure is described below:

\noindent \textit{Algorithm: Eye Fitting Straight Lines in the Modern Era Data Generation}

\noindent \textbf{In parameters:} $y_{\bar{x}}$ for calculating the y-intercept, $\beta_0$; slope $\beta_1$; standard deviation from line $\sigma$; sample size $N = 30$; domain $x_{min}$ and $x_{max}$; fitted value increment $x_{by} = 0.25$.

\noindent \textbf{Out:} List of point data and line data each indicating the parameter identification, x value, and corresponding simulated or fitted y value.

1. Randomly select and jitter N = 30 x-values along the domain, $x_{i=1:N}\in [x_{min}, x_{max}]$.
2. Determine the y-intercept, $\beta_0$, at x = 0 from the provided slope ($\beta_1$) and y-value at the mean of x ($y_{\bar{x}}$) using point-slope equation of a line.
3. Generate "good" errors, $e_{i = 1:n}$ based on $N(0,\sigma)$ by setting a constraint requiring the mean of the first N/3 errors $< |2\sigma|.$
4. Simulate point data based on $y_i = \beta_0 + \beta_1 x_i + e_i$
5. Obtain ordinary least squares regression coefficients, $\hat\beta_0$ and $\hat\beta_1$, for the simulated point data using the `lm` function in the base stats R package.
6. Obtain fitted values every 0.25 increment across the domain from the ordinary least squares regression $\hat y_{k,OLS} = \hat\beta_{0,OLS} + \hat\beta_{1,OLS} x_k$.
7. Output data list of point data and line data each indicating the parameter identification, x value, and corresponding simulated or fitted y value.

```{r eyefitting-parameters}
data.frame(`Parameter Choice` = c("F", "N", "S", "V"),
           y_xbar = c(3.9, 4.11, 3.88, 3.89),
           slope = c(0.66, -0.70, 0.66, 1.98),
           sigma = c(1.98, 2.5, 1.3, 1.5)
           ) %>%
  knitr::kable("latex", 
               digits = 2, 
               escape = F, 
               booktabs = T, 
               linesep = "", 
               align = "c", 
               label = "eyefiting-parameters",
               col.names = c("Parameter Choice", "$y_{\\bar{x}}$", "$\\beta_1$", "$\\sigma$"),
        caption = "Eye Fitting Straight Lines in the Modern Era simulation model parameters")
```

Simulated model equation parameters were selected to reflect the four data sets (F, N, S, and V) used in @mosteller_eye_1981 \cref{tab:eyefitting-parameters}. 
Parameter choices F, N, and S simulated data across a domain of 0 to 20. 
Parameter choice F produces a trend with a positive slope and a large variance while N has a negative slope and a large variance. 
In comparison, S shows trend with a postie slope but a small variance and V yields a steep positive slope over the domain of 4 to 16. 
\cref{fig:eyefitting-simplot} illustrates an example of simulated data for all four parameter choices.
Within the interactive plot code, the aspect ratio defining the x to y axis ratio was set to 1 with a consistent y range buffer of 10\% to allow for users to draw outside of the provided range. 

```{r eyefitting-simplot, fig.height = 4, fig.width = 12, fig.cap = "Eye Fitting Straight Lines in the Modern Era Simulated Data Example", out.width="100%"}
eyefitting_example_simplot
```

### Results

Participants were recruited through Twitter, Reddit, and direct email during May 2021.
A total of \ear{X} individuals completed \ear{Y} unique you draw it task plots. 
All completed you draw it task plots were included in the analysis.

In addition to the participant drawn points, $(x_k, y_{drawn})$, and the ordinary least squares (OLS) regression fitted values, $(x_k, y_{k,OLS})$, a regression equation based on the first principal component (PCA) was used to calculate fitted values, $x_k, y_{k,PCA}$.
For each set of simulated data and parameter combination, the PCA regression equation was determined by using the princomp function in the base R stats package to obtain the rotations of the first principal component for x and y.
The estimated slope, $\hat\beta_{1,PCA}$, is determined by the ratio of the y rotation and x rotation of the first principal component with the y-intercept, $\hat\beta_{0,PCA}$ calculated by the point-slope equation of the line using the mean of of the simulated points, $(\bar x_i, \bar y_i)$.
Fitted values, $y_{k,PCA}$ are then obtained every 0.25 increment across the domain from the PCA regression equation, $\hat y_{k,PCA} = \hat\beta_{0,PCA} + \hat\beta_{1,PCA} x_k$.
\cref{fig:ols-vs-pca-example} illustrates the difference between an OLS regression equation which minimizes the vertical distance and a regression equation with a slope calculated by the first principal component which minimizes the smallest distance (both horizontal and vertical direction).

```{r ols-vs-pca-example, fig.height = 4.5, fig.width = 9, fig.cap="OLS vs PCA Regression Lines", message=FALSE, warning=FALSE, out.width="100%"}
library(ggplot2)
library(magrittr)
library(plyr)

set.seed(2)
corrCoef = 0.5 # sample from a multivariate normal, 10 datapoints
dat = MASS::mvrnorm(10,c(0,0),Sigma = matrix(c(1,corrCoef,2,corrCoef),2,2))
dat[,1] = dat[,1] - mean(dat[,1]) # it makes life easier for the princomp
dat[,2] = dat[,2] - mean(dat[,2])

dat = data.frame(x1 = dat[,1],x2 = dat[,2])

# Calculate the first principle component
# see http://stats.stackexchange.com/questions/13152/how-to-perform-orthogonal-regression-total-least-squares-via-pca
v = dat%>%prcomp%$%rotation
x1x2cor = bCor = v[2,1]/v[1,1]

x1tox2 = coef(lm(x1~x2,dat))
x2tox1 = coef(lm(x2~x1,dat))
slopeData = data.frame(slope = c(x1x2cor,x2tox1[2]),
                       type=c("Principal Component", "Ordinary Least Squares"))

# We want this to draw the neat orthogonal lines.
pointOnLine = function(inp){
  # y = a*x + c (c=0)
  # yOrth = -(1/a)*x + d
  # yOrth = b*x + d
  x0 = inp[1] 
  y0 = inp[2] 
  a = x1x2cor
  b = -(1/a)
  c = 0
  d = y0 - b*x0
  x = (d-c)/(a-b)
  y = -(1/a)*x+d
  return(c(x,y))
}

points = apply(dat,1,FUN=pointOnLine)

segmeData = rbind(data.frame(x=dat[,1],y=dat[,2],xend=points[1,],yend=points[2,],type = "Principal Component"),
                  data.frame(x=dat[,1],y=dat[,2],yend=dat[,1]*x2tox1[2],xend=dat[,1],type="Ordinary Least Squares"))

dat %>%
ggplot(aes(x1,x2))+
  geom_point()+
  geom_abline(data=slopeData,aes(slope = slope,intercept=0,color=type))+
  geom_segment(data=segmeData,aes(x=x,y=y,xend=xend,yend=yend,color=type))+
  facet_grid(.~type)+
  coord_equal()+
  scale_x_continuous("x") +
  scale_y_continuous("y") +
  theme_bw(base_size = 14) +
  theme(aspect.ratio = 1,
        legend.position = "none",
        axis.text    = element_text(size = 12),
        axis.title   = element_text(size = 12),
        legend.title = element_text(size = 12),
        legend.text  = element_text(size = 12),
        # strip.text = element_text(size = 8, margin = margin(0.1,0,0.1,0, "cm")),
        # strip.background = element_rect(size = 0.8),
        legend.key.size = unit(1, "line")
        ) +
  scale_color_manual(values = c("steelblue", "orange"))
```

<!-- To calculate the first principal component fit: https://benediktehinger.de/blog/science/scatterplots-regression-lines-and-the-first-principal-component/ -->

For each participant, the final data set used for analysis contains $x_{ijk}, y_{ijk,drawn}, \hat y_{ijk,OLS}$, and $\hat y_{ijk,PCA}$ for parameter combination $i = S, V, F, N$, j = $1,...N_{participant}$, and $x_{k}$ value $k = 1, ...,4\cdot x_{max} +1$. 
Using both a linear mixed model and a generalized additive mixed model, comparisons of vertical residuals in relation to the OLS fitted values ($e_{k,ols} = y_{k,drawn} - \hat y_{k,ols}$) and PCA fitted values ($e_{k,pca} = y_{k,drawn} - \hat y_{k,pca}$) were made across the domain of x.
\cref{fig: eyefitting-example-plot} displays an example of all three fitted trend lines for parameter choice F.

```{r eyefitting-example-plot, fig.cap = "Eye Fitting Straight Lines in the Modern Era Example", out.width="75%"}
trial.feedback <- read.csv("data/youdrawit/youdrawit-eyefitting-example-feedback.csv") %>%
    mutate(`Parameter Choice` = "F")
trial.sim <- read.csv("data/youdrawit/youdrawit-eyefitting-example-simulated.csv") %>%
    mutate(`Parameter Choice` = "F")
    
trial.feedback %>%
  ggplot(aes(x = x)) +
  geom_line(aes(y = y, color = "OLS", linetype = "OLS")) +
  geom_line(aes(y = ypca, color = "PCA", linetype = "PCA")) +
  geom_line(aes(y = ydrawn, color = "Drawn", linetype = "Drawn")) +
  geom_point(data = trial.sim, aes(y = y)) +
  facet_wrap(~`Parameter Choice`, labeller = labeller(`Parameter Choice` = label_both)) +
  theme_bw(base_size = 14) +
  theme(aspect.ratio = 1,
        legend.position = "bottom",
        axis.text    = element_text(size = 12),
        axis.title   = element_text(size = 12),
        legend.title = element_text(size = 12),
        legend.text  = element_text(size = 12),
        # strip.text = element_text(size = 8, margin = margin(0.1,0,0.1,0, "cm")),
        # strip.background = element_rect(size = 0.8),
        legend.key.size = unit(1, "line")
        ) +
  scale_x_continuous(limits = c(0,20)) +
  scale_color_manual("", values = c("black", "steelblue", "orange")) +
  scale_linetype_manual("", values = c("dashed", "solid", "solid"))
```

Using the lmer function in the lme5 package [@lme4], a linear mixed model (LMM) is fit separately to the OLS and PCA residuals, constraining the fit to a linear trend. 
Parameter choice, $x$, and the interaction between $x$ and parameter choice were treated as fixed effects with a random participant effect accounting for variation due to participant.
The LMM equation for each fit (OLS and PCA) residuals is given by:
\begin{equation}
y_{ijk,drawn} - y_{ijk,fit} = e_{ijk,fit} = \left[\gamma_0 + \alpha_i\right] + \left[\gamma_{1} x_{ijk} + \gamma_{2i} x_{ijk}\right] + p_{j} + \epsilon_{ijk}
\end{equation}
\noindent where

+ $\gamma_0$ is the overall intercept
+ $\alpha_i$ is the effect of the parameter combination on the intercept
+ $\gamma_1$ is the overall slope for x
+ $\gamma_{2i}$ is the effect of the parameter combination on the slope
+ $p_{j} \sim N(0, \sigma^2_{participant})$ is the participant error due to participant variation
+ $\epsilon_{ij} \sim N(0, \sigma^2)$ is the residual error.

Eliminating the linear trend constraint, the bam function in the mgcv package [@mgcv1; @mgcv2; @mgcv3; @mgcv4; @mgcv5], a generalized additive mixed model (GAMM) is fit separately to the OLS and PCA residuals to allow for estimation of smoothing splines.
Parameter choice was treated as a fixed effect with no extimated intercept and a separate smoothing spline for $x$ was estimated for each parameter choice. A random participant effect accounting for variation due to participant and a random spline for each participant accounted for variation in spline for each participant.
The GAMM equation for the each fit (OLS and PCA) residuals is given by:
\begin{equation}
y_{ijk, drawn} - y_{ijk, fit} = e_{ijk,fit} = \alpha_i + s_{i}(x_{ijk}) + p_{j} + s_{j}(x_{ijk})
\end{equation}
\noindent where

+ $\alpha_i$ is the intercept for the parameter choice $i$
+ $s_{i}$ is the smoothing spline for the $i^{th}$ parameter combination
+ $p_{j} \sim N(0, \sigma^2_{participant})$ is the error due to participant variation
+ $s_{j}$ is the random smoothing spline for each participant.

\cref{fig:eyefitting-lmer-residualplots} and \cref{fig:eyefitting-gamm-residualplots} show the estimated trend from both the LMM and GAMM.
These results indicate participants provided a trend line closer in relation to the estimated PCA regression line than the estimated OLS regression line. These results are consistent to those found in [@mosteller_eye_1981].

```{r eyefitting-lmer-residualplots, fig.height = 4, fig.width = 12, fig.cap = "Eye Fitting Straight Lines in the Modern Era LMM results", out.width="100%"}
eyefitting.lmer.plot
```

```{r eyefitting-gamm-residualplots, fig.height = 4, fig.width = 12, fig.cap = "Eye Fitting Straight Lines in the Modern Era GAMM results", out.width="100%"}
eyefitting.gamm.plot
```

In addition to fitting trend lines over the residuals between the drawn values and fitted values, a OLS sum of squares and PCA sums of squares measure was calculated for each you draw it plot.
Sums of squares between fits were compared using lmer [@lme4] to run a linear mixed model (LMM) with a log transformation.
Parameter combination, fit, and the interaction between parameter combination and fit were treated as fixed effects with a random participant effect. 
Define $SS_{ijk}$ as the sums of squares for parameter choice $i = 1,2,3,4$, fit $j=1,2$, and participant $k = 1,...,N_{participant}$.
The LMM equation is given by: 
\begin{equation}
\log\left(SS_{ijk}\right) = \alpha_i + \beta_j + \alpha\beta_{ij} + p_{j} + \epsilon_{ij}
\end{equation}

+ $\alpha_i$ denotes the effect of the $i^{th}$ parameter (S, F, V, N)
+ $\beta_j$ denotes the effect of the $j^{th}$ fit (OLS, PCA)
+ $\alpha\beta_{ij}$ denotes the interaction between the parameter combination and fit
+ $p_{j} \sim N(0, \sigma^2_{participant})$ is the error due to participant variation
+ $\epsilon_{ij} \sim N(0, \sigma^2)$ is the residual error.

\cref{fig:eyefitting-ss-oddsratio} displays estimated odds ratios between the OLS fit and PCA fit for each parameter choice.
While there is no significant effect of fit for any parameter choices, there is indication of the trend found above.

```{r eyefitting-ss-oddsratio, fig.cap = "Eye Fitting Straight Lines in the Modern Era Sum of Squares Results", out.width="75%"}
ss.slicediffs <- read.csv("data/youdrawit/youdrawit-ssSlicediffs-lmer.csv")
ss.slicediffs %>%
  ggplot(aes(x = ratio, y = parm_id)) +
  geom_point() +
  geom_errorbar(aes(xmin = lower.CL, xmax = upper.CL), width = 0.5) +
  geom_vline(xintercept = 1, linetype = "dashed") +
  theme_bw(base_size = 14) +
  theme(aspect.ratio = 0.5,
        axis.text    = element_text(size = 12),
        axis.title   = element_text(size = 12),
        legend.title = element_text(size = 12),
        legend.text  = element_text(size = 12)
        ) +
  scale_x_continuous("Sum of Squares Odds Ratio \n (OLS vs PCA)", limits = c(0,2.5), breaks = seq(0,2.5,0.5)) +
  scale_y_discrete("Data Set")
```

## Prediction of Exponential Trends

### Data Simulation

+ aspect ratio = 1; linear = T/F; start drawing at x = 10; xrange = (0,20), yrange = range(points)*c(0.5,2)

\noindent *Algorithm 3.2: Exponential Data Generation*

**In parameters:** `beta, sd, points_choice = "partial", points_end_scale, N = 30, xmin = 0, xmax = 20, xby = 0.25`

**Out:** data list of point data and line data

1. Randomly select and jitter N = 30 x-values along the domain.
2. Generate "good" errors based on N(0,sd). Set constraint of the mean of the first N/3 = 10 errors less than |2*sd|
3. Simulate point data based on:  `y = exp(x*beta + errorVals)`
4. Obtain starting value for beta: `lm(log(y) ~ x, data = point_data)`
5. Use NLS to fit a better line to the point data: `nls(y ~ exp(x*beta), data = point_data, ...)`
6. Simulate nonlinear least squares line data: `y = exp(x*betahat)`
7. Output data list of point data and line data

### Parameter Selection

+ Visit [You Draw It Development - parameter selection](https://emily-robinson.shinyapps.io/you-draw-it-parameter-selection/) for examples.

+ Exponential (Linear/Log): 2 x 2 x 2 Factorial
    + Beta: 0.1 (sd. 0.09); 0.23 (0.25)
    + Points End: 0.5; 0.75
    + Scale: Linear; Log

### Results

+ advocate smoothing of scatterplots to assist in detecting the shape of the point cloud in situations where the error in the data is substantial, or where the density of points changes along the abscissa @cleveland_graphical_1984
+ Twitter/Reddit/Direct Email Pilot Study (05/03/2021): [Exponential Prediction](https://srvanderplas.github.io/Perception-of-Log-Scales/you-draw-it-development/you-draw-it-pilot-app/analyses/you-draw-it-exponential-prediction-pilot.html)
+ https://shiny.srvanderplas.com/you-draw-it/
+ Twitter/Reddit/Direct Email Pilot Study (05/03/2021): [Exponential Prediction](https://srvanderplas.github.io/Perception-of-Log-Scales/you-draw-it-development/you-draw-it-pilot-app/analyses/you-draw-it-exponential-prediction-pilot.html)

## Discussion and Conclusion


