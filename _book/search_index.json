[["index.html", "HUMAN PERCEPTION OF EXPONENTIALLY INCREASING DATA DISPLAYED ON A LOG SCALE CHAPTER 1 Literature Review 1.1 Introduction to Graphics 1.2 Perception and Psychophysics 1.3 Graphical Experiments 1.4 Logarithmic Scales and Mapping 1.5 Underestimation of Exponential Growth 1.6 Research Objectives", " HUMAN PERCEPTION OF EXPONENTIALLY INCREASING DATA DISPLAYED ON A LOG SCALE Emily Anna Robinson Abstract Log scales are often used to display data over several orders of magnitude within one graph. During the COVID-19 pandemic, we have seen both the benefits and the pitfalls of using log scales to display case counts. Three graphical experimental tasks were conducted to evaluate the impact our choice of scale has on human perception of exponentially increasing trends. The first experiment evaluates whether our ability to perceptually notice differences in exponentially increasing trends is impacted by the choice of scale. We conducted a visual inference experiment in which participants were shown a series of lineup plots (consisting of 19 null panels and 1 target panel) and asked to identify the panel that was most different from the others. Our results indicated that when there was a large difference in curvature between the target plot and null plots, the choice of scale had no impact and participants accurately differentiated between the two curves on both the linear and log scale. However, displaying exponentially increasing data on a log scale improved the accuracy of differentiating between models with slight curvature differences. An exception occurred when identifying a plot with curvature embedded in surrounding plots closely relating to a linear trend, indicating that it is easy to identify a curve in a group of lines but much harder to identify a line in a group of curves. The use of visual inference to identify these guidelines suggests that there are perceptual advantages to log scales when differences are subtle. Our other experimental tasks focus on determining whether there are cognitive disadvantages to log scales: do log scales make it harder to make use of graphical information? We conducted a graphical task similar to the New York Times You Draw It page to test an individuals ability to use and make predictions for exponentially increasing data. We asked participants to draw a line using their computer mouse through the increasing exponential trend shown on both scales. In addition to differentiation and prediction of exponentially increasing data, we conduct an experimental task to test an individuals ability to translate a graph of exponentially increasing data into real value quantities and extend their estimations by making comparisons. The results of our experimental tasks allow us to provide guidelines for readers to actively choose which of many possible graphics to draw, according to some set of design choices, to ensure that our charts are effective. (399 words; 350 word limit) CHAPTER 1 Literature Review Editing text colors: Emily may also use mostly black text as well. 1.1 Introduction to Graphics Advanced technology and computing power has promoted data visualization as a central tool in modern data science (Unwin, 2020). Data visualization is defined as the art of drawing graphical charts in order to display data. illustrates the process of creating a graphic from a data set through the use of variable mapping, data transformations, coordinate systems, and aesthetic features (Vanderplas, Cook, &amp; Hofmann, 2020) . Graphics are useful for data cleaning, exploring data structure, and have been an essential component in communicating information for the last 200 years (Lewandowsky &amp; Spence, 1989; Unwin, 2020; Vanderplas, Cook, &amp; Hofmann, 2020) During the 20th century, companies began utilizing graphics to understand their mechanics and support business decisions; news sources began displaying graphics of weather forecasts as a means to communicate critical information and aid in decision-making (Vanderplas, Cook, &amp; Hofmann, 2020) . Today, we encounter data visualizations everywhere, researchers include graphics to communicate their results in scientific publications and mass media relies on graphics to convey news stories to the public through newspapers, TV, and the Web (Unwin, 2020). Figure 1.1: Graphic flowchart In general, the public holds the self perception that numbers are difficult to understand and that they did not perform well in mathematics during school (Unwin, 2020). In contrary, there tends to be a positive self perception when it comes to graphics as they are viewed more as illustrations and not as critical parts of an argument. While tables can be burdensome to readers, graphics can improve the interpretation and representation of the same data (Uri &amp; Haemer, 1948). Shah, Mayer, &amp; Hegarty (1999) presents an opposing argument claiming there are still difficulties interpreting and explaining quantitative information depicted in graphs. One explanation for the opposing views is that the general public is untrained in the ability required for the evaluation of graphic material. The complexity of graphic devices is directly related to the degree and formality of training necessary for understanding (Haemer &amp; Kelley, 1949). Although statistical graphics have become widely used and valued in science, business, and in many other aspects of life, as creators of graphics, we are too accepting of them as default without asking critical questions about the graphics we create or view (Unwin, 2020). Vanderplas, Cook, &amp; Hofmann (2020) poses the general question we must ask ourselves, how effective is this graph at communicating useful information? Higher quality of technology has influenced the creation, replication, and complexity of graphics as there are an infinitely many number of graphical displays and design choices that can be implemented at faster speeds with more flexibility. The creator of a graphic makes decisions about the variables displayed, the type of graphic, the size of the graphic and the aspect ratio, the colors and symbols used, the scales and limits, the ordering of categorical variables, and the ordering of variables in multivariate displays (Unwin, 2020). In response to the increasing number of design choices, consistent themes and higher standards are being placed on graphics. Selecting from an extensive list of styles and choices of graphics in order to effectively communicate insights into the data is a challenging task. A consistent concern is the lack of theory of graphics available to build on; better theory should result in better graphics. Creators of graphics need an established set of concepts and terminology to build their graphics from so they can actively choose which of many possible graphics to draw in order to ensure their charts are effective. Many efforts have been made to provide guidelines for graphical designs including Wilkinsons Grammar of Graphics (Wilkinson, 2012). These guidelines provide the ground work necessary for data plots to be depicted and interpreted as statistics (Buja et al., 2009; Majumder, Hofmann, &amp; Cook, 2013). Vanderplas, Cook, &amp; Hofmann (2020) define a statistic as, a functional mapping of a variable or set of variables. The grammar of graphics constructs visual statistics through the use of tidy data, characterized as a data set in which each variable is in its own column, each observation is in its own row, and each value is in its own cell (Hadley Wickham &amp; Grolemund, 2016). The grammar allows variables, as designated in columns, to be mapped to different elements of the graphic such as the axes, colors, shapes, or facets. Software, such as Hadley Wickhams ggplot2 (Hadley Wickham, 2011), aims to implement these guidelines recommended through the grammar of graphics. Later, it is illustrated how the structure of tidy data and the construction of graphics as statistics aid themselves for easy experimentation which allows researchers to compare the effectiveness and understand the perception of different types of charts (Vanderplas, Cook, &amp; Hofmann, 2020). It is important to consider the purpose and motivation behind the generation of the chart as well as the complexity and intended audience you intend to view the chart. A general guideline when generating graphics is to keep it familiar in order to not intimidate your audience and to encourage further interaction from the viewers (Unwin, 2020). Despite past attempts to improve the use of graphics in science, Gordon &amp; Finch (2015) evaluated 97 graphs for overall quality, based on five principles of graphical excellence, and found there is still an astonishing lack in the quality of graphics. More startling is the fact that the source of the graphic from an applied science or a statistics graphic had no effect on the quality of the graphic. The improvement in graphics depends on both a better definition of variables, units of measurements, scales, and other graphical elements as well as a typical use of grid lines on an accepted set of graphical forms. With the support of changes in software defaults, future work must be done in order to implement the academic research being conducted in graphics into practice for both academics and non-academics in order to achieve a higher standard of the graphics being presented (Vanderplas, Cook, &amp; Hofmann, 2020). 1.2 Perception and Psychophysics In order to develop guiding principles for generating graphics effective in communication, we must first understand the basic mechanics of the human perceptual system and the biases we are vulnerable to (Goldstein &amp; Brockmole, 2017). The perceptual process is complex and involved . Figure 1.2: Perceptual process This process is broken down into sensation - involving simple processes that occur right at the beginning of a sensory system - and perception - involving higher-order mechanisms and identified with more complex processes. The perceptual process begins when there is a stimulus in the environment and light is reflected and focused back into the viewers eyes. Within the eye, the light reflected is transformed and focused by the eyes optical system and an image is formed on the receptors of the viewers retina. It is important to note that everything a person perceives is based not on direct contact with stimuli but on representations of stimuli that are formed on the receptors and the resulting activity in the persons nervous system. Once light is reflected and focused, our visual receptors respond to the light and transform the light energy into electrical energy through a process called transduction. Signals from the receptors are then transmitted through the retina, to the brain, and then within the brain where perception - what do you see? - and recognition - what is it called? - occur. After recognition, viewers take some sort of motor action - for example, move closer to the object. The perceptual process is not direct and instead takes on more of a cyclic nature where a person may go through many iterations of stimuli, perception, recognition, and action before the final image is identified and understood. When perception occurs, we first experience the preattentive stage in which we observe color, shape, size, and other basic information about the stimuli being perceived. Preattentive perception effects are automatically processed within the first 500 millisectonds of viewing and do not depend on sustained cognitive attention (Vanderplas, Cook, &amp; Hofmann, 2020). Following the preattentive stage, direct attention is required for additional processing to allow us to draw connections between components that assist in our interpretation of the stimuli. When viewing a chart or graph, most insights we gain are due to the cognitive processes that occur after attention is focused on specific aspects of the graph. The knowledge a viewer brings to the situation influences their cognitive evaluation of the stimuli. The relationship between physiology and perception can provide us information about how graphics may be understood and interpreted. According to a cognitive analysis, graph interpretation involves (a) relatively simple pattern perception and association processes in which viewers can associate graphic patterns to quantitative referents and (b) more complex and error-prone inferential processes in which viewers must mentally transform data (Shah, Mayer, &amp; Hegarty, 1999). Pyschophysics, the branch of psychology that deals with the relationships between physical stimuli (e.g. light) and mental phenomena, aims to provide explanations of this relationship and point out human perceptual biases. By examining both behavior and physiology together, we are able to understand the mechanisms responsible for perception. 1.3 Graphical Experiments One way in which we determine the relationship between behavior and physiology is through the use of graphical tests. These tests may take many forms: identifying differences in graphs, reading information off of a chart accurately, using data to make correct real-world decisions, or predicting the next few observations. All of these types of tests require different levels of use and manipulation of the information presented in the chart. The initial push to develop classification and recommendation systems for charts was grounded on heuristics rather than on experimentation (Vanderplas, Cook, &amp; Hofmann, 2020). Request were made for the validation of the perception and utility of statistical charts through graphical experiments. Initial experiments struggled with methodological issues (Croxton &amp; Stein, 1932; Croxton &amp; Stryker, 1927; Eells, 1926) with most early experimentation stemming from psychophysics research on the perception of size and shape (Teghtsoonian, 1965). While a typical psychophysics experiment focuses on whether an effect is detectable and whether the magnitude of the effect can be accurately estimated, these early experiments instead depended on speed and accuracy for plot evaluation (Lewandowsky &amp; Spence, 1989; Spence, 1990; Teghtsoonian, 1965). Cognitive psychologists and statisticians made progress by conducting experiments to identify perceptual errors associated with different styles of graphics and charts (Cleveland &amp; McGill, 1985; Spence, 1990). These later experiments relied on similar methodology as early studies by relying on participants directly reading information from the charts to provide a quantitative estimate or answering a predefined question; as with the early studies, accuracy and response time being evaluated (Amer, 2005; Broersma &amp; Molenaar, 1985; Cleveland &amp; McGill, 1984; Dunn, 1988; Peterson &amp; Schramm, 1954; Tan, 1994). Cleveland &amp; McGill (1984) provide a basis for perceptual judgment, still utilized today, by examining six basic stimuli: position along a common scale, position along nonaligned scales, length, angle, slope, and area. Other experiments established the notion that redesigning graphs can result in the improvement of the viewers interpretation (Shah, Mayer, &amp; Hegarty, 1999). This is done by relying on gestalt principles to minimize the inferential processes and maximize the pattern association processes required to interpret relevant information. The viewer must first encode the visual array by identifying meaningful visual features, such as a straight light slanting downward. Next, the viewer must classify the quantitative measures and relationships in which those visual features illustrate, such as a decreasing linear relationship between x and y. The last step involves translating the quantitative measures and relationships to the variables defined in the data set, such as a population decreasing over years. These studies establish the process in which viewers interact with charts by first perceptually observing the visual features and later translating to cognitive processing of the information depicted by those features (Shah &amp; Carpenter, 1995). In recent years, there have been advancements in the methodology used to investigate the effectiveness of statistical charts (Vanderplas, Cook, &amp; Hofmann, 2020). Some of the new methods, such as the lineup protocol (Buja et al., 2009), utilize the grammar of graphics designation of a data plot as a statistic through the functional mapping of variable(s). This allows the data plot to be tested similar to other statistics, by comparing the actual data plot to a set of plots with the absence of any data structure we can test the likelihood of any perceived structure being significant (Vanderplas, Cook, &amp; Hofmann, 2020). While the methodology of these recent experiments differs from earlier studies, the focus is still on initial perception and graph comprehension with a relatively small amount of work conducted to understand the effect of design choices on higher cognitive processes such as learning or analysis (Green &amp; Fisher, 2009). Most recent graphics experiments have utilized tools such as Amazon Turk, Prolific, Reddit, and other crowd sourcing websites to evaluate the psychophysics and patterns associated with design choices (VanderPlas &amp; Hofmann, 2017). Such studies tend to be limited by the questions asked while other methods such as Thinking aloud and Eye Tracking allow researchers to evaluate the overall perception of the graphic (Vanderplas, Cook, &amp; Hofmann, 2020). 1.4 Logarithmic Scales and Mapping We have recently experienced the impact graphics and charts have on a large scale through the SARSNCOV-2 pandemic (COVID-19). At the beginning of 2020, we saw an influx of dashboards being developed to display case counts, transmission rates, and outbreak regions (GmbH, 2020); mass media routinely showed charts to share information with the public about the progression of the pandemic (Romano, Sotis, Dominioni, &amp; Guidi, 2020). People began seeking out graphical displays of COVID-19 data as a direct result of these pieces of work (GmbH, 2020); providing increased and ongoing exposure to these graphics over time. Many of these graphics helped guide decision makers to implement policies such as shut-downs or mandated mask wearing, as well as facilitated communication with the public to increase compliance (Bavel et al., 2020). As graphics began to play an important role in the communication of the pandemic, creators of graphics were faced with design choices in order to ensure their charts were effective. When faced with data which spans several orders of magnitude, we must decide whether to show the data on its original scale (compressing the smaller magnitudes into relatively little area) or to transform the scale and alter the contextual appearance of the data. One common solution is to use a log scale transformation to display data over several orders of magnitude within one graph. Logarithms turn multiplicative relationships additive, showing elasticities and other proportional changes, and also linearize power laws (Menge et al., 2018). They also have practical purposes, easing the computation of small numbers such as likelihoods and transforming data to fit statistical assumptions. When presenting log scaled data, it is possible to use either un-transformed scale labels (for example, values of 1, 10 and 100 are equally spaced along the axis) or log transformed scale labels (for example, 0, 1, and 2, showing the corresponding powers of 10). We have recently experienced the benefits and pitfalls of using log scales as COVID-19 dashboards displayed case count data on both the log and linear scale (Burn-Murdoch et al., 2020; Fagen-Ulmschneider, 2020). In spring 2020, during the early stages of the COVID-19 pandemic, there were large magnitude discrepancies in case counts at a given time point between different geographic regions (e.g. states and provinces as well as countries and continents). During this time, we saw the usefulness of log scale transformations showing case count curves for areas with few cases and areas with many cases within one chart. As the pandemic evolved, and the case counts were no longer spreading exponentially, graphs with linear scales seemed more effective at spotting early increases in case counts that signaled more localized outbreaks. This is only one recent example of a situation in which both log and linear scales are useful for showing different aspects of the same data. There are long histories of using log scales to display results in ecology, psychophysics, engineering, and physics (Heckler, Mikula, &amp; Rosenblatt, 2013; Menge et al., 2018) . The usefulness of the log scale in science is exemplified in (Munroe, 2005). Figure 1.3: Log scale comic Research suggests our perception and mapping of numbers to a number line is logarithmic at first, but transitions to a linear scale later in development, with formal mathematics education (Dehaene, Izard, Spelke, &amp; Pica, 2008; Siegler &amp; Braithwaite, 2017, 2017; Varshney &amp; Sun, 2013). For example, a kindergartner asked to place numbers 1-10 along a number line would place 3 close to the middle, following the logarithmic perspective (Varshney &amp; Sun, 2013). With basic training, members of remote cultures with a basic vocabulary and minimal education understood the concept that numbers can be mapped into a spacial space (Dehaene, Izard, Spelke, &amp; Pica, 2008). There was a gradual transition from logarithmic to linear scale as the mapping of whole number magnitude representations transitioned from a compressed (approximately logarithmic) distribution to an approximately linear one. These results indicate the universal and cultural-dependent characteristics of the sense of number. This phenomenon was first discovered by Ernst Weber, an early psychophysics researcher, by determining the relationship between the difference threshold (smallest detectable difference between two sensory stimuli; known as the Just Noticable Difference) and the magnitude of a stimulus. This holds true for a variety of stimuli such as weight, light, and sound as well as for a range of magnitudes; larger numbers require a proportional larger difference in order to remain equally discriminate (Dehaene, Izard, Spelke, &amp; Pica, 2008). Known as Webers law, it was established that we do not notice absolute changes in stimuli, but instead that we notice the relative change (Sun, Wang, Goyal, &amp; Varshney, 2012). Numerically, Webers Law is defined as \\[\\begin{equation} \\frac{\\Delta S}{S} = K \\end{equation}\\] where \\(\\Delta S\\) represents the difference threshold, S represents the initial stimulus intensity, and K is called Webers contrast which remains constant as the magnitude of S changes. Gustav Fechner, a founder of psychophysics, provided further extension to Webers law by discovering the relationship between the perceived intensity is logarithmic to the stimulus intensity when observed above a minimal threshold of perception (Sun, Wang, Goyal, &amp; Varshney, 2012). Formally known as the Weber-Fechner law is derived from Webers law as \\[\\begin{equation} P = K\\ln \\frac{S}{S_0} \\end{equation}\\] where P represents the perceived stimulus, K represents Webers contrast, S represents the initial stimulus intensity, and \\(S_0\\) represents the minimal threshold of perception. Assuming there is a direct relationship between perceptual and cognitive processes, it is reasonable to assume numerical representations should also be displayed on a nonlinear, compressed number scale. Therefore, if we perceive logarithmically by default, it is a natural (and presumably low effort) way to display information and should be easy to read and understand/use. The idea is compression enlarges the coding space, thus increasing the dynamic range of perception and firing neurons within our visual system (Nieder &amp; Miller, 2003). Similar to the training and education required to transition from logarithmic mapping to linear mapping, there is also necessary training required in the assessment of graphical displays associated with logarithmic scales. Haemer &amp; Kelley (1949) identify semi-logarithmic charts for temporal series as requiring a certain degree of technical training. 1.5 Underestimation of Exponential Growth Early studies explored the estimation and prediction of exponential growth, finding that growth is underestimated when presented both numerically and graphically but that numerical estimation is more accurate than graphical estimation for exponential curves (Wagenaar &amp; Sagaria, 1975). One way to improve estimation of increasing exponential trends is to provide immediate feedback to participants about the accuracy of their current predictions (Mackinnon &amp; Wearing, 1991). While prior contextual knowledge or experience with exponential growth does not improve estimation, instruction on exponential growth reduces the underestimation: participants adjust their initial starting value but not their perception of growth rate (Jones, 1977; Wagenaar &amp; Sagaria, 1975). Our inability to accurately predict exponential growth might also be addressed by log transforming the data, however, this transformation introduces new complexities; most readers are not mathematically sophisticated enough to intuitively understand logarithmic math and translate that back into real-world effects. In Menge et al. (2018), ecologists were surveyed to determine how often ecologists encounter log scaled data and how well ecologists understand log scaled data when they see it in the literature. Participants were presented two relationships displayed on linear-linear scales, log-log scales with untransformed values, or loglog scales with log transformed values. The authors propose three types of misconceptions participants encountered when presented data on log-log scales: hand-hold fallacy, Zenos zero fallacy, and watch out for curves fallacies. These misconceptions are a result of linear extrapolation assuming that a line in log-log space represents a line instead of the power law in linear-linear space. The study found that in each of these scenarios, participants were confident in their incorrect responses, indicating incorrect knowledge rather than a lack of knowledge. The hand-hold fallacy stems from the misconception that steeper slopes in log-log relationships are steeper slopes in linear-linear space. In fact, it is not only the slope that matters, but also the intercept and the location on the horizontal axis since a line in log-log space represents a power law in linear-linear space (i.e. linear extrapolation). Emerging from Zenos zero fallacy is the misconception that positively sloped lines in log-log space can imply a non-zero value of y when x is zero. This is never true as positively sloped lines in log-log space actually imply that y = 0 when x = 0. This misconception again is a result of linear extrapolation assuming that a line in log-log space represents a line instead of the power law in linear-linear space. The last misconception, watch out for curves fallacies encompasses three faults: (1) lines in log-log space are lines in linear-linear space, (2) lines in log-log space curve upward in linear-linear space, and (3) curves in log-log space have the same curvature in linear-linear space. Linear extrapolation is again responsible for the first and third faults while the second fault is a result of error in thinking that log-log lines represent power laws (which are exponential relationships), and all exponential relationships curve upward; this is only true when the log-log slope is greater than 1. Menge et al. (2018) found that in each of these scenarios, participants were confident in their incorrect responses, indicating incorrect knowledge rather than a lack of knowledge. 1.6 Research Objectives In my research, I conduct three graphical experimental tasks to evaluate the impact our choice of scale (log/linear) has on human perception of exponentially increasing trends. The first experiment evaluates whether our ability to perceptually notice differences in exponentially increasing trends is impacted by the choice of scale. I conducted a visual inference experiment in which participants were shown a series of lineup plots and asked to identify the panel that was most different from the others. The other experimental tasks focus on determining whether there are cognitive disadvantages to log scales: do log scales make it harder to make use of graphical information? I conducted a graphical task similar to the New York Times You Draw It page to test an individuals ability to use and make predictions for exponentially increasing data. Participants were asked to draw a line using their computer mouse through the increasing exponential trend shown on both scales. In addition to differentiation and prediction of exponentially increasing data, an experimental task was conducted to test an individuals ability to translate a graph of exponentially increasing data into real value quantities and extend their estimations by making comparisons. The results of the three experimental tasks provide guidelines for readers to actively choose which of many possible graphics to draw, according to some set of design choices, to ensure that their charts are effective. "],["2-lineups.html", "CHAPTER 2 Perception through lineups 2.1 Introduction 2.2 Data Generation 2.3 Parameter Selection 2.4 Lineup Setup 2.5 Study Design 2.6 Results 2.7 Discussion and Conclusion", " CHAPTER 2 Perception through lineups 2.1 Introduction Previously, we saw how a data plot can be evaluated and treated as a visual statistic, a numerical function which summarizes the data. To evaluate a graph, we have to run our statistic through a visual evaluation - a person. If two different methods of presenting data result in qualitatively different results when evaluated visually, then we can conclude that the visual statistics are significantly different. Recent graphical experiments have utilized statistical lineups to quantify the perception of graphical design choices (Hofmann, Follett, Majumder, &amp; Cook, 2012; Loy, Follett, &amp; Hofmann, 2016; Loy, Hofmann, &amp; Cook, 2017; VanderPlas &amp; Hofmann, 2017). Statistical lineups provide an elegant way of combining perception and statistical hypothesis testing using graphical experiments (Majumder, Hofmann, &amp; Cook, 2013; Vanderplas, Cook, &amp; Hofmann, 2020; H. Wickham, Cook, Hofmann, &amp; Buja, 2010). Lineups are named after the police lineup of criminal investigations where witnesses are asked to identify the criminal from a set of individuals. Similarly, a statistical lineup is a plot consisting of smaller panels; the viewer is asked to identify the plot of the real data from among a set of decoy null plots. A statistical lineup typically consists of 20 panels - 1 target panel and 19 null panels. Figure If the viewer can identify the target panel embedded within the set of null panels, this suggests that the real data is visually distinct from data generated under the null model. provides examples of a statistical lineups. The lineup plot on the left displays increasing exponential data on a linear scale with panel (2 x 5) + 3 as the target. The lineup plot on the right displays increasing exponential data on the log scale with panel 2 x 2 as the target. Crowd sourcing websites such as Amazon Mechanical Turk, Reddit, and Prolifc allow us to collect responses from multiple viewers. Figure 2.1: Lineup example While explicit graphical tests direct the participant to a specific feature of a plot to answer a specific question, implicit graphical tests require the user to identify both the purpose and function of the plot in order to evaluate the plots shown (Vanderplas, Cook, &amp; Hofmann, 2020). Implicit graphical tests, such as lineups, have the advantage of simultaneously visually testing for multiple visual features including outliers, clusters, linear and nonlinear relationships. To lay a foundation for future exploration of the use of log scales, we begin with the most fundamental ability to identify differences in charts; this does not require that participants understand exponential growth, identify log scales, or have any mathematical training. Instead, we are simply testing the change in perceptual sensitivity resulting from visualization choices. The study in this chapter is conducted through visual inference and the use of statistical lineups to differentiate between exponentially increasing curves with differing levels of curvature, using linear and log scales. 2.2 Data Generation In this study, both the target and null data sets were generated by simulating data from an exponential model; the models differ in the parameters selected for the null and target panels. In order to guarantee the simulated data spans the same domain and range of values, we implemented a domain constraint of \\(x\\in [0,20]\\) and a range constraint of \\(y\\in [10,100]\\) with \\(N = 50\\) points randomly assigned throughout the domain and mapped to the y-axis using the exponential model with the selected parameters. These constraints provide some assurance that participants who select the target plot are doing so because of their visual perception differentiating between curvature or growth rate rather than different starting or ending values. Data was simulated based on a three-parameter exponential model with multiplicative errors: \\[\\begin{align} y_i &amp; = \\alpha\\cdot e^{\\beta\\cdot x_i + \\epsilon_i} + \\theta \\\\ \\text{with } \\epsilon_i &amp; \\sim N(0, \\sigma^2). \\nonumber \\end{align}\\] The parameters \\(\\alpha\\) and \\(\\theta\\) are adjusted based on \\(\\beta\\) and \\(\\sigma^2\\) to guarantee the range and domain constraints are met. The model generated \\(N = 50\\) points \\((x_i, y_i), i = 1,...,N\\) where \\(x\\) and \\(y\\) have an increasing exponential relationship. The heuristic data generation procedure is described below: domain \\(x\\in[0,20]\\), range \\(y\\in[10,100]\\), midpoint \\(x_{mid}\\). estimated model parameters \\(\\hat\\alpha, \\hat\\beta, \\hat\\theta\\) Determine the \\(y=-x\\) line scaled to fit the assigned domain and range. Map the values \\(x_{mid} - 0.1\\) and \\(x_{mid} + 0.1\\) to the \\(y=-x\\) line for two additional points. From the set points \\((x_k, y_k)\\) for \\(k = 1,2,3,4\\), obtain the coefficients from the linear model \\(\\ln(y_k) = b_0 +b_1x_k\\) to obtain starting values - \\(\\alpha_0 = e^{b_0}, \\beta_0 = b_1, \\theta_0 = 0.5\\cdot \\min(y)\\) Using the nls() function from the stats package in Rstudio and the starting parameter values - \\(\\alpha_0, \\beta_0, \\theta_0\\) - fit the nonlinear model, \\(y_k = \\alpha\\cdot e^{\\beta\\cdot x_k}+\\theta\\) to obtain estimated parameter values - \\(\\hat\\alpha, \\hat\\beta, \\hat\\theta.\\) sample size \\(N = 50\\), estimated parameters \\(\\hat\\alpha\\), \\(\\hat\\beta\\), and \\(\\hat\\theta\\), \\(\\sigma\\) standard deviation from the exponential curve. \\(N\\) points, in the form of vectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\). Generate \\(\\tilde x_j, j = 1,..., N\\cdot \\frac{3}{4}\\) as a sequence of evenly spaced points in \\([0,20]\\). This ensures the full domain of \\(x\\) is used, fulfilling the constraints of spanning the same domain and range for each parameter combination. Obtain \\(\\tilde x_i, i = 1,...N\\) by sampling \\(N = 50\\) values from the set of \\(\\tilde x_j\\) values. This gaurantees some variability and potential clustring in the exponential growth curve disrupting the perception due to continuity of points. Obtain the final \\(x_i\\) values by jittering \\(\\tilde x_i\\). Calculate \\(\\tilde\\alpha = \\frac{\\hat\\alpha}{e^{\\sigma^2/2}}.\\) This ensures that the range of simulated values for different standard devaition parameters has an equal expected value for a given rate of change due to the non-constant variance across the domain. Generate \\(y_i = \\tilde\\alpha\\cdot e^{\\hat\\beta x_i + e_i}+\\hat\\theta\\) where \\(e_i\\sim N(0,\\sigma^2).\\) 2.3 Parameter Selection For each level of difficulty, we simulated 1000 data sets of \\((x_{ij}, y_{ij})\\) points for \\(i = 1,...,50\\) and \\(j = 1...10\\). Each generated \\(x_i\\) point from was replicated 10 times. Then the lack of fit statistic (LOF) was computed for each simulated data set by calculating the deviation of the data from a linear line. Plotting the density curves of the LOF statistics for each level of difficulty choice allows us to evaluate the ability of differentiating between the difficulty levels and thus detecting the target plot. In , we can see the densities of each of the three difficulty levels. While the LOF statistic provides us a numerical value for discriminating between the difficulty levels, we cannot directly relate this to the perceptual discriminability; it serves primarily as an approximation to ensure that we are testing parameters at several distinct levels of difficulty. Final parameter estimates are shown in . Figure 2.2: Lack of fit statistic density curves 2.4 Lineup Setup Lineup plots were generated by mapping one simulated data set corresponding to difficulty level A to a scatter plot to be identified as the target plot while multiple simulated data sets corresponding to difficulty level B were individually mapped to scatter plots for the null plots. For example, a target plot with simulated data following an increasing exponential curve with obvious curvature is embedded within null plots with simulated data following an increasing exponential trend that is almost linear (i.e. Hard Null - Easy Target). By our constraints, the target plot and null plots will span a similar domain and range. There are a total of six (i.e. \\(3!\\cdot 2!\\)) lineup parameter combinations. Two sets of each lineup parameter combination were simulated (total of 12 test data sets) and plotted on both the linear scale and the log scale (total of 24 test lineup plots). In addition, there are three parameter combinations which generate homogeneous Rorschach lineups, where all panels are from the same distribution. Each participant evaluated one of these lineups, but for simplicity, these evaluations are not described in this paper. 2.5 Study Design Each participant was shown a total of thirteen lineup plots (twelve test lineup plots and one Rorschach lineup plot). Participants were randomly assigned one of the two replicate data sets for each of the six unique lineup parameter combinations. For each assigned test data set, the participant was shown the lineup plot corresponding to both the linear scale and the log scale. For the additional Rorschach lineup plot, participants were randomly assigned one data set shown on either the linear or the log scale. The order of the thirteen lineup plots shown was randomized for each participant. Participants above the age of majority were recruited from Reddits Visualization and Sample Size communities. Since participants recruited on Reddit were not compensated for their time, most participants have an interest in data visualization research. Previous literature suggests that prior mathematical knowledge or experience with exponential data is not associated with the outcome of graphical experiments (VanderPlas &amp; Hofmann, 2015). Participants completed the experiment using a Shiny applet (https://shiny.srvanderplas.com/log-study/). Participants were shown a series of lineup plots and asked to identify the plot that was most different from the others. On each plot, participants were asked to justify their choice and provide their level of confidence in their choice. The goal of this experimental task is to test an individuals ability to perceptually differentiate exponentially increasing trends with differing levels of curvature on both the linear and log scale. 2.6 Results Participant recruitment through Reddit occurred over the course of two weeks during which 58 individuals completed 518 unique test lineup evaluations. Previous studies have found that results do not differ on lineup-related tasks between Reddit and e.g. Amazon Mechanical Turk (VanderPlas &amp; Hofmann, 2017). Participants who completed fewer than 6 lineup evaluations were removed from the study (17 participants, 41 evaluations). The final data set included a total of 41 participants and 477 lineup evaluations. Each plot was evaluated by between 18 and 28 individuals (Mean: 21.77, SD: 2.29). In 67% of the 477 lineup evaluations, participants correctly identified the target panel. Target plot identification was analyzed using the lme4 R package and glmer function (Bates, Mächler, Bolker, &amp; Walker, 2015). Estimates and odds ratio comparisons were calculated using the emmeans R package (Lenth, 2021). Each lineup plot evaluated was assigned a value based on the participant response (correct = 1, not correct = 0). Define \\(Y_{ijkl}\\) to be the event that participant \\(l\\) correctly identifies the target plot for data set \\(k\\) with curvature \\(j\\) plotted on scale \\(i\\). The binary response was analyzed using generalized linear mixed model following a binomial distribution with a logit link function following a row-column blocking design accounting for the variation due to participant and data set respectively as \\[\\begin{equation} \\text{logit }P(Y_{ijk}) = \\eta + \\delta_i + \\gamma_j + \\delta \\gamma_{ij} + s_l + d_k \\end{equation}\\] where We assume that random effects for data set and participant are independent. The analysis of variance table shown in indicate a significant interaction between the curvature combination and scale. Variance due to participant and data set were estimated to be \\(\\sigma^2_{\\text{participant}} = 2.79\\) (s.e. 1.67) and \\(\\sigma^2_{\\text{data}} = 0.44\\) (s.e. 0.66) respectively. On both the log and linear scales, the highest accuracy occurred in lineup plots where the target model and null model had large curvature differences (Easy Null - Hard Target; Hard Null - Easy Target). There is a decrease in accuracy on the linear scale when comparing a target plot with less curvature to null plots with more curvature (Easy Null - Medium Target; Medium Null - Hard Target). Best, Smith, &amp; Stubbs (2007) found that accuracy of identifying the correct curve type was higher when nonlinear trends were presented indicating that it is hard to say something is linear (i.e. something has less curvature), but easy to say that it is not linear; our results concur with this observation. Overall, there are no significant differences in accuracy between curvature combinations when data is presented on a log scale indicating participants were consistent in their success of identifying the target panel on the log scale. displays the estimated (log) odds ratio of successfully identifying the target panel on the log scale compared to the linear scale. The choice of scale has no impact if curvature differences are large (Hard Null - Easy Target; Easy Null - Hard Target). However, presenting data on the log scale makes us more sensitive to slight changes in curvature (Medium Null - Easy Target; Medium Null - Hard Target; Easy Null - Medium Target). An exception occurs when identifying a plot with curvature embedded in null plots close to a linear trend (Hard Null - Medium Target), again supporting the claim that it is easy to identify a curve in a bunch of lines but much harder to identify a line in a bunch of curves (Best, Smith, &amp; Stubbs, 2007). Figure 2.3: Lineups log(odds) results 2.7 Discussion and Conclusion The overall goal of this chapter is to provide basic research to support the principles used to guide design decisions in scientific visualizations of exponential data. In this study, we explore the use of linear and log scales to determine whether our ability to notice differences in exponentially increasing trends is impacted by the choice of scale. Our results indicated that when there was a large difference in curvature between the target plot and null plots, the choice of scale had no impact and participants accurately differentiated between the two curves on both the linear and log scale. However, displaying exponentially increasing data on a log scale improved the accuracy of differentiating between models with slight curvature differences. An exception occurred when identifying a plot with curvature embedded in surrounding plots closely relating to a linear trend, indicating that it is easy to identify a curve in a group of lines but much harder to identify a line in a group of curves. The use of visual inference to identify these guidelines suggests that there are advantages to log scales when differences are subtle. What remains to be seen is whether there are cognitive disadvantages to log scales: do log scales make it harder to make use of graphical information? "],["3-youdrawit.html", "CHAPTER 3 Prediction with you draw it 3.1 Introduction 3.2 Study Design 3.3 Eye Fitting Straight Lines in the Modern Era 3.4 Prediction of Exponential Trends 3.5 Discussion and Conclusion", " CHAPTER 3 Prediction with you draw it 3.1 Introduction In Chapter 2, a base foundation for future exploration of the use of log scales was established by evaluating participants ability to identify differences in charts through the use of lineups. This did not require that participants were able to understand exponential growth, identify log scales, or have any mathematical training; instead, it simply tested the change in perceptual sensitivity resulting from visualization choices. In order to determine whether there are cognitive disadvantages to log scales, we utilize interactive graphics to test an individuals ability to make predictions for exponentially increasing data. In this study, participants are asked to draw a line using their computer mouse through the exponentially increasing trend shown on both the log and linear scale. 3.1.1 Past Methodology Initial studies in the 20th century explored the use of fitting lines by eye through a set of points (Finney, 1951; Mosteller, Siegel, Trapido, &amp; Youtz, 1981). Common methods of fitting trends by eye involve maneuvering a string, black thread, or ruler until the fit is suitable, then drawing the line. In Finney (1951), it was of interest to determine the effect of stopping iterative maximum likelihood calculations after one iteration. Twenty-one scientists were recruited via postal mail and asked to rule two lines in order to judge by eye the positions for a pair of parallel probit regression lines in a biological assay. Results indicated that one cycle of iterations was sufficient based on the starting values provided by eye from the participants. Thirty years later, Mosteller, Siegel, Trapido, &amp; Youtz (1981), sought to understand the properties of least squares and other computed lines by establishing one systematic method of fitting lines by eye. The authors recruited 153 graduate students and post doctoral researchers in Introductory Biostatistics. Participants were asked to fit lines by eye to four sets of points using an 8.5 x 11 inch transparency with a straight line etched completely across the middle. A latin square design with packets of the set of points stapled together in four different orders was used in order to determine if there is an effect of order of presentation. It was found that order of presentation had no effect and that participants tended to fit the slope of the first principal component over the slope of the least squares regression line. In 2015, the New York Times introduced an interactive feature, called You Draw It (Aisch, Cox, &amp; Quealy, 2015; Buchanan, Park, &amp; Pearce, 2017; Katz, 2017). Readers are asked to input their own assumptions about various metrics and learning how these asumptions relate to reality. The NY Times team utilizes Data Driven Documents (D3) that allows readers to predict these metrics through the use of drawing a line on their computer screen with their computer mouse. 3.1.2 Data Driven Documents Major news and research organizations such as the NY Times, FiveThirtyEight, Washing Post, and the Pew Research Center create and customize graphics with Data Driven Documents. Data Driven Documents (D3) is an open-source JavaScript based graphing framework created by Mike Bostock during his time working on graphics at the NY Times. For readers familiar with R, it is notable to consider D3 in JavaScript equivalent to the ggplot2 package in R. Similar to geometric objects and style choices in ggplot2, the grammar of D3 also includes elements such as circles, paths, and rectangles with choices of attributes and styles such as color and size. Data Driven Documents depend on Extensible Markup Langage (XML) to generate graphics and images by binding objects and layers to the plotting area as Scalable Vector Graphics (SVG) in order to preserve the shapes rather than the pixels . Advantages of using D3 include animation and allowing for movement and user interaction such as hovering, clicking, and brushing. Figure 3.1: SVG vs Raster A challenge of working with D3 is the environment necessary to display the graphics and images. The r2d3 package in R provides an efficient integration of D3 visuals and R by displaying them in familiar HTML output formats such as RMarkdown or Shiny applications (Luraschi &amp; Allaire, 2018). The creator of the graphic applies D3.js code to visualize data which has previously been processed within an R setting. r2d3(data = data, script = &quot;d3-source-code.js&quot;, d3_version = &quot;5&quot;) The example R code illustrates the structure of the r2d3 function in R which includes specification of a data frame in R (converted to a JSON file), the D3.js source code file, and the D3 version that accompanies the source code. A default SVG container for layering elements is then generated by the r2d3 function which renders the plot using the source code. Appendix A outlines the development of the you draw it study interactive plots through the use of r2d3 with R shiny applications. 3.2 Study Design This chapter first aims to establish the you draw it method as a tool for measuring predictions of trends; then this method will be used to test an individuals ability to make predictions for exponentially increasing data on both the log and linear scales, referred to as Prediction of Exponential Trends. Mosteller, Siegel, Trapido, &amp; Youtz (1981) was replicated as part of the study data collection in order to validate the you draw it method, this will be referred to as Eye Fitting Straight Lines in the Modern Era. Data for both the Eye Fitting Straight Lines in the Modern Era and Prediction of Exponential Trends studies were collected in conjunction with one another over the same study participant sample. For each individual, a total of 6 data sets - 4 Eye Fitting Straight Lines in the Modern Era and 2 Prediction of Exponential Trends - are generated for each individual the start of the experiment. The 2 data sets corresponding to to the data used in the Prediction of Exponential Trends are then plotted a total of 4 times each by truncating the points at both 50% and 75% of the domain as well as on both the log and linear scales for a total of 8 task plots. Participants in the study are first shown 2 you draw it plot practice plots followed by 12 you draw it task plots. The order of all 12 task plots were randomly assigned for each individual in a completely randomized design where users saw the 4 task plots from the Eye Fitting Straight Lines in the Modern Era simulated data interspersed with the 8 task plots from the Prediction of Exponential Trends simulated data. 3.3 Eye Fitting Straight Lines in the Modern Era 3.3.1 Data Simulation All data processing was conducted in R before being passed to the D3.js code. Both studies, Eye Fitting Straight Lines in the Modern Era and Prediction of Exponential Trends, generate 30 points for \\(x\\in [0, 20]\\) based on a specific model (linear regression and exponential regression respectively) and selected coefficient and variance parameters. The corresponding model is then fit to the simulated points the obtain the best fit line and estimated values every 0.25 increments across the domain. Each function then outputs a list of point data and line data both indicating the parameter identification, x value, and y value. Aspect ratio = 1; yrange = range(all eye fitting data) range(1.1, 1.1) Algorithm: Eye Fitting Straight Lines in the Modern Era Data Generation* In parameters: y_xbar, slope, sigma, N = 30, xmin, xmax, xby = 0.25 Out: data list of point data and line data Randomly select and jitter N = 30 x-values along the domain. Determine y-intercept at x = 0 from the provided slope and y-intercept at the mean of x (y_xbar) using point-slope form: y - y_xbar = m(x-xbar) Generate good errors based on N(0,sigma). Set constraint of the mean of the first N/3 = 10 errors less than |2*sigma| Simulate point data based on \\(y = yintercept + slope*x + error\\) Obtain ordinary least squares regression coefficients: lm(y ~ x, data = point_data) Simulate least squares regression line data: y = yintercepthat + slopehat*x Output data list of point data and line data 3.3.2 Parameter Selection Parameter combinations were selected to simulate data that replicates the data sets (S, F, V, N) in (Mosteller, Siegel, Trapido, &amp; Youtz, 1981). One-way ANOVA with 4 treatments S: positive slope, low variance (y_xbar = 3.88, slope = 0.66, sigma = 1.3, xrange = (0,20)) F: positive slope, high variance (y_xbar = 3.9, slope = 0.66, sigma = 1.98, xrange = (0,20)) V: steep positive slope (y_xbar = 3.89, slope = 1.98, sigma = 1.5, xrange = (4,18)) N: negative slope, high variance (y_xbar = 4.11, slope = -0.70, sigma = 2.5, xrange = (0,20)) 3.3.3 Results To calculate the first principal component fit: https://benediktehinger.de/blog/science/scatterplots-regression-lines-and-the-first-principal-component/ Subset the simulated point data for a particular participant and parameter (S, F, V, N). Fit a principal component over the x and y vectors from the point data (note these are the points show in the plot below). Use prcomp. Call this pca.mod Obtain the slope and intercept: Using the pca.mod rotations we obatin the slope as: \\(\\text{pca.slope} = \\frac{\\text{pca.mod rotation}[y,PC1]}{\\text{pca.mod rotation}[x,PC1]}\\) Using point-slope form we obtain the intercept as: Obtain ypca values by: \\(y_{pca} = \\text{pca.slope} \\cdot x_{\\text{feedback data}} + \\text{pca.intercept}\\) Figure 3.2: Eye Fitting Straight Lines in the Modern Era Example Treatments: parm_ID (S, F, V, N) x (0, 20) Response: raw residuals residualols = ydrawn - yols, denoted \\(e_{ols}\\) residualpca = ydrawn - ypca, denoted \\(e_{pca}\\) Experimental Design: Each participant saw each of the 4 plots (new simulated data for each) OLS GAMM Model: \\[y_{drawn} - y_{ols} = e_{ij,ols} = \\left[\\gamma_0 + \\alpha_i\\right] + \\left[s_1(x_{ij}) + s_{2i}(x_{ij}) \\right] + p_{j} + \\epsilon_{ij}\\] \\(\\gamma_0\\) is the overall intercept \\(\\alpha_i\\) is the effect of the parameter combination on the intercept (i.e. how much the intercept adjusts for each parameter combo) \\(s_1\\) is the overall spline equation for x \\(s_{2i}\\) is the adjustment to the spline equation for each parameter combination (think unequal slopes) \\(p_{j} \\sim N(0, \\sigma^2_{participant})\\) is the participant error due to participant variation \\(\\epsilon_{ij} \\sim N(0, \\sigma^2)\\) is the residual error. PCA GAMM Model: \\[y_{drawn} - y_{pca} = e_{ij,pca} = \\left[\\gamma_0 + \\alpha_i\\right] + \\left[s_1(x_{ij}) + s_{2i}(x_{ij}) \\right] + p_{j} + \\epsilon_{ij}\\] \\(\\gamma_0\\) is the overall intercept \\(\\alpha_i\\) is the effect of the parameter combination on the intercept (i.e. how much the intercept adjusts for each parameter combo) \\(s_1\\) is the overall spline equation for x \\(s_{2i}\\) is the adjustment to the spline equation for each parameter combination (think unequal slopes) \\(p_{j} \\sim N(0, \\sigma^2_{participant})\\) is the participant error due to participant variation \\(\\epsilon_{ij} \\sim N(0, \\sigma^2)\\) is the residual error. Figure 3.3: Eye Fitting Straight Lines in the Modern Era Simulated Data Example Figure 3.4: Eye Fitting Straight Lines in the Modern Era Residual Models Figure 3.5: Eye Fitting Straight Lines in the Modern Era Sum of Squares Results 3.4 Prediction of Exponential Trends 3.4.1 Data Simulation aspect ratio = 1; linear = T/F; start drawing at x = 10; xrange = (0,20), yrange = range(points)*c(0.5,2) Algorithm 3.2: Exponential Data Generation* In parameters: beta, sd, points_choice = \"partial\", points_end_scale, N = 30, xmin = 0, xmax = 20, xby = 0.25 Out: data list of point data and line data Randomly select and jitter N = 30 x-values along the domain. Generate good errors based on N(0,sd). Set constraint of the mean of the first N/3 = 10 errors less than |2*sd| Simulate point data based on: y = exp(x*beta + errorVals) Obtain starting value for beta: lm(log(y) ~ x, data = point_data) Use NLS to fit a better line to the point data: nls(y ~ exp(x*beta), data = point_data, ...) Simulate nonlinear least squares line data: y = exp(x*betahat) Output data list of point data and line data 3.4.2 Parameter Selection Visit You Draw It Development - parameter selection for examples. Exponential (Linear/Log): 2 x 2 x 2 Factorial Beta: 0.1 (sd. 0.09); 0.23 (0.25) Points End: 0.5; 0.75 Scale: Linear; Log 3.4.3 Results advocate smoothing of scatterplots to assist in detecting the shape of the point cloud in situations where the error in the data is substantial, or where the density of points changes along the abscissa Cleveland &amp; McGill (1984) Twitter/Reddit/Direct Email Pilot Study (05/03/2021): Exponential Prediction https://shiny.srvanderplas.com/you-draw-it/ Twitter/Reddit/Direct Email Pilot Study (05/03/2021): Exponential Prediction 3.5 Discussion and Conclusion "],["4-estimation.html", "CHAPTER 4 Numerical Translation and Estimation 4.1 Introduction", " CHAPTER 4 Numerical Translation and Estimation 4.1 Introduction Such complex inferential processes involve quantitatively transforming the information in the display (e.g., mentally transforming from a linear to logarithmic scale or calculating the difference between two or more data points; Cleveland, 1984, 1985). Estimation Questions: Timmers &amp; Wagenaar (1977) If nothing will stop this decreasing trend, what is your prediction for 1980? (Group 1) When will the process reach  (a certain level)? (Group 2) "],["5-conclusion.html", "CHAPTER 5 Conclusion", " CHAPTER 5 Conclusion If you feel it necessary to include an appendix, it goes here. --> "],["A-youdrawit-with-shiny.html", "A You Draw It Setup with Shiny A.1 You draw it D3.js source code", " A You Draw It Setup with Shiny Interactive plots for the study were created using the r2d3 package and integrating D3 source code with an R shiny application. In this section I outline the data simulation process conducted in R, constructing interactive plots with D3.js code and R shiny A.1 You draw it D3.js source code Figure A.1: Interactive plot development # Define variable margins const margin = {left: 55, right: 10, top: options.title ? 40: 10, bottom: options.title? 25: 55}; # Define variable default line attributes const default_line_attrs = Object.assign({ fill: &quot;none&quot;, stroke: options.data_line_color || &#39;steelblue&#39;, strokeWidth: 4, strokeLinejoin: &quot;round&quot;, strokeLinecap: &quot;round&quot;, }, options.line_style); # defines a changing variable called state let state = Object.assign({ line_data: data.line_data, point_data: data.point_data, svg: svg.append(&#39;g&#39;).translate([margin.left, margin.top]).attr(&quot;class&quot;, &quot;wrapper&quot;), w: height*options.aspect_ratio - margin.left - margin.right, h: height - margin.top - margin.bottom, }, options); # To distinguish between code that runs at initialization-time only and # code that runs when data changes, organize your code so that the code # which responds to data changes is contained within the r2d3.onRender() r2d3.onRender(function(data, svg, width, height, options) { state.line_data = data.line_data; state.point_data = data.point_data; state = Object.assign(state, options); state.options = options; state.w = height*options.aspect_ratio; start_drawer(state); }); # An explicit resize handler r2d3.onResize(function(width, height, options) { state.w = height*state.options.aspect_ratio - margin.left - margin.right; state.h = height - margin.top - margin.bottom; start_drawer(state, reset = false); }); # Main function that draws current state of viz function start_drawer(state, reset = true){ const scales = setup_scales(state); if(!state.free_draw){ draw_true_line(state, scales, state.draw_start); } # Cover hidden portion with a rectangle (disabled) # const line_hider = setup_line_hider(state.svg, state.draw_start, scales); # if we reset (these are points that can be drawn) remove what user has drawn. if(reset){ state.drawable_points = setup_drawable_points(state); } # if we have points, we draw user&#39;s line. draw_user_line(state, scales); draw_rectangle(state, scales); draw_finished_line(state, scales, state.draw_start); # draw points for initial portion if(state.points != &quot;none&quot;){ draw_points(state, scales); } # invert from pixle to data scale when they draw their points const on_drag = function(){ const drag_x = scales.x.invert(d3.event.x); const drag_y = scales.y.invert(d3.event.y); fill_in_closest_point(state, drag_x, drag_y); draw_user_line(state, scales); draw_rectangle(state, scales); }; # line_status is set by draw watcher - get user status line # if some points missing - in progress # complete line - done const on_end = function(){ # Check if all points are drawn so we can reveal line const line_status = get_user_line_status(state); if(line_status === &#39;done&#39;){ # User has completed line drawing # if(state.show_finished) line_hider.reveal(); # if(!state.free_draw) line_hider.reveal(); if(state.show_finished){ draw_finished_line(state, scales, state.draw_start); } if(state.shiny_message_loc){ # Make sure shiny is available before sending message if(typeof Shiny !== &#39;undefined&#39;){ # Send drawn points off to server Shiny.onInputChange( state.shiny_message_loc, state.drawable_points.map(d =&gt; d.y) ); } else { console.log(`Sending message to ${state.shiny_message_loc}`); } } } }; setup_draw_watcher(state.svg, scales, on_drag, on_end); # Do we have a title? if(state.title){ state.svg.append(&#39;text&#39;) .at({ y: -margin.top/2, dominantBaseline: &#39;middle&#39;, fontSize: &#39;1.5rem&#39;, }) .style(&#39;font-family&#39;, system_font) .text(state.title); } } function setup_drawable_points({line_data, free_draw, draw_start}){ if(free_draw){ return line_data.map(d =&gt; ({x: d.x, y: null})); } else { return line_data .filter(d =&gt; d.x &gt;= draw_start) .map((d,i) =&gt; ({ x: d.x, y: i === 0 ? d.y: null })); } } function get_user_line_status({drawable_points, free_draw}){ const num_points = drawable_points.length; const num_filled = d3.sum(drawable_points.map(d =&gt; d.y === null ? 0: 1)); const num_starting_filled = free_draw ? 0: 1; if(num_filled === num_starting_filled){ return &#39;unstarted&#39;; } else if(num_points === num_filled){ return &#39;done&#39;; } else { return &#39;in_progress&#39;; } } # Draw visable portion of line function draw_true_line({svg, line_data, draw_start}, scales){ var df = line_data.filter(function(d){ return d.x&lt;=draw_start}) state.svg.selectAppend(&quot;path.shown_line&quot;) .datum(df) .at(default_line_attrs) .attr(&quot;d&quot;, scales.line_drawer); } function draw_points({svg, point_data, points_end, points}, scales){ if(points == &quot;partial&quot;){ var df = point_data.filter(function(d){return (d.x&lt;=points_end)}); } else { var df = point_data; } const dots = state.svg.selectAll(&quot;circle&quot;).data(df) dots .enter().append(&quot;circle&quot;) .merge(dots) .attr(&quot;cx&quot;, d =&gt; scales.x(d.x)) .attr(&quot;cy&quot;, d =&gt; scales.y(d.y)) .attr(&quot;r&quot;, 2) .style(&quot;fill&quot;, &quot;black&quot;) .style(&quot;opacity&quot;, 0.8) .style(&quot;stroke&quot;, &quot;black&quot;) } function draw_rectangle({svg, drawable_points, line_data, draw_start, width, height, free_draw, x_by}, scales){ if(get_user_line_status(state) === &#39;unstarted&#39;){ if(free_draw){ var xmin = line_data[0].x var len = line_data.length - 1 var xmax = line_data[len].x var drawSpace_start = scales.x(xmin) var drawSpace_end = scales.x(xmax) } else { var drawSpace_start = scales.x(draw_start) var drawSpace_end = state.w } } else { if(get_user_line_status(state) === &#39;done&#39;){ var drawSpace_start = scales.x(100) var drawSpace_end = scales.x(110) } else { var df = drawable_points.filter(function(d){return (d.y === null)}); var xmin = df[0].x - x_by; var len = line_data.length - 1 var xmax = line_data[len].x var drawSpace_start = scales.x(xmin) var drawSpace_end = scales.x(xmax) } } const draw_region = state.svg.selectAppend(&quot;rect&quot;); draw_region .attr(&quot;x&quot;, drawSpace_start) .attr(&quot;width&quot;,drawSpace_end - drawSpace_start) .attr(&quot;y&quot;, 0) .attr(&quot;height&quot;, state.h) //.style(&quot;fill&quot;, &quot;#e0f3f3&quot;) .style(&quot;fill-opacity&quot;, 0.4) .style(&quot;fill&quot;, &quot;rgba(255,255,0,.8)&quot;) } function draw_user_line(state, scales){ const {svg, drawable_points, drawn_line_color} = state; const user_line = state.svg.selectAppend(&quot;path.user_line&quot;); # Only draw line if there&#39;s something to draw. if(get_user_line_status(state) === &#39;unstarted&#39;){ user_line.remove(); return; } # Draws the points the user is drawing with their mouse user_line .datum(drawable_points) .at(default_line_attrs) .attr(&#39;stroke&#39;, drawn_line_color) .attr(&quot;d&quot;, scales.line_drawer) .style(&quot;stroke-dasharray&quot;, (&quot;1, 7&quot;)); } function draw_finished_line({svg, line_data, draw_start, free_draw}, scales){ if(!free_draw){ var df = line_data.filter(function(d){ return d.x &gt;= draw_start}) } else { var df = line_data } const finished_line = state.svg.selectAppend(&quot;path.finished_line&quot;) # Only draw line if there&#39;s something to draw. if(get_user_line_status(state) === &#39;unstarted&#39;){ finished_line.remove(); return; } finished_line .datum(df) .at(default_line_attrs) .attr(&quot;d&quot;, scales.line_drawer) .attr(&quot;opacity&quot;, 0.5) } # from state we need drawable_points function fill_in_closest_point({drawable_points, pin_start, free_draw}, drag_x, drag_y){ # find closest point on data to draw let last_dist = Infinity; let current_dist; # If nothing triggers break statement than closest point is last point let closest_index = drawable_points.length - 1; const starting_index = free_draw ? 0 : (pin_start ? 1: 0); # for loop to check where closest point to where I am for(i = starting_index; i &lt; drawable_points.length; i++){ current_dist = Math.abs(drawable_points[i].x - drag_x); # If distances start going up we&#39;ve passed the closest point if(last_dist - current_dist &lt; 0) { closest_index = i - 1; break; } last_dist = current_dist; } drawable_points[closest_index].y = drag_y; } function setup_draw_watcher(svg, scales, on_drag, on_end){ svg.selectAppend(&#39;rect.drag_watcher&#39;) .at({ height: scales.y.range()[0], width: scales.x.range()[1], fill: &#39;grey&#39;, fillOpacity: 0, }) .call( d3.drag() .on(&quot;drag&quot;, on_drag) .on(&quot;end&quot;, on_end) ); } function add_axis_label(label, y_axis = true){ const bump_axis = y_axis ? &#39;x&#39;: &#39;y&#39;; const axis_label_style = { [bump_axis]: bump_axis == &#39;y&#39; ? 3: -2, textAnchor: &#39;end&#39;, fontWeight: &#39;500&#39;, fontSize: &#39;0.9rem&#39;, }; return g =&gt; { let last_tick = g.select(&quot;.tick:last-of-type&quot;); const no_ticks = last_tick.empty(); if(no_ticks){ last_tick = g.append(&#39;g&#39;) .attr(&#39;class&#39;, &#39;tick&#39;); } last_tick.select(&quot;line&quot;).remove(); last_tick.selectAppend(&quot;text&quot;) .at(axis_label_style) .html(label); }; } # Setup scales for visualization function setup_scales(state){ # multi-assign: x_range, etc. coming from options const {w, h, line_data, x_range, y_range, x_name, y_name, linear} = state; # convert x from data scale to pixle scale const x = d3.scaleLinear() .domain(x_range || d3.extent(line_data, d =&gt; d.x)) .range([0, w]); //console.log(linear); if (linear == &#39;true&#39;) { //console.log(&#39;in linear block&#39;); # converts from data linear scale to pixle scale var y = d3.scaleLinear() .domain(y_range || d3.extent(line_data, d =&gt; d.y)) .range([h, 0]); } else { //console.log(&#39;in log block&#39;); # converts from data log scale to pixle scale var y = d3.scaleLog() .domain(y_range || d3.extent(line_data, d =&gt; d.y)) .range([h, 0]).base(10); } const xAxis = d3.axisBottom().scale(x).tickSizeOuter(0); const yAxis = d3.axisLeft().scale(y).tickFormat(d3.format(&quot;.4&quot;)).tickSizeOuter(0); const xAxisGrid = d3.axisBottom().scale(x).tickSize(-h).tickFormat(&#39;&#39;); const yAxisGrid = d3.axisLeft().scale(y).tickSize(-w).tickFormat(&#39;&#39;); state.svg.selectAll(&quot;g.x_grid&quot;).remove() state.svg.selectAll(&quot;g.y_grid&quot;).remove() # could call axis-grid &quot;fred&quot; state.svg.selectAll(&quot;g.axis-grid&quot;).remove() state.svg.selectAll(&quot;path.shown_line&quot;).remove() state.svg.selectAll(&quot;circle&quot;).remove() state.svg.selectAppend(&quot;g.x_grid&quot;) .attr(&#39;class&#39;, &#39;x axis-grid&#39;) .translate([0,h]) .call(xAxisGrid); state.svg.selectAppend(&quot;g.y_grid&quot;) .attr(&#39;class&#39;, &#39;y axis-grid&#39;) .call(yAxisGrid); state.svg.selectAll(&quot;.axis-grid .tick&quot;) .style(&quot;stroke&quot;, &quot;light-grey&quot;) .style(&quot;opacity&quot;, &quot;.3&quot;); state.svg.selectAppend(&quot;g.x_axis&quot;) .translate([0,h]) .call(xAxis); state.svg.selectAppend(&quot;g.y_axis&quot;) .call(yAxis); const line_drawer = d3.line() .defined(d =&gt; d.y !== null) .x(d =&gt; x(d.x)) .y(d =&gt; y(d.y)); return { x, y, line_drawer, }; } "],["references.html", "References", " References Aisch, G., Cox, A., &amp; Quealy, K. (2015, May). You draw it: How family income predicts childrens college chances. The New York Times. The New York Times. Retrieved from https://www.nytimes.com/interactive/2015/05/28/upshot/you-draw-it-how-family-income-affects-childrens-college-chances.html Amer, T. (2005). Bias due to visual illusion in the graphical presentation of accounting information. Journal of Information Systems, 19(1), 118. Bates, D., Mächler, M., Bolker, B., &amp; Walker, S. (2015). Fitting linear mixed-effects models using lme4. Journal of Statistical Software, 67(1), 148. http://doi.org/10.18637/jss.v067.i01 Bavel, J. J. V., Baicker, K., Boggio, P. S., Capraro, V., Cichocka, A., Cikara, M.,  Willer, R. (2020). Using social and behavioural science to support COVID-19 pandemic response. Nature Human Behaviour, 4(5), 460471. http://doi.org/10.1038/s41562-020-0884-z Best, L. A., Smith, L. D., &amp; Stubbs, D. A. (2007). Perception of Linear and Nonlinear Trends: Using Slope and Curvature Information to Make Trend Discriminations. Perceptual and Motor Skills, 104(3), 707721. http://doi.org/10.2466/pms.104.3.707-721 Broersma, H., &amp; Molenaar, I. (1985). Graphical perception of distributional aspects of data. Computational Statistics Quarterly, 2(1), 5372. Buchanan, L., Park, H., &amp; Pearce, A. (2017, January). You draw it: What got better or worse during obamas presidency. The New York Times. The New York Times. Retrieved from https://www.nytimes.com/interactive/2017/01/15/us/politics/you-draw-obama-legacy.html Buja, A., Cook, D., Hofmann, H., Lawrence, M., Lee, E.-K., Swayne, D. F., &amp; Wickham, H. (2009). Statistical inference for exploratory data analysis and model diagnostics. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 367(1906), 43614383. http://doi.org/10.1098/rsta.2009.0120 Burn-Murdoch, J., Nevitt, C., Tilford, C., Rininsland, A., Kao, J. S., Elliott, O.,  Stabe, M. (2020). Coronavirus tracked: Has the epidemic peaked near you? Coronavirus chart: see how your country compares. Financial Times. Retrieved from https://ig.ft.com/coronavirus-chart/?areas=eur Cleveland, W. S., &amp; McGill, R. (1984). Graphical Perception: Theory, Experimentation, and Application to the Development of Graphical Methods. Retrieved from http://euclid.psych.yorku.ca/www/psy6135/papers/ClevelandMcGill1984.pdf Cleveland, W. S., &amp; McGill, R. (1985). Graphical Perception and Graphical Methods for Analyzing Scientific Data. Science, New Series, 229(4716), 828833. Retrieved from http://www.jstor.org/stable/1695272 Croxton, F. E., &amp; Stein, H. (1932). Graphic comparisons by bars, squares, circles, and cubes. Journal of the American Statistical Association, 27(177), 5460. Croxton, F. E., &amp; Stryker, R. E. (1927). Bar charts versus circle diagrams. Journal of the American Statistical Association, 22(160), 473482. Dehaene, S., Izard, V., Spelke, E., &amp; Pica, P. (2008). Log or linear? Distinct intuitions of the number scale in western and amazonian indigene cultures. Science, 320(5880), 12171220. Dunn, R. (1988). Framed rectangle charts or statistical maps with shading: An experiment in graphical perception. The American Statistician, 42(2), 123129. Eells, W. C. (1926). The relative merits of circles and bars for representing component parts. Journal of the American Statistical Association, 21(154), 119132. Fagen-Ulmschneider, W. (2020). 91-DIVOC. An interactive visualization of COVID-19. University of Illinois. Retrieved from https://91-divoc.com/pages/covid-visualization/ Finney, D. J. (1951). Subjective Judgment in Statistical Analysis: An Experimental Study. Journal of the Royal Statistical Society. Series B (Methodological), 13(2), 284297. Retrieved from https://www.jstor.org/stable/2984070 GmbH, D. (2020, July). Youve informed the public with visualizations about the coronavirus. Thank you. Chartable. Retrieved from https://blog.datawrapper.de/datawrapper-effect-corona/index.html Goldstein, E. B., &amp; Brockmole, J. R. (2017). Sensation and Perception, 480. Gordon, I., &amp; Finch, S. (2015). Statistician Heal Thyself: Have We Lost the Plot? Journal of Computational and Graphical Statistics, 24(4), 12101229. http://doi.org/10.1080/10618600.2014.989324 Green, T. M., &amp; Fisher, B. (2009). The personal equation of complex individual cognition during visual interface interaction. In Workshop on human-computer interaction and visualization (pp. 3857). Springer. Haemer, K. W., &amp; Kelley, T. L. (1949). Presentation Problems: Suiting the Chart to the Audience: Common Graphic Devices Classified According to Ease of Reading. The American Statistician, 3(5), 1111. http://doi.org/10.1080/00031305.1949.10501608 Heckler, A. F., Mikula, B., &amp; Rosenblatt, R. (2013). Student accuracy in reading logarithmic plots: The problem and how to fix it. In 2013 IEEE Frontiers in Education Conference (FIE) (pp. 10661071). http://doi.org/10.1109/FIE.2013.6684990 Hofmann, H., Follett, L., Majumder, M., &amp; Cook, D. (2012). Graphical Tests for Power Comparison of Competing Designs. IEEE Transactions on Visualization and Computer Graphics, 18(12), 24412448. http://doi.org/10.1109/TVCG.2012.230 Jones, G. V. (1977). Polynomial perception of exponential growth. Perception &amp; Psychophysics, 21(2), 197198. http://doi.org/10.3758/BF03198726 Katz, J. (2017, April). You draw it: Just how bad is the drug overdose epidemic? The New York Times. The New York Times. Retrieved from https://www.nytimes.com/interactive/2017/04/14/upshot/drug-overdose-epidemic-you-draw-it.html Lenth, R. V. (2021). Emmeans: Estimated marginal means, aka least-squares means. Retrieved from https://CRAN.R-project.org/package=emmeans Lewandowsky, S., &amp; Spence, I. (1989). The Perception of Statistical Graphs. Sociological Methods &amp; Research, 18(2-3), 200242. http://doi.org/10.1177/0049124189018002002 Loy, A., Follett, L., &amp; Hofmann, H. (2016). Variations of q  q Plots: The Power of Our Eyes! The American Statistician, 70(2), 202214. http://doi.org/10.1080/00031305.2015.1077728 Loy, A., Hofmann, H., &amp; Cook, D. (2017). Model Choice and Diagnostics for Linear Mixed-Effects Models Using Statistics on Street Corners. Journal of Computational and Graphical Statistics, 26(3), 478492. http://doi.org/10.1080/10618600.2017.1330207 Luraschi, J., &amp; Allaire, J. (2018). r2d3: Interface to D3 visualizations. Retrieved from https://CRAN.R-project.org/package=r2d3 Mackinnon, A. J., &amp; Wearing, A. J. (1991). Feedback and the forecasting of exponential change. Acta Psychologica, 76(2), 177191. http://doi.org/10.1016/0001-6918(91)90045-2 Majumder, M., Hofmann, H., &amp; Cook, D. (2013). Validation of Visual Statistical Inference, Applied to Linear Models. Journal of the American Statistical Association, 108(503), 942956. http://doi.org/10.1080/01621459.2013.808157 Menge, D. N. L., MacPherson, A. C., Bytnerowicz, T. A., Quebbeman, A. W., Schwartz, N. B., Taylor, B. N., &amp; Wolf, A. A. (2018). Logarithmic scales in ecological data presentation may cause misinterpretation. Nature Ecology &amp; Evolution, 2(9), 13931402. http://doi.org/10.1038/s41559-018-0610-7 Mosteller, F., Siegel, A. F., Trapido, E., &amp; Youtz, C. (1981). Eye Fitting Straight Lines. The American Statistician, 35(3), 150152. http://doi.org/10.1080/00031305.1981.10479335 Munroe, R. (2005). Log scale. xkcd: A webcomic of romance, sarcasm, math, and language. Creative Commons. Retrieved from https://xkcd.com/1162/ Nieder, A., &amp; Miller, E. K. (2003). Coding of cognitive magnitude: Compressed scaling of numerical information in the primate prefrontal cortex. Neuron, 37(1), 149157. Peterson, L. V., &amp; Schramm, W. (1954). How accurately are different kinds of graphs read? Audio Visual Communication Review, 178189. Romano, A., Sotis, C., Dominioni, G., &amp; Guidi, S. (2020). The Scale of COVID-19 Graphs Affects Understanding, Attitudes, and Policy Preferences ({SSRN} {Scholarly} {Paper} No. ID 3588511). Rochester, NY: Social Science Research Network. Retrieved from https://papers.ssrn.com/abstract=3588511 Shah, P., &amp; Carpenter, P. A. (1995). Conceptual limitations in comprehending line graphs. Journal of Experimental Psychology: General, 124(1), 43. Shah, P., Mayer, R. E., &amp; Hegarty, M. (1999). Graphs as aids to knowledge construction: Signaling techniques for guiding the process of graph comprehension. Journal of Educational Psychology, 91(4), 690. Siegler, R. S., &amp; Braithwaite, D. W. (2017). Numerical Development. Annual Review of Psychology, 68(1), 187213. http://doi.org/10.1146/annurev-psych-010416-044101 Spence, I. (1990). Visual psychophysics of simple graphical elements. Journal of Experimental Psychology: Human Perception and Performance, 16(4), 683692. http://doi.org/10.1037/0096-1523.16.4.683 Sun, J. Z., Wang, G. I., Goyal, V. K., &amp; Varshney, L. R. (2012). A framework for Bayesian optimality of psychophysical laws. Journal of Mathematical Psychology, 56(6), 495501. http://doi.org/10.1016/j.jmp.2012.08.002 Tan, J. K. (1994). Human processing of two-dimensional graphics: Information-volume concepts and effects in graph-task fit anchoring frameworks. International Journal of Human-Computer Interaction, 6(4), 414456. Teghtsoonian, M. (1965). The judgment of size. The American Journal of Psychology, 78(3), 392402. Timmers, H., &amp; Wagenaar, W. A. (1977). Inverse statistics and misperception of exponential growth. Perception &amp; Psychophysics, 21(6), 558562. http://doi.org/10.3758/BF03198737 Unwin, A. (2020). Why is Data Visualization Important? What is Important in Data Visualization? Harvard Data Science Review. http://doi.org/10.1162/99608f92.8ae4d525 Uri, G., &amp; Haemer, K. (1948). Presentation Problems: Graphic Representation for Multiple Linear Correlation. The American Statistician, 2(6), 1919. http://doi.org/10.1080/00031305.1948.10483416 Vanderplas, S., Cook, D., &amp; Hofmann, H. (2020). Testing Statistical Charts: What Makes a Good Graph? Annual Review of Statistics and Its Application, 7(1), 6188. http://doi.org/10.1146/annurev-statistics-031219-041252 VanderPlas, S., &amp; Hofmann, H. (2015). Spatial reasoning and data displays. IEEE Transactions on Visualization and Computer Graphics, 22(1), 459468. VanderPlas, S., &amp; Hofmann, H. (2017). Clusters Beat Trend!? Testing Feature Hierarchy in Statistical Graphics. Journal of Computational and Graphical Statistics, 26(2), 231242. http://doi.org/10.1080/10618600.2016.1209116 Varshney, L. R., &amp; Sun, J. Z. (2013). Why do we perceive logarithmically? Significance, 10(1), 2831. http://doi.org/10.1111/j.1740-9713.2013.00636.x Wagenaar, W. A., &amp; Sagaria, S. D. (1975). Misperception of exponential growth. Perception &amp; Psychophysics, 18(6), 416422. http://doi.org/10.3758/BF03204114 Wickham, Hadley. (2011). ggplot2. Wiley Interdisciplinary Reviews: Computational Statistics, 3(2), 180185. Wickham, H., Cook, D., Hofmann, H., &amp; Buja, A. (2010). Graphical inference for infovis. IEEE Transactions on Visualization and Computer Graphics, 16(6), 973979. http://doi.org/10.1109/TVCG.2010.161 Wickham, Hadley, &amp; Grolemund, G. (2016). R for data science: Import, tidy, transform, visualize, and model data. \" OReilly Media, Inc.\". Wilkinson, L. (2012). The grammar of graphics. In Handbook of computational statistics (pp. 375414). Springer. "]]
