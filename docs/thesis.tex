% From https://github.com/UWIT-IAM/UWThesis
\documentclass[print]{nuthesis}
\usepackage{amssymb, amsthm, amsmath, amsfonts}
\usepackage{wasysym}
\usepackage{mathrsfs}
% \usepackage{hyperref}
\usepackage{graphicx}
\usepackage{lineno}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{listings}
%\usepackage{breqn}
\usepackage{cancel, enumerate}
\usepackage{rotating, environ}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[inline]{enumitem}
\usepackage{dirtree}

\newtheorem{thm}{Theorem}
\newtheorem{defn}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{cor}{Corollary}

% Syntax highlighting #22
  \usepackage{color}
  \usepackage{fancyvrb}
  \newcommand{\VerbBar}{|}
  \newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
  \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
  % Add ',fontsize=\small' for more characters per line
  \usepackage{framed}
  \definecolor{shadecolor}{RGB}{248,248,248}
  \newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
  \newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
  \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
  \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
  \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
  \newcommand{\BuiltInTok}[1]{#1}
  \newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
  \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
  \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
  \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
  \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
  \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
  \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
  \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
  \newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
  \newcommand{\ExtensionTok}[1]{#1}
  \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
  \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
  \newcommand{\ImportTok}[1]{#1}
  \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
  \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
  \newcommand{\NormalTok}[1]{#1}
  \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
  \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
  \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
  \newcommand{\RegionMarkerTok}[1]{#1}
  \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
  \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
  \newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
  \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
  \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
  \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}

%% https://github.com/rstudio/rmarkdown/issues/1649
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newenvironment{CSLReferences}[2]%
{\setlength{\parindent}{0pt}%
\everypar{\setlength{\hangindent}{\cslhangindent}}\ignorespaces}%
{\par}

% fix for pandoc 1.14
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

%% something about tables, from https://github.com/ismayc/thesisdown/issues/122
\usepackage{calc}

%% for copyright symbol
\usepackage{textcomp}

%% to allow to rotate pages to landscape
\usepackage{lscape}
%% to adjust table column width
\usepackage{tabularx}

% suppress bottom page numbers on first page of each chapter
% because they overlap with text
\usepackage{etoolbox}
\patchcmd{\chapter}{plain}{empty}{}{}

%% for more attractive tables
\usepackage{booktabs}
\usepackage{longtable}


\usepackage{graphicx}


% Double spacing, if you want it.
\def\dsp{\def\baselinestretch{2.0}\large\normalsize}
% \dsp

% If the Grad. Division insists that the first paragraph of a section
% be indented (like the others), then include this line:
\usepackage{indentfirst}

%%%%%%%%%%%%%%%%%%
% If you want to use "sections" to partition your thesis
% un-comment the following:
%
% \counterwithout{section}{chapter}
% \setsecnumdepth{subsubsection}
% \def\sectionmark#1{\markboth{#1}{#1}}
% \def\subsectionmark#1{\markboth{#1}{#1}}
% \renewcommand{\thesection}{\arabic{section}}
% \renewcommand{\thesubsection}{\thesection.\arabic{subsection}}
% \makeatletter
% \let\l@subsection\l@section
% \let\l@section\l@chapter
% \makeatother
% 
% \renewcommand{\thetable}{\arabic{table}}
% \renewcommand{\thefigure}{\arabic{figure}}

%%%%%%%%%%%%%%%%%%


%% Stuff from https://github.com/suchow/Dissertate

% The following line would print the thesis in a postscript font

% \usepackage{natbib}
% \def\bibpreamble{\protect\addcontentsline{toc}{chapter}{Bibliography}}

\setcounter{tocdepth}{1} % Print the chapter and sections to the toc
% controls depth of table of contents (toc): 0 = chapter, 1 = section, 2 = subsection

\usepackage{natbib}


% commands and environments needed by pandoc snippets
% extracted from the output of `pandoc -s`
%% Make R markdown code chunks work
\usepackage{array}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifxetex
  \usepackage{fontspec,xltxtra,xunicode}
  \defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase}
\else
  \ifluatex
    \usepackage{fontspec}
    \defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase}
  \else
    \usepackage[utf8]{inputenc}
  \fi
\fi
\usepackage{color}
\usepackage{fancyvrb}


\ifxetex
  \usepackage[setpagesize=false, % page size defined by xetex
              unicode=false, % unicode breaks when used with xetex
              xetex,
              colorlinks=true,
              linkcolor=blue]{hyperref}
\else
  \usepackage[unicode=true,
              colorlinks=true,
              linkcolor=blue]{hyperref}
\fi
\hypersetup{breaklinks=true, pdfborder={0 0 0}}
\setlength{\parindent}{20pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\setcounter{secnumdepth}{2} %% controls section numbering, e.g. 1 or 1.2, or 1.2.3


\input{preamble.tex}

\begin{document}
% \linenumbers{}
%% Start formatting the first few special pages
%% frontmatter is needed to set the page numbering correctly
\frontmatter
%% from thesisdown
% To pass between YAML and LaTeX the dollar signs are added by CII
\title{HUMAN PERCEPTION OF EXPONENTIALLY INCREASING DATA DISPLAYED ON A LOG SCALE EVALUATED THROUGH EXPERIMENTAL GRAPHICS TASKS}
\author{Emily Anna Robinson}
\adviser{Susan VanderPlas and Reka Howard}
\adviserAbstract{}
\major{Statistics}
\degreemonth{August}
\degreeyear{2022}
% \copyrightpage
%%
%% For most people the defaults will be correct, so they are commented
%% out. To manually set these, just uncomment and make the needed
%% changes.
%% \college{Your college}
%% \city{Your City}
%%
%% For most people the following can be changed with a class
%% option. To manually set these, just uncomment the following and
%% make the needed changes.
%% \doctype{Thesis or Dissertation}
%% \degree{Your degree}
%% \degreeabbreviation{Your degree abbr.}
%%
%% Now that we know everything we need, we can generate the title page
%% itself.
%%
\maketitle


\begin{abstract}
    Log scales are often used to display data over several orders of magnitude within one graph. We conducted a series of three graphical studies to evaluate the impact displaying data on the log scale has on human perception of exponentially increasing trends compared to displaying data on the linear scale. Each study was related to a different graphical task, each requiring a different level of interaction and cognitive use of the data being presented. The first experiment evaluated whether our ability to perceptually notice differences in exponentially increasing trends is impacted by the choice of scale. Participants were shown a set of plots and asked to identify which plot appeared to differ most from the other plots. Results indicated the choice of scale changes the contextual appearance of the data leading to slight perceptual advantages for both scales depending on the curvatures of the trend lines being compared. The second study validated a new method, `You Draw It', for testing statistical graphics and introduced an appropriate statistical analysis method for comparing visually fitted trend lines to statistical regression results. This new method was then used to test participants ability to make forecast predictions for exponentially increasing trends on both scales. The results from the analysis showed a clear underestimation of forecasting trends with high exponential growth rates when participants were asked to make predictions on the linear scale; improvement in forecasts were made when participants were asked to make predictions on the log scale. The third study evaluated graph comprehension as it relates to the contextual scenario of the data shown. Overall, our results suggested that log logic is difficult and that anchoring and rounding biases result in a sacrifice in accuracy in estimates made on the log scale for large magnitudes. The studies conducted in this research relied on graphical tasks of varying complexity to help us understand the perceptual and cognitive advantages and disadvantages of displaying exponentially increasing data on the log scale. The results are instrumental in establishing guidelines for making design choices about scale which result in data visualizations effective at communicating the intended result.
\end{abstract}

%% Optional
\begin{copyrightpage}
\end{copyrightpage}

%% Optional
% \begin{dedication}
% Dedicated to\ldots{}
% \end{dedication}

%%%%%%%%%%%%%%%%%%%
% Acknowledgments
%%%%%%%%%%%%%%%%%%%
\begin{acknowledgments}
All of this would not have been possible without my incredible network and support system of friends, family, and mentors throughout the years. I want to thank everyone who encouraged me in any way along my graduate school journey.

I was fortunate to have been introduced to statistics by some amazing professors at Winona State University during my undergraduate degree. All of you invested in my future and gave me opportunities that built my foundation of statistical knowledge with activities outside the classroom and internship experiences. Dr.~Tisha Hooks, thank you for being someone I know I can always count on for advice and Dr.~Chris Malone, thank you for planting the small mustard seed of being a professor in my mind by telling me, ``go to grad school and teach.'' Thank you to my friends and peers at WSU who navigated classes and life with me, you taught me the importance of surrounding yourself with a group of collegues to collaborate and share ideas with. To Andrew Hansen for continuing on to our master's together at University of Nebraska - Lincoln; you have always seen the best in me and never doubted what I was capable of achieving.

During graduate school, I have had many wonderful mentors. To my advisors, Dr.~Susan VanderPlas and Dr.~Reka Howard, thank you for showing me that you can be both an unbelievable mother and a kick ass academic. Reka, thank you for giving me the freedom to explore and discover an area of statistics I am passionate about; you have been encouraging me from day one. Susan, thank you for sharing your world of graphics with me and continuing to push me to be a better researcher; I am looking forward to collaborating more in the future. To my committee members, Dr.~Erin Blankenship and Dr.~Heather Akin, thank you for your enthusiasm about my work and being a part of my Ph.D.~program. To other prominent mentors at UNL, Dr.~Kathy Hanford and Dr.~Walter Stroup, thank you for training me to be a statistical consultant and value collaborative work. Dr.~Stroup, thank you for hiking a mountain with me at JSM and advising me not to trust anyone under 14,000 feet.

My family is a top notch bunch. To my parents and biggest fans, Wayne and Ann Robinson, thank you for always believeing in me and knowing when I need extra support or encouragement. To my sister Kelby Hildebrandt and brother in-law Christen Hildebrandt, thank you for always being curious about my work; to my sweet nephew, Slade Hildebrandt, you never fail to make me smile. To my brother, Mark Robinson, thank you for speaking my language of statistics and to my sister Ainslie Robinson, I am forever grateful for our countless FaceTime chats and virtual pandemic lunches.

Nobody makes it through life without a great group of friends and I am forever grateful for my statistics gals who were there to support me throughout this entire journey. Jessica Hauschild and Kelsey Karnik, you have been solid rocks and the most reliable friends I have ever had. To Dr.~Ella Burnham, you were the best office mate I could have asked for; thank you for making going into work fun and for taking random study break walks with me. Dr.~Donna Chen, thank you for being my adventure buddy and encouraging me to grow in my faith. To my graduate school roommate, Alison Kleffner, there are not enough words to thank you for being one of my best friends and biggest cheerleaders in Lincoln, NE; graduate school would not have been the same without someone to work from home with during the pandemic. Thank you for always laughing at my jokes and going along with my crazy ideas; I am excited to see where life takes you in your future. I was incredibly blessed with the best cohort of peers to navigate classes, consulting, and research with; Dr.~Vamsi Manthena and (soon to be) Dr.~Miguel Fudolig, thank you for not only being my peers, but friends I can count on and collegues to bounce ideas off of. You both challenged me to push my goals further than I could ever imagine. To my friends who supported me from a distance, than you for always picking up the phone and meeting me where I was at in life. Dr.~Katilin Wohnoutka, PT, DPT, thank you for walking along side me through our twenties and showing me how to embrace the unknown.

I want to give a big thanks to Southwood Lutheran Church for giving me a church home throughout graduate school. You gave me the space to grow stronger in my faith and challenged me to develop my own beleifs about the world around me. It is a testament to you that I strategically planned my summers around the annual mission trips; thank you for taking me in as an outsider and making me a part of these incredible weeks. To Pastor Greg Olson, thank you for giving me the courage to take a leap of faith on my future.

Lastly, I want to thank Dr.~Taylor Alison Swift for making beautiful music that got me through some long days and nights of writing.

It takes a village to accomplish something great and I am so thankful for the community that has surrounded me during my time in Lincoln, NE. To the Cal Poly Statistics Department, I look forward to joining this incredible group of faculty; you have already shown me the support I know I will have in my next stage of life.
\end{acknowledgments}
%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%
% Grant Information
%%%%%%%%%%%%%%%%%%%
% \begin{grantinfo}
%     % Add any grant info here
% \end{grantinfo}

%%%%%%%%%%%%%%%%%%%
% ToC
%%%%%%%%%%%%%%%%%%%
\tableofcontents

%%%%%%%%%%%%%%%%%%%
% List of Figures
%%%%%%%%%%%%%%%%%%%
\listoffigures
\listoftables

%%%%%%%%%%%%%%%%%%%
% Start of the document
%%%%%%%%%%%%%%%%%%%
\mainmatter


\hypertarget{literature-review}{%
\chapter{Literature Review}\label{literature-review}}

\hypertarget{motivation-and-background}{%
\section{Motivation and Background}\label{motivation-and-background}}

We have recently experienced the impact graphics and charts have on a large scale through the SARSNCOV-2 pandemic (COVID-19).
At the beginning of 2020, we saw an influx of dashboards being developed to display case counts, transmission rates, and outbreak regions (Rost, 2020); mass media routinely showed charts to share information with the public about the progression of the pandemic (Romano, Sotis, Dominioni, \& Guidi, 2020).
Fagen-Ulmschneider (2020) began the 91-DIVOC project to explore the global growth of COVID-19 through interactive graphics updated daily.
The interactive graphics allowed viewers to explore the current status of COVID-19 by selecting their desired regions, axes, axis scale, and measure of interest (for example, case count, death count, and vaccine count); \cref{fig:91divoc-cases-july2021} (Fagen-Ulmschneider, 2020) shows the new confirmed COVID-19 cases per day, normalized by population, as of July 2021.
Other graphics displayed COVID-19 data as maps \pcref{fig:covid19-summer2020-risk-map} with color indicating the severity and risk in each US county (Jha et al., 2021).
People began seeking out graphical displays of COVID-19 data as a direct result of these pieces of work (Rost, 2020); providing increased and ongoing exposure to these graphics over time.
\cref{fig:covid19-datawrapper-views-july2020} illustrates the increased views Datawrapper, a user-friendly web tool used to create basic interactive charts, had during the COVID-19 pandemic (Rost, 2020).
Many of these graphics helped guide decision makers to implement policies such as shut-downs or mandated mask wearing, as well as facilitated communication with the public to increase compliance (Bavel et al., 2020).
As graphics began to play an important role in sharing information with the public, creators of graphics were faced with design choices in order to ensure their charts were effective at accurately communicating the current status of the pandemic.
In order to make educated decisions when designing a chart, we need guidelines as established through experimentation to ensure the graphic is effective at communicating the intended result.

\begin{figure}[tbp]

{\centering \includegraphics[width=0.75\linewidth,]{images/91dovic-cases-july2021} 

}

\caption[91-DIVOC new daily case counts as of July 2021]{New daily COVID-19 case counts as of July 2021 shown in the 91-DIVOC dashboard.}\label{fig:91divoc-cases-july2021}
\end{figure}

\begin{figure}[tbp]

{\centering \includegraphics[width=0.75\linewidth,]{images/covid19-summer2020-risk-map} 

}

\caption[COVID-19 risk level map as of July 2020]{COVID-19 risk level map as of July 2020.}\label{fig:covid19-summer2020-risk-map}
\end{figure}

\begin{figure}[tbp]

{\centering \includegraphics[width=0.75\linewidth,]{images/covid19-datawrapper-views-july2020} 

}

\caption[Datawrapper daily chart views during COVID-19]{Datawrapper daily chart views during COVID-19.}\label{fig:covid19-datawrapper-views-july2020}
\end{figure}

\hypertarget{misleading-graphics}{%
\section{Misleading Graphics}\label{misleading-graphics}}

There are many ways in which plots may inaccurately display the data and be ineffective or misleading in sharing information and results (Szafir, 2018).
Misleading charts might have (1) bad form such as 3D pie charts or a plot type that is unsuitable for the type of data \pcref{fig:misleading-graphics-form} (BusyAd6668, 2022; Stewart-Lowndes et al., 2017), (2) include too much ``chartjunk'', resulting in clutter or displaying useless data \pcref{fig:misleading-graphics-chartjunk} (Dongarra, Meuer, Strohmaier, et al., 1997; Wordtips, 2022), or (3) have bad axes such as a mismatch between scale and context or just plain bad math \pcref{fig:misleading-graphics-bad-axes} (Smith, 2017; Villa, 2022).
\cref{fig:misleading-graphics-just-bad} demonstrates how a chart can be misleading in more than one way; the map of pet ownership violates guidelines by mapping too many variables to visual encodings which results in clutter and bad form (Benjamin-Cat, 2018).

\begin{figure}[tbp]

{\centering \includegraphics[width=0.49\linewidth,]{images/misleading-graphics-3d-pie-batteries} \includegraphics[width=0.49\linewidth,]{images/misleading-graphics-bad-form-better-science} 

}

\caption[Misleading graphical form examples]{These figures display information in bad form which results in an ineffective chart.}\label{fig:misleading-graphics-form}
\end{figure}

\begin{figure}[tbp]

{\centering \includegraphics[width=0.49\linewidth,]{images/misleading-graphics-wordle-map} \includegraphics[width=0.49\linewidth,]{images/misleading-graphics-bad-form-supercomputers} 

}

\caption[Misleading 'chartjunk' examples]{These figures provide examples of how too much clutter can be misleading.}\label{fig:misleading-graphics-chartjunk}
\end{figure}

\begin{figure}[tbp]

{\centering \includegraphics[width=0.49\linewidth,]{images/misleading-graphics-bad-axes-rouble-to-dollar} \includegraphics[width=0.49\linewidth,]{images/misleading-graphics-pizza} 

}

\caption[Misleading bad axes]{These figures are misleading in their axis selections and bad math. (Left) Selecting a baseline $y$-axis value of 0 minimizes the large decline in the rate of the rouble in 2022. (Right) The probabilities sum to greater than 100\%.}\label{fig:misleading-graphics-bad-axes}
\end{figure}

\begin{figure}[tbp]

{\centering \includegraphics[width=0.5\linewidth,]{images/misleading-graphics-multiple-issues-pet-ownership} 

}

\caption[Misleading graphic multiple violations example]{This figure demonstrates how a chart can be misleading in more than one way; the map of pet ownership violates guidelines by mapping too many variables to visual encodings which results in clutter and bad form.}\label{fig:misleading-graphics-just-bad}
\end{figure}

Baumer, Kaplan, \& Horton (2021) shares an example of a misleading graphic in the news shown in May 2020 when Georgia published a graphical display of COVID-19 cases \pcref{fig:covid-19-reporting}.
This graphic was highly misleading in communicating the state of the pandemic due to the ordering along the \(x\)-axis.
Notice the case count for April \(17^{th}\) appears to the right of April \(19^{th}\), and that the order of the counties has been selected so that the case counts are monotonically decreasing for each day of reporting.
The appearance of this graphic leads viewers to believe COVID cases are decreasing.
Shortly after the graphic was released, the governor's office made a statement that in future charts, chronological order would be used to display time due to public demand.

\begin{figure}[tbp]

{\centering \includegraphics[width=1\linewidth,]{images/covid-ga-recreation} 

}

\caption[COVID-19 reporting]{Misleading graphic example displaying COVID-19 cases in Georgia as of May 2020. Notice the case count for April $17^{th}$ appears to the right of April $19^{th}$, and that the order of the counties has been selected so that the case counts are monotonically decreasing for each day of reporting.}\label{fig:covid-19-reporting}
\end{figure}

Misleading charts are not only found in mass media, but graphics displayed in academic research and science are still falling short of the standards. Gordon \& Finch (2015) evaluated 97 graphs for overall quality, based on five principles of graphical excellence including: (1) show the data clearly (2) use simplicity in design (3) use good alignment on a common scale for quantities to be compared (4) keep the visual encoding transparent (5) use graphical forms consistent with principles 1 and 4.
The authors rated 39\% of the 97 graphs sampled as poor, indicating there is still an astonishing lack in the quality of graphics.
More startling is the fact that the source of the graphic from an applied science or a graphic from statistics had no effect on the quality of the graphic.

Although statistical graphics have become widely used and valued in science, business, and in many other aspects of life, we may be too accepting of easy-to-create, default data displays, using them without critically questioning the data and/or how effective the display is at displaying the data. Attempts to improve the creation and use of charts have been ongoing since the early \(20^{th}\) century (Croxton \& Stein, 1932; Croxton \& Stryker, 1927; Eells, 1926; Von Huhn, 1927).
We address the ways in which researchers use experimentation to test statistical graphics in order to establish guidelines and improve graphics in \protect\hyperlink{testing-statistical-graphics}{Section 1.4}.
In efforts to achieve a higher standard of the graphics being presented, work must be done to implement the academic research being conducted in graphics into practice.
For example, better definitions of variables, units of measurements, scales, and other graphical elements is necessary in order to improve the overall quality of graphics.
Changes in software defaults such as the originally set number of bins in a bar chart can help support the improvement of graphs in both statistics and the applied science.

\hypertarget{graphical-frameworks}{%
\section{Graphical Frameworks}\label{graphical-frameworks}}

A consistent concern is the lack of theory of graphics available to build on; better theory should result in better data visualizations.
In order to improve the construction and distribution of charts, we need an established set of concepts and terminology so we can actively choose which of many possible graphics to draw in order to ensure our charts are effective at communicating the intended result.
Many efforts have been made to provide frameworks and classification systems for graphical designs; the most useful for our purposes is Wilkinson's Grammar of Graphics (Wilkinson, 2013).
The grammar of graphics serves as the fundamental framework for data visualization with the notion that graphics are built from the ground up by specifying exactly how to create a particular graph from a given data set.
Visual representations are constructed through the use of ``tidy data'' which is characterized as a data set in which each variable is in its own column, each observation is in its own row, and each value is in its own cell (Wickham \& Grolemund, 2016).
Graphics are viewed as a mapping from variables in a data set (or statistics computed from the data) to visual attributes such as the axes, colors, shapes, or facets on the canvas in which the chart is displayed.
\cref{fig:graphic-flowchart} illustrates the process of creating a graphic from a data set through the use of variable mapping, data transformations, coordinate systems, and aesthetic features (Vanderplas, Cook, \& Hofmann, 2020)
Software, such as \texttt{ggplot2} (Wickham, 2016), aims to implement the framework of creating charts and graphics using the layered framework of the grammar of graphics.

\begin{figure}[tbp]

{\centering \includegraphics[width=0.5\linewidth,]{images/graphic-flowchart} 

}

\caption[Graphic flowchart]{The flowchart illustrates the process of creating a graphic from a data set through the use of variable mapping, data transformations, coordinate systems, and aesthetic features.}\label{fig:graphic-flowchart}
\end{figure}

\hypertarget{good-graphics}{%
\section{Heuristics and ``Good'' Graphics}\label{good-graphics}}

Charts have been an essential component in communicating information for the last 200 years (Lewandowsky \& Spence, 1989).
Some of these charts succeeded in effectively showing the data in order for viewers to extract meaningful information.
For example, during 1870, 1880, and 1890, the ``Statistical Atlas of the United States'' (United States Census Office. 9Th Census, 1870 \& Walker, 1874) produced high quality and engaging graphics \pcref{fig:statistical-atlas-state-population}.
These data visualizations were created without the use of modern technology, demonstrating that exceptional graphics were achievable before the use of computers.

\begin{figure}[tbp]

{\centering \includegraphics[width=0.85\linewidth,]{images/statistical-atlas-state-population} 

}

\caption[Statistical Atlas of the United States (1870)]{The Statistical Atlas of the United States (1870) produced high quality graphics. This figure displays the population of each state where square size represents the proportion of the states population separated into three regions representing the origin and race of the population. The rectangle shown to the right represents the proportion of residents born in the state who have become residents of other states.}\label{fig:statistical-atlas-state-population}
\end{figure}

In the following decades, recommendations and guidelines emerged in order to help improve the overall quality of graphics.
Wickham (2013) gives a review and critique of the first formal advice for creating good graphics presented by The International Institute of Statistics in 1901 and Andrews (2022) recalls seventeen general suggestions for the visualization of statistical and quantitative data published by The American Society of Mechanical Engineers (ASME) in 1915.
Here we take a look at select guidelines from these articles and connect them to work conducted almost 100 years after the recommendations were made.

When creating graphics, early guidelines state to keep symbols to a minimum; Tufte (1985) coins the term ``chartjunk'' which refers to any of the visual elements in the chart that are unnecessary for the viewer to comprehend the information represented in the plot.
The ASME guidelines supported the minimization of ``chartjunk'' earlier that century by recommending that charts do not show any more coordinate lines than necessary to guide the eye in reading the diagram.
Guidelines for the use of lengths over areas or volumes were established in both sets of early recommendations, but not formally tested until the late \(20^{th}\) century by Cleveland \& McGill (1987).
The International Institute of Statistics included a recommendation for selecting the ratio of the scales such that the slope of the phenomenon corresponds to the tangent of the curve at a 45\(^{\circ}\) angle.
Almost 100 years later, Cleveland, McGill, \& McGill (1988) again makes the recommendation to ``bank to 45\(^{\circ}\)'' and Heer \& Agrawala (2006) explores extensions to propose alternate optimization criteria as well as introduce a technique meant to implement the original guideline.
Additional suggestions include the proper use of scales such as arrangements, labels, and baselines; the ASME advocates for careful consideration of the limiting lines for curves drawn on logarithmic coordinates \pcref{fig:standards-of-graphics-log-scale}.

\begin{figure}[tbp]

{\centering \includegraphics[width=1\linewidth,]{images/standards-of-graphics-log-scale} 

}

\caption[Logarithmic scales ASME guidelines]{The ASME (1915) reccommended when curves are drawn on logarithmic coördinates, the limiting lines of the diagram should each be at some power of ten on the logarithmic scales. Note the use of the minor gridline breaks unequally spaced visually, but equally spaced numerically.}\label{fig:standards-of-graphics-log-scale}
\end{figure}

Helpful suggestions for creating ``good'' graphics are extremely common (and sometimes conflicting). In the absence of an underlying understanding of how graphics are perceived and used, these suggestions are largely ineffective and sometimes harmful. It is essential to have guidelines established through careful experimentation combined with an understanding of the perceptual and cognitive processes involved in the use of statistical graphics.

\hypertarget{testing-statistical-graphics}{%
\section{Testing Statistical Graphics}\label{testing-statistical-graphics}}

One way in which we establish guidelines is through the use of graphical tests (Cleveland \& McGill, 1984; Lewandowsky \& Spence, 1989; Spence, 1990; VanderPlas \& Hofmann, 2015).
These tests may take many forms: identifying differences in graphs, accurately reading information off a chart, using data to make correct real-world decisions, or predicting the next few observations.
All of these types of tests require different levels of use and manipulation of the information presented in the chart.

The initial push to develop classification and recommendation systems for charts was grounded on heuristics rather than on experimentation (Kruskal, 1975; Macdonald-Ross, 1977).
Requests were made for the validation of the perception and utility of statistical charts through graphical experiments.
Most early experimentation (Croxton \& Stein, 1932; Croxton \& Stryker, 1927; Eells, 1926) stemmed from psychophysics research on the perception of size and shape.
In attempts to understand the human perception and judgment of component parts, Eells (1926) instructed students to think of each circle diagram \pcref{fig:eells-compoment-parts} as representing 100\% and write their best estimate of the percentage of the whole in each sector.
Participants were told not to hurry, but to work steadily in order to determine efficiency of judgment.
Students were then asked to analyze their mental processes used to make their estimates and indicate the method that best matches: by areas of sectors, by central angles, by arcs on the circumference, by subtending chords.
This process was repeated three days later by presenting students the same data represented in bar diagrams \pcref{fig:eells-compoment-parts}.
Results of the study led the authors to argue for the use of circle diagrams to show component parts based on both participant accuracy and speed.
In response, Croxton \& Stryker (1927) evaluated the accuracy of judgment of two types of charts (bars and circles) in efforts to reach a consistent conclusion.
During class, students were individually presented pairs of diagrams (without scales) on cards and asked to estimate the percentages displayed in the diagram.
It was found that the bar was preferable to the circle when shown percentages that deviate from quarters, but that the circle is strongly preferred when shown percentages separating the diagrams into 25\% or 50\%; this introduces the concept of anchoring discussed further in \protect\hyperlink{estimation-biases}{Section 1.6.3}.

\begin{figure}[tbp]

{\centering \includegraphics[width=0.75\linewidth,]{images/eells-component-parts} 

}

\caption[Eells (1926) component parts diagrams]{Component part diagrams shown to study participants in Eells (1926). Researchers were interested in comparing the partition percentage estimate between circle and bar diagrams.}\label{fig:eells-compoment-parts}
\end{figure}

While a typical psychophysics experiment focuses on whether an effect is detectable and whether the magnitude of the effect can be accurately estimated, these early experiments instead depended on speed and accuracy for plot evaluation (Lewandowsky \& Spence, 1989; Spence, 1990; Teghtsoonian, 1965).
In attempts to understand the visual psychophysics of simple graphical elements, Spence (1990) presented stimuli (tables, lines - horizontal and vertical, bars, boxes, cylinders, pie charts, and disk charts) to participants on a monitor screen in a computer lab.
Participants were asked to use their cursor to position the marker to indicate the proportion to the apparent sizes of the elements \pcref{fig:spence-1990-proportion}.
Results found that the table elements (numbers), pie elements, and bar elements led to the most accurate proportion estimates; boxes and disk elements resulted in the least accurate estimates.
Measuring the speed at which participants made their judgments, it was found that two- and three- dimensional stimuli (for example, pie charts and box charts) assisted in faster judgment than zero- or one- dimensional stimuli (for example, tables and lines).

\begin{figure}[tbp]

{\centering \includegraphics[width=0.75\linewidth,]{images/spence-1990-proportion} 

}

\caption[Spence (1990) task display]{Example of a stimuli shown to study participants in Spence (1990). Participants were asked to use their cursor to position the marker to indicate the proportion to the apparent sizes of the elements.}\label{fig:spence-1990-proportion}
\end{figure}

Cognitive psychologists and statisticians made progress by conducting experiments to identify perceptual errors associated with different styles of graphics and charts (Cleveland \& McGill, 1984, 1985; Shah, Mayer, \& Hegarty, 1999).
Cleveland \& McGill (1984) provides a basis for perceptual judgment, still utilized today, by examining six basic plot objects: position along a common scale, position along nonaligned scales, length, angle, slope, and area.
In Cleveland \& McGill (1985), these plot objects are ordered by accuracy performed through graphical-perception tasks; for example, comparisons of angles resulted in more difficult judgments than between lengths of lines.
Shah et al. (1999) established the notion that redesigning graphs can result in the improvement of the viewer's interpretation of the data.
For example, the use of gestalt principles (Goldstein \& Cacciamani, 2021) such as proximity, similarity, and good continuation can help minimize the inferential processes and maximize the pattern association processes required to interpret relevant information.
In \protect\hyperlink{underestimation}{Section 1.8} we see how the hierarchy of accuracy in plot objects presented in Cleveland \& McGill (1985) can explain biases in our interpretation and use of graphics.

During the \(\text{21}^{\text{st}}\) century, there have been advancements in the methodology used to investigate the effectiveness of statistical charts (Majumder, Hofmann, \& Cook, 2013).
Buja et al. (2009a) introduced the lineup protocol in which data plots are depicted and interpreted as statistics.
Supported by the grammar of graphics, a data plot can be characterized as a statistic, defined as, ``a functional mapping of a variable or set of variables'' (Vanderplas et al., 2020).
This allows the data plot to be tested similar to other statistics, by comparing the actual data plot to a set of plots with the absence of any data structure we can test the likelihood of any perceived structure being significant.
The construction of data plots as statistics allows for easy experimentation, granting researchers the ability to compare the effectiveness of and understand the perception of different types of charts.
While the lineup protocol differs from methodology used in earlier studies, the focus is still on initial perception with a relatively small amount of work conducted to understand the effect of design choices on higher cognitive processes such as learning or analysis (Green \& Fisher, 2009).
Lineups serve as a powerful tool for testing \emph{perceived} differences by eliminating ambiguous questions.
However, the lineup protocol is constrained by the inability to test higher order cognitive skills such as accurately reading information off of a graph or drawing conclusions from the graph, limiting their ability to be used for testing real-world applications.

\hypertarget{task-complexity}{%
\section{Task Complexity}\label{task-complexity}}

In order to understand how our visual system perceives statistical charts, we must first consider the complexity of the graphic and how viewers are interacting with the data and information being displayed (Tory \& Moller, 2004).
The efficiency in which a viewer extracts data and information from a graphical display is greatly affected by the complexity in the task environment.
Cognitive fit refers to a match between the representation of the data and the complexity of the task; the representation and tools should support the task strategies, thus reducing the complexity of the task (Vessey, 1991).
Carpenter \& Shah (1998) identifies pattern recognition, interpretative processes, and integrative processes as strategies and processes required to complete tasks of varying degrees of complexity.
Pattern recognition requires the viewer to encode graphic patterns while interpretive processes operate on those patterns to construct meaning.
Integrative processes then relate the meanings to the contextual scenario as inferred from labels and titles.
These processes are critical when determining cognitive fit since they provide the link between the graphical representation and task (Vessey, 1991).
For example, perceptual differences may be identified through pattern recognition while estimation tasks would require integrative processes.
Tory \& Moller (2004) argues for multiple visual representations of the data since the users' information needs are dependent on both domain and task.
Therefore, we must consider and determine how the viewer is perceiving and interacting with the graphic as this can influence their understanding of the data and information.

\hypertarget{graph-comprehension}{%
\section{Graph Comprehension}\label{graph-comprehension}}

Higher order cognitive processes require viewers to translate the visual features into conceptual relations by interpreting titles, labels, and scales.
In order to understand how viewers are interpreting and using the data and information displayed on the chart, studies have asked participants to read information directly from a chart and provide a quantitative estimate or answer a predefined question (Amer, 2005; Broersma \& Molenaar, 1985; Dunn, 1988; Peterson \& Schramm, 1954; Spence, 1990; Tan, 1994).
For instance, Amer (2005) demonstrates that visual illusion may bias decision making and graph comprehension, even if the graphs are constructed according to best practice.
Participants were presented a cost volume profit graph \pcref{fig:amer-poggendorff-illusion} with two crossing lines (revenue and cost) and asked to estimate three values: (1) the amount of total revenues on the ordinate corresponding to the endpoint of the total-revenue line plotted on the graph (2) the amount of total costs on the ordinate corresponding to the endpoint of the total-cost line plotted on the graph and (3) the amount of costs/revenues on the ordinate at the break even point---the point where the two lines cross.
Results indicate that decision makers may consistently underestimate or overestimate the values displayed on line graphs due to what is called the ``Poggendorff illusion.''

\begin{figure}[tbp]

{\centering \includegraphics[width=1\linewidth,]{images/amer-poggendorff-illusion} 

}

\caption[Amer (2005) cost volume profit graph]{Participants in Amer (2005) were shown this plot in order to test their graph comprehension and identify visual biases. This figure illustrates the 'Poggendorff illusion' which results in visual biases of underestimation and overestimation.}\label{fig:amer-poggendorff-illusion}
\end{figure}

\hypertarget{lit-questioning}{%
\subsection{Questioning}\label{lit-questioning}}

An important consideration in understanding graph comprehension is is the questions being asked of the viewer (Graesser, Swamer, Baggett, \& Sell, 2014).
Low level questions address the content and interpretation or explicit material while deeper questions require inference, application, and evaluation of the information being presented.
Three levels of graph comprehension have emerged from mathematics education research (Curcio, 1987; Friel, Curcio, \& Bright, 2001; Glazer, 2011; Jolliffe, 1991; R. Wood, 1968).
The three behaviors related to graph comprehension involve (1) literal reading of the data (elementary level), (2) reading between the data (intermediate level), and (3) reading beyond the data (advanced level).
Curcio (1987) aligns two multiple choice questions with each level of comprehension related to a graph showing the height of four children in centimeters \pcref{fig:children-height}.
Two literal items required the viewer to read the data, title, or axis label in order to answer, ``What does this graph tell you?'' or ``How tall was xxx?''
Comparison items required comparisons and the use of mathematical concepts to answer, ``Who was the tallest?'' and ``How much taller was x than y?''
Lastly, extension items required an extension, prediction, or inference such as, ``If x grows 5 centimeters and y grows 10 centimeters by Sept.~1981, who will be taller and by how much?
In Friel et al. (2001), several studies were reviewed and their questions were placed in the taxonomy of skills required for answering questions at each level.
In addition to the graph's visual features and questioning, it is important for researchers to give careful consideration to the context of the graphic on the viewers comprehension.

\begin{figure}[tbp]

{\centering \includegraphics[width=0.8\linewidth,]{images/children-height} 

}

\caption[Comprehension of heights, Curcio (1987)]{This plot was used in Curcio (1987) along with questions related to the heights of the four children in order to better understand graph comprehension skills.}\label{fig:children-height}
\end{figure}

\hypertarget{estimation-strategies}{%
\subsection{Estimation Strategies}\label{estimation-strategies}}

While not exclusive to extracting numerical values from charts, mathematics education research places an emphasis on quantitative estimation skills (Hogan \& Brezinski, 2003).
Three modes of estimation are taught as part of the mathematics curriculum in schools: numerosity, measurement, and computational estimation.
Numerosity estimation requires the estimation of the number of items in a group or array; for example, guessing the number of M\&M's in a jar.
Measurement estimation requires participants to provide an estimated value related to an object; for instance, an estimated length of a string or weight or a box.
Computational estimation is the third mode which refers to estimated answers to computations as a way to avoid exact calculations.
These estimates may be presented in either algorithmic form or a contextual scenario with words.
A longer history of quantitative estimation can be found in psychometric literature in which estimation tasks appeared in early psychometric studies of mental abilities.

In efforts to develop estimation skills, research has been conducted to evaluate strategies for estimating tasks.
Common strategies related to measurement estimation, involve reference point estimation, benchmark estimation, unit iteration, and guess and check.
Joram, Gabriele, Bertheau, Gelman, \& Subrahmanyam (2005) was interested in the relationship among strategy use and accuracy of students' representations of standard measurement units and measurement accuracy.
In this study, students were asked to estimate the lengths of two objects and explain their process.
The researchers used talk aloud protocols to prompt students to communicate their estimation strategies to an interviewer; results found that students who used a reference point had a more accurate representation of standard units and estimates of length than students who did not use a reference point.
Gail Jones, Gardner, Taylor, Forrester, \& Andre (2012) examined the effect of scale (metric verses English) and task context on the accuracy of measurement estimation for linear distances.
The study showed that students were less accurate in estimating metric units as compared to English units and that estimation accuracy was highly dependent on task context.
Forrester, Latham, \& Shire (1990) argues that estimation, approximating, and measuring are key components in the intuitive understanding of dimension and scale necessary to manipulate information and interact effectively with our environment.
Without open ended conversations, our research demonstrates how the use of graphical tasks of varying complexities conducted through an online system can provide insight about the tactics and procedures used to extract meaning from a chart.

\hypertarget{estimation-biases}{%
\subsection{Estimation Biases}\label{estimation-biases}}

Certain biases including anchoring and rounding to multiples of five or ten arise in open-ended estimation tasks.
When it comes to understanding graphics, anchoring is prominent in both graphical representations and data extraction tasks (Tan \& Benbasat, 1990).
Anchoring bias refers to an individual using easily observed visual cues such as grid lines or ``anchors'' when extracting information such as the \(x\) or \(y\) value on a chart (Tan, 1994 ; Godlonton, Hernandez, \& Murphy, 2018).
In addition to \(x\)-value and \(y\)-value anchoring, entity anchoring refers to anchoring on group information withing a data set.
Rounding errors occur out of natural human preference to provide rounded figures even if a precise estimate is desired or requested (Myers, 1954).
Schneeweiss, Komlos, \& Ahmad (2010) outlines distortion in results as a consequence of rounding and suggests the use of corrections when conducting statistical regression analyses on data prone to rounding.

Scale and axis labels are other critical factors in estimation accuracy.
Dunham \& Osborne (1991) argue that if there is not proper attention given to the scale when using a line graph, there is a potential for issues when interpreting asymmetric scales and when choosing appropriate scales for the graphic.
Beeby \& Taylor (1973) found that when asked to read data from line graphs, viewers consistently misread the \(y\)-axis scale; when alternate grid lines were labeled, the unlabeled grid lines were read as halves.
This misrepresentation is highlighted for asymmetric scales where spatial distance does not necessarily equate to numerical or quantitative difference.
The choice of scale can change the shape of a graph, thus creating a conceptual demand for the viewer when constructing a mental image of the graph (Leinhardt, Zaslavsky, \& Stein, 1990).

\hypertarget{logarithmic-scales-and-mapping}{%
\section{Logarithmic Scales and Mapping}\label{logarithmic-scales-and-mapping}}

A major issue we encountered in the creation of COVID-19 plots was how to display data from a wide range of values.
When faced with data which spans several orders of magnitude, we must decide whether to show the data on its original scale (compressing the smaller magnitudes into relatively little area) or to transform the scale and alter the contextual appearance of the data.
One common solution is to use a log scale transformation to display data over several orders of magnitude within one graph.
Exponential curves are a common source of data in which smaller magnitudes are compressed into a smaller area;
\cref{fig:log-scales} presents an exponential curve displayed on both the linear and log scale illustrating the use of the log scale when displaying data which spans several magnitudes.
Logarithms convert multiplicative relationships (for example, 1 \& 10 displayed 10 units apart and 10 \& 100 displayed 90 units apart) to additive relationships (for example, 1 \& 10 and 10 \& 100 both equally spaced along the axis), showing proportional relationships and linearizing power functions (Menge et al., 2018).
They also have practical purposes, easing the computation of small numbers such as likelihoods and transforming data to fit statistical assumptions.
When presenting log scaled data, it is possible to use either un-transformed scale labels (for example, values of 1, 10 and 100 are equally spaced along the axis) or log transformed scale labels (for example, 0, 1, and 2, showing the corresponding powers of 10).

\begin{figure}[tbp]

{\centering \includegraphics[width=1\linewidth,]{thesis_files/figure-latex/log-scales-1} 

}

\caption[Linear scale verses log scale]{These plots present an exponential curve displayed on both the linear and log scale and illustrate the use of the log scale when displaying data which spans several magnitudes.}\label{fig:log-scales}
\end{figure}

In spring 2020, during the early stages of the COVID-19 pandemic, there were large magnitude discrepancies in case counts at a given time point between different geographic regions (for example states and provinces as well as countries and continents).
During this time, we saw the usefulness of log scale transformations showing case count curves for areas with few cases and areas with many cases within one chart.
The usefulness of log scales in comparing deaths attributed to COVID-19 between countries as of March 2020 is illustrated in \cref{fig:covid19-FT-deaths-march2020-log}; the diagonal reference lines provide a visual aid useful for interpretation (Burn-Murdoch et al., 2020).
As the pandemic evolved, and the case counts were no longer spreading exponentially, graphs with linear scales seemed more effective at spotting early increases in case counts that signaled more localized outbreaks. In \cref{fig:covid19-FT-june2020-case-counts-linear} and \cref{fig:covid19-FT-june2020-case-counts-log}, the daily case counts as of June 30, 2020 are displayed on both the linear and log scales respectively (Burn-Murdoch et al., 2020).
The effect of the linear scale \pcref{fig:covid19-FT-june2020-case-counts-linear} appears to evoke a stronger reaction from the public than the log scale \pcref{fig:covid19-FT-june2020-case-counts-log} as daily case counts are clearly rising rapidly during the summer wave.
This is only one recent example of a situation in which both log and linear scales are useful for showing different aspects of the same data(Fagen-Ulmschneider, 2020).

There are long histories of using log scales to display results in ecology, psychophysics, engineering, and physics (Heckler, Mikula, \& Rosenblatt, 2013; Menge et al., 2018).
In Waddell (2005), comparisons were made between the linear and logarithmic scales for the relationship between dosage and carcinogenicity in rodents.
Results favored the use of logarithmic scales for doses in order to put the relative doses into perspective whereas using a linear scale to administer doses to animals with the same chemicals to which humans are exposed does not provide useful, comparative information.
Given the widespread use of logarithmic scales, it is important to understand the implications of their use in order to provide guidelines for best use.

\begin{figure}[tbp]

{\centering \includegraphics[width=0.9\linewidth,]{images/covid19-FT-03.23.2020-log} 

}

\caption[Covid-19 deaths (log scale) as of March 23, 2020]{Covid-19 deaths (log scale) as of March 23, 2020.}\label{fig:covid19-FT-deaths-march2020-log}
\end{figure}

\begin{figure}[tbp]

{\centering \includegraphics[width=0.9\linewidth,]{images/covid19-FT-case-count-06.30.2020-linear} 

}

\caption[Covid-19 case counts (linear scale) as of June 30, 2020]{Covid-19 case counts (linear scale) as of June 30, 2020.}\label{fig:covid19-FT-june2020-case-counts-linear}
\end{figure}

\begin{figure}[tbp]

{\centering \includegraphics[width=0.9\linewidth,]{images/covid19-FT-case-count-06.30.2020-log} 

}

\caption[Covid-19 case counts (log scale) as of June 30, 2020]{Covid-19 case counts (log scale) as of June 30, 2020.}\label{fig:covid19-FT-june2020-case-counts-log}
\end{figure}

When we first learn to count, we begin counting by ones (for example, 1, 2, 3, etc.), then by tens (for example, 10, 20, 30, etc.), and advancing to hundreds (for example, 100, 200, 300, etc.), following the base10 order of magnitude system (for example, 1, 10, 100, etc.).
Research suggests our perception and mapping of numbers to a number line is logarithmic at first, but transitions to a linear scale later in development, with formal mathematics education (Dehaene, Izard, Spelke, \& Pica, 2008; Siegler \& Braithwaite, 2017, 2017; Varshney \& Sun, 2013).
For example, a kindergartner asked to place numbers one through ten along a number line would place three close to the middle, following the logarithmic perspective (Varshney \& Sun, 2013); \cref{fig:log-number-line} demonstrates how a kindergartner might map numbers along a number line.
Dehaene et al. (2008) found that with basic training, members of remote cultures with a basic vocabulary and minimal education understood the concept that numbers can be mapped into a spacial space; for example, numbers can be mapped to a number line or numbers can be mapped onto a clock.
There was a gradual transition from logarithmic to linear scale as the mapping of whole number magnitude representations transitioned from a compressed (approximately logarithmic) distribution to an approximately linear one.
These results indicate the universal and cultural-dependent characteristics of the sense of number.
Regardless of training, our visual system is still vulnerable to biases related to our perception of different stimuli such as weight, light, or sound.
Known as \emph{Weber's law}, it was established that we do not notice absolute changes in stimuli, but instead that we notice the relative change (Fechner, 1860).
The \emph{Weber-Fechner law} extended the discovery and states the relationship between the perceived intensity is logarithmic to the stimulus intensity when observed above a minimal threshold of perception.

\begin{figure}[tbp]

{\centering \includegraphics[width=0.65\linewidth,]{thesis_files/figure-latex/log-number-line-1} 

}

\caption[Kindergarten example of mapping numbers 1-10 along a number line]{Kindergarten example of mapping numbers 1-10 along a number line.}\label{fig:log-number-line}
\end{figure}

Assuming there is a direct relationship between perceptual and cognitive processes, it is reasonable to assume numerical representations should also be displayed on a nonlinear, compressed number scale. Therefore, if we perceive logarithmically by default, it is a natural (and presumably low effort) way to display information and should be easy to read and understand/use.
The idea is compression enlarges the coding space, thus increasing the dynamic range of perception and firing neurons within our visual system (Nieder \& Miller, 2003).
Similar to the training and education required to transition from logarithmic mapping to linear mapping, there is also necessary training required in the assessment of graphical displays associated with logarithmic scales. Haemer \& Kelley (1949) identify semi-logarithmic charts for temporal series as requiring a certain degree of technical training.

\hypertarget{underestimation}{%
\section{Underestimation of Exponential Growth}\label{underestimation}}

In addition to biases which result from the use of log scales, there is a general misinterpretation of exponential growth; \cref{fig:exponential-stages-comic} (Von Bergmann, 2021) illustrates how individuals in public health interpret exponential growth distinctly different from scientists during early, middle, and late stages of growth.
Exponential growth is often misjudged in early stages, appearing to have a small growth rate.
As exponential growth continues, the middle stage appears to be growing, but not at an astounding rate, appearing more quadratic.
It is not until late stages of exponential growth when it is quite apparent that there is exponential growth occurring.
This misinterpretation can lead to decisions made under inaccurate understanding causing future consequences.

\begin{figure}[tbp]

{\centering \includegraphics[width=1\linewidth,]{images/exponential-stages-comic} 

}

\caption[Log scale comic]{Comic illustrating the general misinterpretation of exponential growth.}\label{fig:exponential-stages-comic}
\end{figure}

Early studies explored the estimation and prediction of exponential growth and found that growth is underestimated when presented both numerically and graphically (Wagenaar \& Sagaria, 1975).
The hierarchy of plot objects found in Cleveland \& McGill (1985) can provide a possible explanation for the underestimation that occurs in exponentially increasing trends; the exponential trend can be thought of as a series of tangential angles leading to less accurate judgement of the next points.
Results from Wagenaar \& Sagaria (1975) indicated that numerical estimation is more accurate than graphical estimation for exponential curves.
Experimental studies were conducted in order to determine strategies to improve the accuracy of estimation of exponential growth (Gregory Jones, 1977; MacKinnon \& Wearing, 1991; Wagenaar \& Sagaria, 1975).
There was no improvement in estimation found when participants had contextual knowledge or experience with exponential growth, but instruction on exponential growth reduced the underestimation; participants adjusted their initial starting value but not their perception of the growth rate (Gregory Jones, 1977; Wagenaar \& Sagaria, 1975).
MacKinnon \& Wearing (1991) found that estimation was improved by providing immediate feedback to participants about the accuracy of their current predictions.

Our inability to accurately predict exponential growth might also be addressed by log transforming the data, however, this transformation introduces new complexities; most readers are not mathematically sophisticated enough to intuitively understand logarithmic math and translate that back into real-world effects.
In Menge et al. (2018), ecologists were surveyed to determine how often ecologists encounter log scaled data and how well ecologists understand log scaled data when they see it in the literature.
Participants were presented three relationships displayed on linear-linear scales, log-log scales with untransformed values, or log--log scales with log transformed values \pcref{fig:menge-plots}.
The authors propose three types of misconceptions participants encountered when presented data on log-log scales: `hand-hold fallacy', `Zeno's zero fallacy', and `watch out for curves fallacies'.
These misconceptions are a result of linear extrapolation assuming that a line in log-log space represents a line instead of the power law (which is an exponential relationship) in linear-linear space.

\begin{figure}[tbp]

{\centering \includegraphics[width=1\linewidth,]{images/menge-plots} 

}

\caption[Graphs viewed in Menge (2018) survey]{Graphs presented to participants in Menge (2018). Three relationships were displayed on the linear-linear scales, log-log scales with transformed values, or log-log scales with log transformed values. These figures demonstrate misconceptions participants encountered when presented data on the log-log scales.}\label{fig:menge-plots}
\end{figure}

The `hand-hold fallacy' stems from the misconception that steeper slopes in log-log relationships are steeper slopes in linear-linear space, illustrated in \cref{fig:menge-plots} d-f.~
In fact, it is not only the slope that matters, but also the intercept and the location on the horizontal axis since a line in log-log space represents a power law in linear-linear space (linear extrapolation).
Emerging from `Zeno's zero fallacy' is the misconception that positively sloped lines in log-log space can imply a non-zero value of y when x is zero, illustrated in \cref{fig:menge-plots} a-c and d-f.
This is never true as positively sloped lines in log-log space actually imply that \(y = 0\) when \(x = 0\). This misconception again is a result of linear extrapolation assuming that a line in log-log space represents a line instead of the power law in linear-linear space.
The last misconception, `watch out for curves fallacies' encompasses three faults: (1) lines in log-log space are lines in linear-linear space, illustrated in \cref{fig:menge-plots} d-f, (2) lines in log-log space curve upward in linear-linear space, illustrated in \cref{fig:menge-plots} d-f, and (3) curves in log-log space have the same curvature in linear-linear space, illustrated in \cref{fig:menge-plots} g-i.
Linear extrapolation is again responsible for the first and third faults while the second fault is a result of error in thinking that log-log lines represent power laws, and all exponential relationships curve upward; this is only true when the log-log slope is greater than one.
Menge et al. (2018) found that in each of these scenarios, participants were confident in their incorrect responses, indicating incorrect knowledge rather than a lack of knowledge.

\hypertarget{research-objectives}{%
\section{Research Objectives}\label{research-objectives}}

In this research, we conducted a series of three graphical studies to evaluate the impact displaying data on the log scale has on human perception of exponentially increasing trends compared to displaying data on the linear scale.
Each study was related to a different graphical task, each requiring a different level of interaction and cognitive use of the data being presented.
The first experiment evaluated whether our ability to perceptually notice differences in exponentially increasing trends is impacted by the choice of scale.
We conducted a visual inference experiment in which participants were shown a series of lineups and asked to identify the plot that differed most from the surrounding plots.
The other experimental tasks focus on determining whether there are cognitive disadvantages to log scales: do log scales make it harder to make use of graphical information?
To test an individual's ability to make predictions for exponentially increasing data, participants were asked to draw a line using their computer mouse through an exponentially increasing trend shown on both the linear and log scale.
In addition to differentiation and prediction of exponentially increasing data, an estimation task was conducted to test an individuals' ability to translate a graph of exponentially increasing data into real value quantities and extend their estimations by making comparisons.
Combined, the three studies provide a comprehensive evaluation of the impact of displaying exponentially increasing data on a log scale as it relates to perception, prediction, and estimation.
The results of these studies help us to make recommendations and provide guidelines for the use of log scales.

\hypertarget{lineups}{%
\chapter{Perception through lineups}\label{lineups}}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

To lay a foundation for future exploration of the use of log scales, we begin with the most fundamental ability: to identify differences in charts. Identifying differences does not require that participants understand exponential growth, identify log scales, or have any mathematical training.
Instead, we am simply testing the change in \emph{perceptual sensitivity} resulting from visualization choices.
The study in this chapter is conducted through visual inference and the use of statistical lineups (Buja et al., 2009a) to differentiate between exponentially increasing curves with differing levels of curvature, using linear and log scales.

\hypertarget{visual-inference}{%
\section{Visual Inference}\label{visual-inference}}

In \protect\hyperlink{testing-statistical-graphics}{Section 1.4}, we explained how a data plot can be evaluated and treated as a visual statistic, a numerical function which summarizes the data.
To evaluate a graph, the statistic (data plot) must be run through a visual evaluation - a person.
We can conclude the visual statistics are significantly different if two different methods of presenting data result in qualitatively different results when evaluated visually.
Recent graphical experiments have utilized statistical lineups to quantify the perception of graphical design choices (Hofmann, Follett, Majumder, \& Cook, 2012; Loy, Follett, \& Hofmann, 2016; Loy, Hofmann, \& Cook, 2017; VanderPlas \& Hofmann, 2017).
Statistical lineups provide an elegant way of combining perception and statistical hypothesis testing using graphical experiments (Majumder et al., 2013; Vanderplas et al., 2020; Wickham, Cook, Hofmann, \& Buja, 2010).
`Lineups' are named after the `police lineup' of criminal investigations where witnesses are asked to identify the criminal from a set of individuals.
Similarly, a statistical lineup is a plot consisting of smaller panels; the viewer is asked to identify the panel containing the real data from among a set of decoy null plots.
Null plots display data under the assumption there is no relationship and can be generated by permutation or simulation.
A statistical lineup typically consists of 20 panels - one target panel and 19 null panels.
If the viewer can identify the target panel randomly embedded within the set of null panels, this suggests that the real data is visually distinct from data generated under the null model.
\cref{fig:lineup-example} provides examples of statistical lineups.
The lineup plot on the left displays increasing exponential data displayed on a linear scale with panel 13 as the target; the lineup plot on the right displays increasing exponential data on the log base ten scale with panel 4 as the target.

\begin{figure}[tbp]

{\centering \includegraphics[width=\linewidth,]{thesis_files/figure-latex/lineup-example-1} 

}

\caption[Lineup examples]{The lineup plot on the left displays increasing exponential data on a linear scale with panel (2 x 5) + 3 as the target. The lineup plot on the right displays increasing exponential data on the log scale with panel 2 x 2 as the target.}\label{fig:lineup-example}
\end{figure}

While explicit graphical tests direct the participant to a specific feature of a plot to answer a specific question, implicit graphical tests require the user to identify both the purpose and function of the plot in order to evaluate the plots shown (Vanderplas et al., 2020).
Implicit graphical tests, such as lineups, have the advantage of simultaneously visually testing for multiple visual features including outliers, clusters, linear and nonlinear relationships. Responses from multiple viewers are collected through convenience sampling (in informal situations) or crowd sourcing websites such as Prolific, Amazon Mechanical Turk, and Reddit (in more formal situations).

\hypertarget{data-generation}{%
\section{Data Generation}\label{data-generation}}

In this study, both the target and null data sets were generated by simulating data from an exponential model; the models differ in the parameters selected for the null and target panels.
In order to guarantee the simulated data spans the same domain and range of values, we begin with a domain constraint of \(x\in [0,20]\) and a range constraint of \(y\in [10,100]\) with \(N = 50\) points randomly assigned throughout the domain and mapped to the \(y\)-axis using the exponential model with the selected parameters.
These constraints provide some assurance that participants who select the target plot are doing so because of their visual perception differentiating between curvature or growth rate rather than different starting or ending values.

Data were simulated based on a three-parameter exponential model with multiplicative errors:
\begin{align}
y_i & = \alpha\cdot e^{\beta\cdot x_i + \epsilon_i} + \theta \\
\text{with } \epsilon_i & \sim N(0, \sigma^2). \nonumber
\end{align}
The parameters \(\alpha\) and \(\theta\) are adjusted based on \(\beta\) and \(\sigma^2\) to guarantee the range and domain constraints are met.
The model generated \(N = 50\) points \((x_i, y_i), i = 1,...,N\) where \(x\) and \(y\) have an increasing exponential relationship.
The heuristic data generation procedure is described in \cref{alg:lineup-parameter-estimation-algorithm} and \cref{alg:lineup-exponential-data-simulation-algorithm}.

\begin{algorithm}
  \caption{Lineup Parameter Estimation}\label{alg:lineup-parameter-estimation-algorithm}
  \begin{algorithmic}[1]
    \Statex \textbullet~\textbf{Input Parameters:} domain $x\in[0,20]$, range $y\in[10,100]$, midpoint $x_{mid}$.
    \Statex \textbullet~\textbf{Output Parameters:} estimated model parameters $\hat\alpha, \hat\beta, \hat\theta$.
    \State Determine the $y=-x$ line scaled to fit the assigned domain and range.
    \State Map the values $x_{mid} - 0.1$ and $x_{mid} + 0.1$ to the $y=-x$ line for two additional points.
    \State From the set points $(x_k, y_k)$ for $k = 1,2,3,4$, obtain the coefficients from the linear regression model $\ln(y_k) = b_0 +b_1x_k$ to obtain starting values - $\alpha_0 = e^{b_0}, \beta_0 =  b_1, \theta_0 = 0.5\cdot \min(y)$
    \State Using the \texttt{nls} function from the base \texttt{stats} package in Rstudio and the starting parameter values - $\alpha_0, \beta_0, \theta_0$ - fit the nonlinear model, $y_k = \alpha\cdot e^{\beta\cdot x_k}+\theta$ to obtain estimated parameter values - $\hat\alpha, \hat\beta, \hat\theta.$
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}
  \caption{Lineup Exponential Data Simulation}\label{alg:lineup-exponential-data-simulation-algorithm}
  \begin{algorithmic}[1]
    \Statex \textbullet~\textbf{Input Parameters:} sample size $N = 50$, estimated parameters $\hat\alpha$, $\hat\beta$, and $\hat\theta$, from \cref{alg:lineup-parameter-estimation-algorithm}, and $\sigma$ standard deviation from the exponential curve.
    \Statex \textbullet~\textbf{Output Parameters:} $N$ points, in the form of vectors $\mathbf{x}$ and $\mathbf{y}$.
    \State Generate $\tilde x_j, j = 1,..., \frac{3}{4}N$ as a sequence of evenly spaced points in $[0,20]$. This ensures the full domain of $x$ is used, fulfilling the constraints of spanning the same domain and range for each parameter combination.
    \State Obtain $\tilde x_i, i = 1,...N$ by sampling $N = 50$ values from the set of $\tilde x_j$ values. This guarantees some variability and potential clustering in the exponential growth curve disrupting the perception due to continuity of points.
    \State Obtain the final $x_i$ values by jittering $\tilde x_i$.
    \State Calculate $\tilde\alpha = \frac{\hat\alpha}{e^{\sigma^2/2}}.$ This ensures that the range of simulated values for different standard deviation parameters has an equal expected value for a given rate of change due to the non-constant variance across the domain.
    \State Generate $y_i = \tilde\alpha\cdot e^{\hat\beta x_i + e_i}+\hat\theta$ where $e_i\sim N(0,\sigma^2).$
  \end{algorithmic}
\end{algorithm}

\hypertarget{lineups-parameter-selection}{%
\section{Parameter Selection}\label{lineups-parameter-selection}}

We followed a `Goldilocks' inspired procedure to choose three levels of trend curvature (low curvature, medium curvature, and high curvature). For each curvature level, we simulated 1000 data sets of \((x_{ij}, y_{ij})\) points for \(i = 1,...,50\) \(x\)-values and \(j = 1...10\) corresponding \(y\)-values per \(x\)-value.
Each generated \(x_i\) point from \cref{alg:lineup-exponential-data-simulation-algorithm} was replicated ten times.
On each of the individual data sets, we conducted a linear regression model and computed the lack of fit statistic (LOF) which measures the deviation of the data from the linear regression model.
The density curves of the LOF statistics for each level of curvature are plotted \pcref{fig:lof-density-curves} to to provide a metric for differentiating between the curvature levels and thus detecting the target plot.
While the LOF statistic provides a numerical value for discriminating between the difficulty levels, it cannot be directly related to the perceptual discriminability; it serves primarily as an approximation to ensure that we are testing parameters at several distinct curvature levels.
Final parameters used for data simulation are shown in \cref{tab:parameter-data}.

\begin{figure}[tbp]

{\centering \includegraphics[width=1\linewidth,]{thesis_files/figure-latex/lof-density-curves-1} 

}

\caption[Lineup parameter selection]{Density plot of the lack of fit statistic showing separation of difficulty levels: obvious curvature, noticable curvature, and almost linear.}\label{fig:lof-density-curves}
\end{figure}

\begin{table}

\caption{\label{tab:parameter-data}Lineup data simulation final parameters}
\centering
\begin{tabular}[t]{ccccccc}
\toprule
 & $x_{mid}$ & $\hat\alpha$ & $\tilde\alpha$ & $\hat\beta$ & $\hat\theta$ & $\hat\sigma$\\
\midrule
High Curvature & 14.5 & 0.91 & 0.88 & 0.23 & 9.10 & 0.25\\
Medium Curvature & 13.0 & 6.86 & 6.82 & 0.13 & 3.14 & 0.12\\
Low Curvature & 11.5 & 37.26 & 37.22 & 0.06 & -27.26 & 0.05\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{lineup-setup}{%
\section{Lineup Setup}\label{lineup-setup}}

Lineup plots were generated by mapping one simulated data set corresponding to curvature level A to a scatter plot to be identified as the target panel while multiple simulated data sets corresponding to curvature level B were individually mapped to scatter plots for the null panels.
The \texttt{nullabor} package in R (Buja et al., 2009b) was used to randomly assign the target plot to one of the panels surrounded by panels containing null plots.
For example, a target plot with simulated data following an increasing exponential curve with high curvature is randomly embedded within null plots with simulated data following an increasing exponential trend with low curvature.
By the implemented constraints, the target panel and null panels will span a similar domain and range.
There are a total of six lineup curvature combinations; \cref{fig:curvature-combination-example} illustrates the six lineup curvature combinations (top: linear scale; bottom: log scale) where the green line indicates the curvature level designated to the target plot while the black line indicates the curvature level assigned to the null plots.
Two sets of each lineup curvature combination were simulated (total of twelve test data sets) and plotted on both the linear scale and the log scale (total of 24 test lineup plots).
In addition, there are three curvature combinations which generate homogeneous ``Rorschach'' lineups, where all panels are from the same distribution.
Each participant evaluated one of these lineups, but for simplicity, these evaluations are not described in this chapter and their analysis is left to a later date.

\begin{figure}[tbp]

{\centering \includegraphics[width=1\linewidth,]{thesis_files/figure-latex/curvature-combination-example-1} 

}

\caption[Lineup curvature combinations]{Thumbnail plots illustrating the six curvature combinations displayed on both scales (linear and log). The green line indicates the curvature level to be identified as the target plot from amongst a set of null plots with the curvature level indicated by the black line.}\label{fig:curvature-combination-example}
\end{figure}

\hypertarget{study-design}{%
\section{Study Design}\label{study-design}}

Each participant was shown a total of thirteen lineup plots (twelve test lineup plots and one Rorschach lineup plot).
Participants were randomly assigned one of the two replicate data sets for each of the six unique lineup curvature combinations.
For each assigned test data set, the participant was shown the lineup plot corresponding to both the linear scale and the log scale.
For the additional Rorschach lineup plot, participants were randomly assigned one data set shown on either the linear or the log scale.
The order of the thirteen lineup plots shown was randomized for each participant.

Participants above the age of majority in their region were recruited from Prolific, a survey site that connects researchers to study participants.
Participants were compensated for their time and participated in all three related graphical studies consecutively.
The lineup study in this chapter was completed first in the series of graphical studies.
Previous literature suggests that prior mathematical knowledge or experience with exponential data is not associated with the outcome of graphical experiments (VanderPlas \& Hofmann, 2015).
Participants completed the series of graphical tests using a R Shiny application found \href{https://shiny.srvanderplas.com/perception-of-statistical-graphics/}{here}.

Participants were shown a series of lineup plots and asked to identify the plot that was most different from the others.
On each plot, participants were asked to justify their choice and provide their level of confidence in their choice.
The goal of this graphical task is to test an individuals ability to perceptually differentiate exponentially increasing trends with differing levels of curvature on both the linear and log scale.

\hypertarget{results}{%
\section{Results}\label{results}}

Participant recruitment and study deployment was conducted via Prolific, a crowd sourcing website, on Wednesday, March 23, 2022 during which 325 individuals completed 4492 unique test lineup evaluations.
Only participants who completed the lineup study were included in the final data set which included a total of 311 participants and 3958 lineup evaluations.
Each plot was evaluated between 141 and 203 times (Mean: 164.92, SD: 14.9).
Participants correctly identified the target panel in 47\% of the 1981 lineup evaluations made on the linear scale and 65.3\% of the 1977 lineup evaluations made on the log scale.

Target plot identification was analyzed using the \texttt{glmer} function in the \texttt{lme4} R package (Bates, Mächler, Bolker, \& Walker, 2015).
Estimates and odds ratio comparisons between the log and linear scales were calculated using the \texttt{emmeans} R package (Lenth, 2021).
Each lineup plot evaluated was assigned a binary value based on the participant response (correct = 1, not correct = 0).
Define \(Y_{ijkl}\) to be the event that participant \(l = 1,...,N_\text{participant}\) correctly identifies the target plot for data set \(k = 1,2\) with curvature combination \(j = 1,2,3,4,5,6\) plotted on scale \(i = 1,2\).
The binary response was analyzed using a generalized linear mixed model following a binomial distribution with a logit link function with a row-column blocking design accounting for the variation due to participant and data set respectively as
\begin{equation}
\text{logit }P(Y_{ijk}) = \eta + \delta_i + \gamma_j + \delta \gamma_{ij} + s_l + d_k
\end{equation}
\noindent where

\begin{itemize}
\item $\eta$ is the baseline average probability of selecting the target plot
\item $\delta_i$ is the effect of scale $i = 1,2$
\item $\gamma_j$ is the effect of curvature combination $j = 1,2,3,4,5,6$
\item $\delta\gamma_{ij}$ is the two-way interaction between the $i^{th}$ scale and $j^{th}$ curvature combination
\item $s_l \sim N(0,\sigma^2_\text{participant})$is the random effect for participant characteristics
\item $d_k \sim N(0,\sigma^2_{\text{data}})$ is the random effect for data specific characteristics. 
\end{itemize}

\noindent We assume that random effects for data set and participant are independent.

Results indicate a strong interaction between the curvature combination and scale (\(\chi^2_5 = 294.443\); \(\text{p} <0.0001\)). Variance due to participant and data set were estimated to be \(\hat\sigma^2_{\text{participant}} = 1.19\) (s.e. 1.09) and \(\hat\sigma^2_{\text{data}} = 0.433\) (s.e. 0.66) respectively.

On both the log and linear scales, the highest accuracy occurred in lineup plots where the target model and null model had a large curvature difference and the target plot had more curvature than the null plots (high curvature target plot embedded in low curvature null plots).
There is a decrease in accuracy on the linear scale when comparing a target plot with less curvature to null plots with more curvature (medium curvature target plot embedded in high curvature null plots; low curvature target plot embedded in medium curvature null plots; low curvature target plot embedded in high curvature null plots).
L. Best, Smith, \& Stubbs (2007) found that accuracy of identifying the correct curve type was higher when nonlinear trends were presented indicating that it is hard to say something is linear (something has less curvature), but easy to say that it is not linear; our results concur with this observation.
\cref{fig:odds-ratio-plot} displays the estimated (log) odds ratio of successfully identifying the target panel on the log scale compared to the linear scale. The thumbnail figures to the right of the plot illustrate the curvature combination on both the linear (left thumbnail) and log base ten (right thumbnail) scales associated with the \(y\)-axis label.
The choice of scale has no impact if curvature differences are large and the target plot had more curvature than the null plots (high curvature target plot embedded in low curvature null plots).
However, presenting data on the log scale makes us more sensitive to slight changes in curvature (low or high curvature target plot embedded in medium curvature null plots; medium curvature target plot embedded in high curvature null plots) and large differences in curvature when the target plot has less curvature than the null plots (low curvature target plot embedded in high curvature null plots).
An exception occurs when identifying a plot with curvature embedded in null plots close to a linear trend (medium curvature target panel embedded in low curvature null panels).
The results indicate that participants were more accurate at detecting the target panel on the linear scale than the log scale.
When examining this curvature combination, the same perceptual effect occurs as what we previously saw, but in a different context of scales.
On the linear scale, participants are perceptually identifying a curved trend from close to a linear trend whereas after the logarithmic transformation, participants are perceptually identifying a trend close to linear from a curved trend.
This again supports the claim that it is easy to identify a curve in a bunch of lines but much harder to identify a line in a bunch of curves (L. Best et al., 2007).

\begin{figure}[tbp]

{\centering \includegraphics[width=\linewidth,]{thesis_files/figure-latex/odds-ratio-plot-1} 

}

\caption[Lineups log(odds) results]{Estimated (log) odds ratio of successfully identifying the target panel on the log scale compared to the linear scale. The y-axis indicates the the model parameters used to simulate the null plots with the target plot model parameter selection designated by shape and shade of green. The thumbnail figures on the right display the curvature combination as shown in \cref{fig:curvature-combination-example} on both scales (linear - left, log - right).}\label{fig:odds-ratio-plot}
\end{figure}

\hypertarget{discussion-and-conclusion}{%
\section{Discussion and Conclusion}\label{discussion-and-conclusion}}

The overall goal of this chapter is to provide basic research to support the principles used to guide design decisions in scientific visualizations of exponential data.
In this study, we explored the use of linear and log scales to determine whether our ability to notice differences in exponentially increasing trends is impacted by the choice of scale.
The results indicated that when there was a large difference in curvature between the target plot and null plots and the target plot had more curvature than the null plots, the choice of scale had no impact and participants accurately differentiated between the two curves on both the linear and log scale.
However, displaying exponentially increasing data on a log scale improved the accuracy of differentiating between models with slight curvature differences or large curvature differences when the target plot had less curvature than the null plots.
An exception occurred when identifying a plot with curvature embedded in surrounding plots closely relating to a linear trend, indicating that it is easy to identify a curve in a group of lines but much harder to identify a line in a group of curves.
The use of visual inference to identify these guidelines suggests that there are \emph{perceptual} advantages to log scales when differences are subtle.
What remains to be seen is whether there are cognitive disadvantages to log scales: do log scales make it harder to make use of graphical information?

\hypertarget{youdrawit}{%
\chapter{Prediction with `You Draw It'}\label{youdrawit}}

\hypertarget{introduction-1}{%
\section{Introduction}\label{introduction-1}}

In \protect\hyperlink{lineups}{Chapter 2}, a base foundation for future exploration of the use of log scales was established by evaluating participants ability to identify differences in charts through the use of lineups.
This did not require that participants were able to understand exponential growth, identify log scales, or have any mathematical training; instead, it simply tested whether individuals are able to perceptually distinguish different curvature and slopes in a standard scatterplot.
This is necessary, but not sufficient, to determine whether individuals are capable of higher-level interaction with statistical data on log and linear scales.
In order to determine whether there are cognitive disadvantages to log scales, we utilized interactive graphics to test an individual's ability to make predictions for exponentially increasing data.
In this study, participants were asked to draw a line using their computer mouse through an exponentially increasing trend shown on both the log and linear scales.

\hypertarget{a-review-of-regression-and-prediction}{%
\subsection{A Review of Regression and Prediction}\label{a-review-of-regression-and-prediction}}

Our visual system is naturally built to look for structure and identify patterns.
For instance, points going down from left to right indicates a negative correlation between the \(x\) and \(y\) variables.
In the past, manual methods have been used to compare our intuitive visual sense of patterns to those determined by statistical methods.
Initial studies in the \(20^{th}\) century explored the use of fitting lines by eye through a set of points (Finney, 1951; Mosteller, Siegel, Trapido, \& Youtz, 1981).
Common methods of fitting trends by eye involve maneuvering a string, black thread, or ruler until the fit is suitable, then drawing the line through the set of points.

Researchers in Finney (1951) were interested in assessing the effect of stopping iterative maximum likelihood calculations after one iteration.
Many techniques in statistical analysis are performed with the aid of iterative calculations such as Newton's method or Fisher's scoring.
Guesses are made at the best estimates of certain parameters and these guesses are then used as the basis of a computation which yields a new set of approximation to the parameter estimates; this same procedure is then performed on the new parameter estimates and the computing cycle is repeated until convergence, as determined by the statistician, is reached.
The author was interested in whether one iteration of calculations was sufficient in the estimation of parameters connected with dose-response relationships.
One measure of interest in dose-response relationships is the relative potency between a test preparation of doses and standard preparation of does; relative potency is calculated as the ratio of two equally effective doses between the two preparation methods.
\cref{fig:subjective-judgement} shows a pair of parallel probit responses in a biological assay.
The \(x\)-axis is the \(\log_{1.5}\) dose level for four dose levels (for example, doses 4, 6, 9, and 13 correspond correspond to equally spaced values on a logarithmic scale, labeled 0, 1, 2, and 3) and the \(y\)-axis is the corresponding probit response as calculated in Finney \& Stevens (1948); circles correspond to the test preparation method while the crosses correspond to the standard preparation method.
For these sort of assays, the does-response relationship follows a linear regression of the probit response on the logarithm of the dose levels; the two preparation methods can be constrained to be parallel (Jerne \& Wood, 1949), limiting the relative potency to one consistent value.
In this study, twenty-one scientists were recruited via postal mail and asked to ``rule two lines'' in order to judge by eye the positions for a pair of parallel probit regression lines in a biological assay \pcref{fig:subjective-judgement}.
The author then computed one iterative calculation of the relative potency based on starting values as indicated by the pair of lines provided by each participant and compared these relative potency estimates to that which was estimated by the full probit technique (reaching convergence through multiple iterations).
Results indicated that one cycle of iterations for calculating the relative potency was sufficient based on the starting values provided by eye from the participants.

\begin{figure}[tbp]

{\centering \includegraphics[width=0.5\linewidth,]{images/02-you-draw-it/subjective-judgement-plot} 

}

\caption[Subjective Judgement in Statistical Analysis (1951) parallel probits]{Parallel probit responses in a biological assay shown to study participants in Subjective Judgement in Statistical Analysis (1951). The $x$-axis is the $\log_{1.5}$ dose level and the $y$-axis is the corresponding probit response; circles correspond to the test preparation method while the crosses correspond to the standard preparation method.}\label{fig:subjective-judgement}
\end{figure}

Thirty years later, Mosteller et al.~(1981), sought to understand the properties of least squares and other computed lines by establishing one systematic method of fitting lines by eye.
The authors recruited 153 graduate students and post doctoral researchers in Introductory Biostatistics.
Participants were asked to fit lines by eye to four sets of points \pcref{fig:mosteller-eyefitting-plot} using an 8.5 x 11 inch transparency with a straight line etched completely across the middle.
A latin square design (Anderson \& McLean, 1974) with packets of the set of points stapled together in four different sequences was used to determine if there is an effect of order of presentation; results indicated that order of presentation had no effect.
Without a formal analysis of the study, the researchers discussed the idea that participants tended to fit the slope of the first principal component (error minimized orthogonally, both horizontal and vertical, to the regression line) over the slope of the least squares regression line (error minimized vertically to the regression line) \pcref{fig:ols-vs-pca-example}.

\begin{figure}[tbp]

{\centering \includegraphics[width=0.7\linewidth,]{images/02-you-draw-it/eyefitting-straight-lines-plots} 

}

\caption[Eye Fitting Straight Lines (1981) data sets]{Scatter plots of the data shown to study participants in Eye Fitting Straight Lines (1981).}\label{fig:mosteller-eyefitting-plot}
\end{figure}

Recently, Ciccione \& Dehaene (2021) conducted a comprehensive set of studies investigating human ability to detect trends in graphical representations from a psychophysical approach.
Participants were asked to judge trends, estimate slopes, and conduct extrapolation.
To estimate slopes, participants were asked to report the slope of the best-fitting regression line using a track-pad to adjust the tilt of a line on the screen.
Results indicated the slopes participants reported were always in excess of the ideal slopes, both in the positive and in the negative direction, and those biases increase with noise and with number of points.
This supports the results found in Mosteller et al.~(1981) and suggest that participants might use Deming regression (Deming, DEMING, Sons, Chapman, \& (Londyn)., 1943), which is equivalent to a regression equation based ont the first principal component or principal axes and minimizes the Euclidean distance of points from the line, when fitting a line to a noisy scatter-plot.

While not explicitly intended for perceptual testing, in 2015, the New York Times introduced an interactive feature, called `You Draw It' (Aisch, Cox, \& Quealy, 2015; Buchanan, Park, \& Pearce, 2017; Katz, 2017), where readers input their own assumptions about various metrics and compare how these assumptions relate to reality.
The New York Times team utilizes Data Driven Documents (D3) that allow readers to predict these metrics through the use of drawing a line on their computer screen with their computer mouse.
\cref{fig:nyt-caraccidents} (Katz, 2017) is one such example in which readers were asked to draw the line for the missing years providing what they estimated to be the number of Americans who have died every year from car accidents, since 1990.
After the reader completed drawing the line, the actual observed values were revealed and the reader was able to check their estimated knowledge against the actual reported data.

\begin{figure}[tbp]

{\centering \includegraphics[width=0.75\linewidth,]{images/02-you-draw-it/nyt-caraccidents-frame4} 

}

\caption[New York Times 'You Draw It' feature]{New York Times 'You Draw It' feature; readers were asked to use their mouse to draw the line for the missing years in order to provide what they estimated to be the number of Americans who have died every year from car accidents, since 1990.}\label{fig:nyt-caraccidents}
\end{figure}

\hypertarget{data-driven-documents}{%
\subsection{Data Driven Documents}\label{data-driven-documents}}

Major news and research organizations such as the New York Times, FiveThirtyEight, Washington Post, and the Pew Research Center create and customize graphics with Data Driven Documents (D3).
In June 2020, the New York Times released a front page displaying figures that represent each of the 100,000 lives lost from the COVID-19 pandemic until that point in time (Barry et al., 2020); this visualization was meant to bring about a visceral reaction and resonate with readers.
During 2021 March Madness, FiveThirtyEight created a roster-shuffling machine which allowed readers to build their own NBA contender through interactivity (R. Best \& Boice, 2021).
Data Driven Documents (D3) is an open-source JavaScript based graphing framework created by Mike Bostock during his time working on graphics at the New York Times.
The grammar of D3 includes elements such as circles, paths, and rectangles with choices of attributes and styles such as color and size.
Data Driven Documents depend on Extensible Markup Language (XML) to generate graphics and images by binding objects and layers to the plotting area as Scalable Vector Graphics (SVG) in order to preserve the shapes rather than the pixels \pcref{fig:raster-vs-vector} (Tol, 2021).
Advantages of using D3 include animation and allowing for movement and user interaction such as hovering, clicking, and brushing.

\begin{figure}[tbp]

{\centering \includegraphics[width=0.7\linewidth,]{images/02-you-draw-it/raster-vs-vector} 

}

\caption{SVG vs raster}\label{fig:raster-vs-vector}
\end{figure}

A challenge of working with D3 is the environment necessary to display the graphics and images.
The \texttt{r2d3} package in R provides an efficient integration of D3 visuals and R by displaying them in familiar HTML output formats such as RMarkdown or Shiny applications (Luraschi \& Allaire, 2018).
The creator of the graphic applies \texttt{D3.js} source code to visualize data which has previously been processed within an R setting.

The example R code illustrates the structure of the \texttt{r2d3} function which includes specification of a data frame in R (converted to a JSON file), the D3.js source code file, and the D3 version that accompanies the source code.
A default SVG container for layering elements is then generated by the \texttt{r2d3} function which renders the plot using the source code.
\protect\hyperlink{youdrawit-with-shiny}{Appendix A} outlines the development of the `You Draw It' interactive plots used in this study through the use of \texttt{r2d3} and R shiny applications.
\cref{fig:youdrawit-example} provides an example of a `You Draw It' interactive plot as was shown to participants during the study.
The first frame shows what the participant saw along with the prompt, ``Use your mouse to fill in the trend in the yellow box region''.
Next, the yellow box region moved along as the participant drew their trend-line until the yellow region disappeared, indicating the participant had filled in the entire domain.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{r2d3}\NormalTok{(}\AttributeTok{data =}\NormalTok{ data, }\AttributeTok{script =} \StringTok{"d3{-}source{-}code.js"}\NormalTok{,}
    \AttributeTok{d3\_version =} \StringTok{"5"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[tbp]

{\centering \includegraphics[width=1\linewidth,]{images/02-you-draw-it/ydiExample-0.10-10-linear} 

}

\caption['You Draw It' task plot example]{Example of a 'You Draw It' interactive plot as shown to participants during the study. The first frame shows what the participant saw along with the prompt, \textit{'Use your mouse to fill in the trend in the yellow box region'}. Next, the yellow box region moved along as the participant drew their trend-line until the yellow region disappeared, indicating the participant had filled in the entire domain.}\label{fig:youdrawit-example}
\end{figure}

\hypertarget{study-design-1}{%
\section{Study Design}\label{study-design-1}}

This chapter contains two sub-studies; the first aims to establish `You Draw It' as a tool for measuring predictions of trends fitted by eye and a method for testing graphics, the second then applies `You Draw It' to test an individual's ability to make predictions for exponentially increasing data on the log and linear scale.
The first sub-study, referred to as Eye Fitting Straight Lines in the Modern Era, was intended to implement the `You Draw It' feature as a way to measure the patterns we see in data. We validate the `You Draw It' method for testing graphics by replicating the less technological study conducted by Mosteller et al. (1981).
Based on previous research, we hypothesize that visual regression tends to mimic principle component or Deming regression rather than an ordinary least squares regression.
In order to assess this hypothesis, we introduce a method for statistically modeling the participant drawn lines using generalized additive mixed models (GAMM).
The second sub-study, referred to as Prediction of Exponential Trends, uses the established `You Draw It' method to test an individual's ability to make predictions for exponentially increasing data on both the log and linear scales.
We then use the GAMMS to analyze participant drawn lines; a benefit of using a GAMM is the estimation of smoothing splines, allowing for flexibility in the residual trend and analysis of nonlinear trends.

A total of six data sets - four Eye Fitting Straight Lines in the Modern Era and two Prediction of Exponential Trends - are generated for each individual at the start of the experiment.
The two simulated data sets corresponding to the simulated data models used in the Prediction of Exponential Trends sub-study are then plotted a total of four times each with different aesthetic and scale choices for a total of eight task plots.
Participants in the study are first shown two `You Draw It' practice plots followed by twelve `You Draw It' task plots.
The order of all twelve task plots was randomly assigned for each individual in a completely randomized design where users saw the four task plots from the Eye Fitting Straight Lines in the Modern Era sub-study interspersed with the eight task plots from the Prediction of Exponential Trends sub-study.

The `You Draw It' study in this chapter was completed second in the series of the three graphical studies and took about fifteen minutes for participants to complete drawn trend lines for the twelve `You Draw It' task plots.
Participants completed the series of graphical tests using a R Shiny application found \href{https://shiny.srvanderplas.com/perception-of-statistical-graphics/}{here}.
Participant recruitment and study deployment was conducted via Prolific, a crowd sourcing website, on Wednesday, March 23, 2022 during which a total of 302 individuals completed 1254 unique `You Draw It' task plots for the first sub-study and 309 individuals completed 2520 unique `You Draw It' task plots associated with the second sub-study.

\hypertarget{eye-fitting-straight-lines-in-the-modern-era}{%
\section{Eye Fitting Straight Lines in the Modern Era}\label{eye-fitting-straight-lines-in-the-modern-era}}

Finney (1951) and Mosteller et al. (1981) use methods such as using a ruler, string, or transparency sheet to fit straight lines through a set of points.
This section replicates the study found in Mosteller et al. (1981) and extends this study with formal statistical analysis methods in order to establish `You Draw It' as a tool and method for testing graphics.

\hypertarget{data-generation-1}{%
\subsection{Data Generation}\label{data-generation-1}}

All data processing was conducted in R before being passed to the \texttt{D3.js} source code.
A total of \(N = 30\) points \((x_i, y_i), i = 1,...N\) were generated for \(x_i \in [x_{min}, x_{max}]\) where \(x\) and \(y\) have a linear relationship.
Data were simulated based on linear model with additive errors:
\begin{align}
y_i & = \beta_0 + \beta_1 x_i + e_i \\
\text{with } e_i & \sim N(0, \sigma^2). \nonumber
\end{align}
The parameters \(\beta_0\) and \(\beta_1\) were selected to replicate Mosteller et al. (1981) with \(e_i\) generated by rejection sampling in order to guarantee the points shown align with that of the fitted line.
An ordinary least squares regression was then fit to the simulated points in order to obtain the best fit line and fitted values in 0.25 increments across the domain, \((x_k, \hat y_{k,OLS}), k = 1, ..., 4 x_{max} +1\).
The data simulation function then outputted a list of point data and line data both indicating the parameter identification, \(x\) value, and corresponding simulated or fitted \(y\) value.
The data simulation procedure is described in \cref{alg:eyefitting-algorithm}.

\begin{algorithm}
  \caption{Eye Fitting Straight Lines in the Modern Era Data Simulation}\label{alg:eyefitting-algorithm}
  \begin{algorithmic}[1]
    \Statex \textbullet~\textbf{Input Parameters:} $y_{\bar{x}}$ for calculating the y-intercept, $\beta_0$; slope $\beta_1$; standard deviation from line $\sigma$; sample size of points $N = 30$; domain $x_{min}$ and $x_{max}$; fitted value increment $x_{by} = 0.25$.
    \Statex \textbullet~\textbf{Output Parameters:} List of point data and line data each indicating the parameter identification, $x$ value, and corresponding simulated or fitted $y$ value.
    \State Randomly select and jitter $N = 30$ $x$ values along the domain, $x_{i=1:N}\in [x_{min}, x_{max}]$.
    \State Determine the $y$-intercept, $\beta_0$, at $x = 0$ from the provided slope ($\beta_1$) and $y$ value at the mean of $x$ ($y_{\bar{x}}$) using point-slope equation of a line.
    \State Generate ``good" errors, $e_{i = 1:N}$ based on $N(0,\sigma)$ by setting a constraint requiring the mean of the first $\frac{1}{3}\text{N}$ errors $< |2\sigma|.$
    \State Simulate point data based on $y_i = \beta_0 + \beta_1 x_i + e_i$
    \State Obtain ordinary least squares regression coefficients, $\hat\beta_0$ and $\hat\beta_1$, for the simulated point data using the \texttt{lm} function in the \texttt{stats} package in base R.
    \State Obtain fitted values every 0.25 increment across the domain from the ordinary least squares regression $\hat y_{k,OLS} = \hat\beta_{0,OLS} + \hat\beta_{1,OLS} x_k$.
    \State Output data list of point data and line data each indicating the parameter identification, $x$ value, and corresponding simulated or fitted $y$ value.
  \end{algorithmic}
\end{algorithm}

\begin{table}

\caption{\label{tab:eyefitting-parameters}Eye Fitting Straight Lines in the Modern Era simulation model parameters}
\centering
\begin{tabular}[t]{cccc}
\toprule
Parameter Choice & $y_{\bar{x}}$ & $\beta_1$ & $\sigma$\\
\midrule
S & 3.88 & 0.66 & 1.30\\
F & 3.90 & 0.66 & 1.98\\
V & 3.89 & 1.98 & 1.50\\
N & 4.11 & -0.70 & 2.50\\
\bottomrule
\end{tabular}
\end{table}

Simulated model equation parameters were selected to reflect the four data sets (F, N, S, and V) used in Mosteller et al. (1981) \pcref{tab:eyefitting-parameters}.
Parameter choices F, N, and S simulated data across a domain of 0 to 20.
Parameter choice F produced a trend with a positive slope and a large variance while N had a negative slope and a large variance.
In comparison, S resulted a trend with a positive slope with a small variance and V yielded a steep positive slope with a small variance over the domain of 4 to 16.
\cref{fig:eyefitting-simplot} illustrates an example of simulated data for all four parameter choices intended to reflect the trends seen in \cref{fig:mosteller-eyefitting-plot}.
Aesthetic design choices were made consistent across each of the interactive `You Draw It' plots; the \(y\)-axis range extended 10\% beyond (above and below) the range of the simulated data points to allow for users to draw outside the simulated data set range and minimize participants anchoring their lines to the edges of the graph.

\begin{figure}[tbp]

{\centering \includegraphics[width=1\linewidth,]{thesis_files/figure-latex/eyefitting-simplot-1} 

}

\caption[Eye Fitting Straight Lines in the Modern Era simulated data example]{Scatter plots of example simulated data in Eye Fitting Straight Lines in the Modern Era sub-study. The four parameter choices were intended to reflect the trends seen in \cref{fig:mosteller-eyefitting-plot}.}\label{fig:eyefitting-simplot}
\end{figure}

\hypertarget{results-1}{%
\subsection{Results}\label{results-1}}

In addition to the participant drawn points, \((x_k, y_{k,drawn})\), and the ordinary least squares (OLS) regression fitted values, \((x_k, \hat y_{k,OLS})\), a regression equation with a slope based on the first principal component (PCA) was used to calculate fitted values, \((x_k, \hat y_{k,PCA})\).
For each set of simulated data and parameter choice, the PCA regression slope, \(\hat\beta_{1,PCA}\), and y-intercept, \(\hat\beta_{0,PCA}\), were determined by using the \texttt{mcreg} function in the \texttt{mcr} package in R (Schuetzenmeister \& Model, 2021) which implements Deming regression (equivalent to a regression based on the slope of the first principal component).
Fitted values, \(\hat y_{k,PCA}\) were then obtained every 0.25 increment across the domain from the PCA regression equation, \(\hat y_{k,PCA} = \hat\beta_{0,PCA} + \hat\beta_{1,PCA} x_k\).
\cref{fig:ols-vs-pca-example} illustrates the difference between an OLS regression equation which minimizes the vertical distance of points from the line and a regression equation with a slope calculated by the first principal component which minimizes the smallest distance of points from the line.

\begin{figure}[tbp]

{\centering \includegraphics[width=1\linewidth,]{thesis_files/figure-latex/ols-vs-pca-example-1} 

}

\caption[OLS vs PCA regression lines]{Comparison between an OLS regression equation which minimizes the vertical distance of points from the line and a regression equation with a slope calculated by the first principal component which minimizes the smallest distance of points from the line.}\label{fig:ols-vs-pca-example}
\end{figure}

For each participant, the final data set used for analysis contains \(x_{ijk}, y_{ijk,drawn}, \hat y_{ijk,OLS}\), and \(\hat y_{ijk,PCA}\) for parameter choice \(i = 1,2,3,4\), \(j = 1,...N_\text{participant}\), and \(x_{ijk}\) value \(k = 1, ...,4 x_{max} + 1\).
Using both a linear mixed model (LMM) and a generalized additive mixed model (GAMM), comparisons of vertical residuals in relation to the OLS fitted values (\(e_{ijk,OLS} = y_{ijk,drawn} - \hat y_{ijk,OLS}\)) and PCA fitted values (\(e_{ijk,PCA} = y_{ijk,drawn} - \hat y_{ijk,PCA}\)) were made across the domain.
\cref{fig:eyefitting-example-plot} displays an example of all three fitted trend lines for parameter choice F.

\begin{figure}[tbp]

{\centering \includegraphics[width=0.7\linewidth,]{thesis_files/figure-latex/eyefitting-example-plot-1} 

}

\caption[Eye Fitting Straight Lines in the Modern Era feedback data example]{Example of three trend lines showing the the OLS fitted, PCA fitted, and participant drawn values overlaid on the simulated data points.}\label{fig:eyefitting-example-plot}
\end{figure}

Using the \texttt{lmer} function in the \texttt{lme4} package (Bates et al., 2015), a LMM is fit separately to the OLS and PCA residuals, constraining the fit to a linear trend.
Parameter choice, \(x\), and the interaction between \(x\) and parameter choice were treated as fixed effects with a random participant effect accounting for variation due to participant.
The LMM equation for each fit (OLS and PCA) residuals is given by:
\begin{equation}
y_{ijk,drawn} - \hat y_{ijk,fit} = e_{ijk,fit} = \left[\gamma_0 + \alpha_i\right] + \left[\gamma_{1} x_{ijk} + \gamma_{2i} x_{ijk}\right] + p_{j} + \epsilon_{ijk}
\end{equation}
\noindent where

\begin{itemize}
\tightlist
\item
  \(y_{ijk,drawn}\) is the drawn y-value for the \(i^{th}\) parameter choice, \(j^{th}\) participant, and \(k^{th}\) increment of \(x\)-value
\item
  \(\hat y_{ijk,fit}\) is the fitted y-value for the \(i^{th}\) parameter choice, \(j^{th}\) participant, and \(k^{th}\) increment of \(x\)-value corresponding to either the OLS or PCA fit
\item
  \(e_{ijk,fit}\) is the residual between the drawn and fitted y-values for the \(i^{th}\) parameter choice, \(j^{th}\) participant, and \(k^{th}\) increment of \(x\)-value corresponding to either the OLS or PCA fit
\item
  \(\gamma_0\) is the overall intercept
\item
  \(\alpha_i\) is the effect of the \(i^{th}\) parameter choice (F, S, V, N) on the intercept
\item
  \(\gamma_1\) is the overall slope for \(x\)
\item
  \(\gamma_{2i}\) is the effect of the parameter choice on the slope
\item
  \(x_{ijk}\) is the \(x\)-value for the \(i^{th}\) parameter choice, \(j^{th}\) participant, and \(k^{th}\) increment
\item
  \(p_{j} \sim N(0, \sigma^2_\text{participant})\) is the random error due to the \(j^{th}\) participant's characteristics
\item
  \(\epsilon_{ijk} \sim N(0, \sigma^2)\) is the residual error.
\end{itemize}

Eliminating the linear trend constraint, the \texttt{bam} function in the \texttt{mgcv} package (S. Wood, 2003, 2004, 2011, 2017; S. Wood, Pya, \& Säfken, 2016) is used to fit a GAMM separately to the OLS and PCA residuals to allow for estimation of smoothing splines.
Parameter choice was treated as a fixed effect with no estimated intercept and a separate smoothing spline for \(x\) was estimated for each parameter choice.
A random participant effect accounting for variation due to participant and a random spline for each participant accounted for variation in spline for each participant.
The GAMM equation for each fit (OLS and PCA) residuals is given by:
\begin{equation}
y_{ijk, drawn} - \hat y_{ijk, fit} = e_{ijk,fit} = \alpha_i + s_{i}(x_{ijk}) + p_{j} + s_{j}(x_{ijk})
\end{equation}
\noindent where

\begin{itemize}
\tightlist
\item
  \(y_{ijk,drawn}\) is the drawn y-value for the \(i^{th}\) parameter choice, \(j^{th}\) participant, and \(k^{th}\) increment of \(x\)-value
\item
  \(\hat y_{ijk,fit}\) is the fitted y-value for the \(i^{th}\) parameter choice, \(j^{th}\) participant, and \(k^{th}\) increment of \(x\)-value corresponding to either the OLS or PCA fit
\item
  \(e_{ijk,fit}\) is the residual between the drawn and fitted y-values for the \(i^{th}\) parameter choice, \(j^{th}\) participant, and \(k^{th}\) increment of \(x\)-value corresponding to either the OLS or PCA fit
\item
  \(\alpha_i\) is the intercept for the parameter choice \(i\)
\item
  \(s_{i}\) is the smoothing spline for the \(i^{th}\) parameter choice
\item
  \(x_{ijk}\) is the \(x\)-value for the \(i^{th}\) parameter choice, \(j^{th}\) participant, and \(k^{th}\) increment
\item
  \(p_{j} \sim N(0, \sigma^2_\text{participant})\) is the error due to participant variation
\item
  \(s_{j}\) is the random smoothing spline for each participant.
\end{itemize}

\cref{fig:eyefitting-lmer-residualplots} and \cref{fig:eyefitting-gamm-residualplots} show the estimated trends of residuals (vertical deviation of participant drawn points from both the OLS and PCA fitted points) as modeled by a LMM and GAMM respectively.
A random sample of 75 participants was selected to display individual participant residuals behind the overall residual trend.
Examining the plots, the estimated trends of PCA residuals (orange) appear to align more parallel and closer to the \(y=0\) horizontal (dashed) line than the OLS residuals (blue).
In particular, this trend is more prominent in parameter choices with large variances (F and N).
These results are consistent to those found in Mosteller et al. (1981) indicating participants fit a trend line closer to the estimated regression line with the slope of the first principal component than the estimated OLS regression line.
This study established `You Draw It' as a method for graphical testing and reinforced the differences between intuitive visual model fitting and statistical model fitting, providing information about human perception as it relates to the use of statistical graphics.

\begin{figure}[tbp]

{\centering \includegraphics[width=1\linewidth,]{thesis_files/figure-latex/eyefitting-lmer-residualplots-1} 

}

\caption[Eye Fitting Straight Lines in the Modern Era LMM results]{Estimated trends of residuals (vertical deviation of participant drawn points from both the OLS (blue) and PCA (orange) fitted points) as fit by the linear mixed model. A random sample of 75 participants was selected to display the individual participant residuals behind the overall trend.}\label{fig:eyefitting-lmer-residualplots}
\end{figure}

\begin{figure}[tbp]

{\centering \includegraphics[width=1\linewidth,]{thesis_files/figure-latex/eyefitting-gamm-residualplots-1} 

}

\caption[Eye Fitting Straight Lines in the Modern Era GAMM results]{Estimated trends of residuals (vertical deviation of participant drawn points from both the OLS (blue) and PCA (orange) fitted points) as fit by the generalized additive mixed model. A random sample of 75 participants was selected to display the individual participant residuals behind the overall trend.}\label{fig:eyefitting-gamm-residualplots}
\end{figure}

\hypertarget{prediction-of-exponential-trends}{%
\section{Prediction of Exponential Trends}\label{prediction-of-exponential-trends}}

The results from the first sub-study validated `You Draw It' as a tool for testing graphics and introduced an appropriate statistical analysis method for comparing visually fitted trend lines to statistical regression results.
This sub-study was designed to test an individual's ability to make predictions for exponentially increasing data on both the log and linear scales, addressing cognitive understanding of log scales.
Participants were asked to draw a line using their computer mouse through the exponentially increasing trend shown on both the log and linear scale.

\hypertarget{data-generation-2}{%
\subsection{Data Generation}\label{data-generation-2}}

All data processing was conducted in R before being passed to the D3.js source code.
A total of \(N = 30\) points \((x_i, y_i), i = 1,...N\) were generated for \(x_i\in [x_{min}, x_{max}]\) where \(x\) and \(y\) have an exponential relationship.
Data were simulated based on a one parameter exponential model with multiplicative errors:
\begin{align}
y_i & = e^{\beta x_i + e_i} \\
\text{with } e_i & \sim N(0, \sigma^2). \nonumber
\end{align}
The parameter, \(\beta\), was selected to reflect the rate of exponential growth with \(e_i\) generated by rejection sampling in order to guarantee the points shown align with that of the fitted line displayed in the initial plot frame.
A nonlinear least squares regression is then fit to the simulated points in order to obtain the best fit line and fitted values in 0.25 increments across the domain, \((x_m, \hat y_{m,NLS}), k = 1, ..., 4 x_{max} +1\).
The data simulation function then outputs a list of point data and line data both indicating the parameter identification, \(x\) value, and corresponding simulated or fitted \(y\) value.
The data simulation procedure is described in \cref{alg:exponential-prediction-alg}.

\begin{algorithm}
  \caption{Prediction of Exponential Trends Data Simulation}\label{alg:exponential-prediction-alg}
  \begin{algorithmic}[1]
    \Statex \textbullet~\textbf{Input Parameters:} $\beta$ growth rate; standard deviation from exponential curve $\sigma$; sample size of points $N = 30$; domain $x_{min}$ and $x_{max}$; fitted value increment $x_{by} = 0.25$.
    \Statex \textbullet~\textbf{Output Parameters:} List of point data and line data each indicating the parameter identification, $x$ value, and corresponding simulated or fitted $y$ value.
    \State Randomly select and jitter $N = 30$ $x$-values along the domain, $x_{i=1:N}\in [0, 20]$.
    \State Generate ``good" errors, $e_{i = 1:N}$ based on $N(0,\sigma)$ by setting a constraint requiring the mean of the first $\frac{1}{3} N$ errors $< |2\sigma|.$
    \State Simulate point data based on $y_i = e^{\beta x_i + e_i}$.
    \State Fit the equation $\log(y_i) = \beta x_i$ to obtain an estimated starting value $\beta_0$. 
    \State Obtain nonlinear least squares regression coefficient, $\hat\beta_{NLS}$, for the simulated point data fitting using the \texttt{nls} function in the base \texttt{stats} R package.
    \State Obtain fitted values every 0.25 increment across the domain from the nonlinear least squares regression $\hat y_{m,NLS} = e^{\hat\beta_{NLS} x_m}$.
    \State Output data list of point data and line data each indicating the parameter identification, $x$ value, and corresponding simulated or fitted $y$ value.
  \end{algorithmic}
\end{algorithm}

Model equation parameter, \(\beta\), was selected to reflect two exponential growth rates (low: \(\beta = 0.10, \sigma = 0.09\) and high: \(\beta = 0.23, \sigma = 0.25\)) as determined by visual inspection with growth rate parameter selection from the lineup study in \protect\hyperlink{lineups-parameter-selection}{Chapter 2} used as a starting point.
Each growth rate parameter was used to simulate data across a domain of 0 to 20.
The two simulated data sets (low and high exponential growth rates) were then shown four times each by truncating the points shown at both 50\% and 75\% of the domain as well as on both the log and linear scales for a total of eight interactive plots reflecting a factorial treatment design.
\protect\hyperlink{exponential-prediction-plots}{Appendix B} displays visual examples of all eight interactive plots.
Aesthetic design choices were made consistent across each of the interactive `You Draw It' plots; the \(y\)-axis extended 50\% below the lower limit of the simulated data range and 200\% beyond the upper limit of the simulated data range to allow for users to draw outside the data set range, and participants were asked to start drawing at 50\% of the domain (for example, at \(x = 10\)).
Reflecting the treatment design for each plot, the y-axis was assigned to be displayed on either the linear scale or log scale.

\hypertarget{results-2}{%
\subsection{Results}\label{results-2}}

A LOESS smoother (local regression) was fit to each user line to allow for visual inspection.
For each participant \(l = 1,...N_\text{participant}\), the final data set used for analysis contained \(x_{ijklm}, y_{ijklm,drawn}, \hat y_{ijklm,loess}\), and \(\hat y_{ijklm,NLS}\) for growth rate \(i = 1,2\), points truncated \(j = 1,2\), scale \(k = 1,2\) and \(x_{ijklm}\) value for increment \(m = 1, ...,81\).
\cref{fig:exponential-yloess-spaghetti-plot} displays spaghetti plots for each of the eight treatment combinations.
The spaghetti plot with a high growth rate suggests participants underestimated the exponential trend when asked to draw a trend line on the linear scale compared to when asked to draw a trend line on the log scale.
In particular, this suggestion is most noticeable when points are truncated at 50\% with the underestimation beginning at a later \(x\) value when points are truncated at 75\%.

\begin{figure}[tbp]

{\centering \includegraphics[width=1\linewidth,]{thesis_files/figure-latex/exponential-yloess-spaghetti-plot-1} 

}

\caption[Exponential prediction spaghetti plot]{Spaghetti plot of results from the exponential prediction sub-study. Participants drawn lines on the linear scale are shown in blue and the log scale are shown in orange. Variability in the statistically fitted regression lines occured due to a unique data set being simulated for each individual; the gray band shows the range fitted values from the statistically fitted regression lines.}\label{fig:exponential-yloess-spaghetti-plot}
\end{figure}

Allowing for flexibility, the \texttt{bam} function in the \texttt{mgcv} package (S. Wood, 2003, 2004, 2011, 2017; S. Wood et al., 2016) was used to fit a GAMM to estimate trends of vertical residuals from the participant drawn line in relation to the NLS fitted values (\(e_{ijklm,NLS} = y_{ijklm,drawn} - \hat y_{ijklm,NLS}\)) across the domain.
The combination between growth rate, point truncation, and scale was treated as a fixed effect with no estimated intercept and a separate smoothing spline for \(x\) was estimated for each treatment combination.
A random participant effect accounting for variation due to participant and a random spline for each participant accounted for variation in spline for each participant.
The GAMM equation for residuals is given by:
\begin{equation}
y_{ijklm,drawn} - \hat y_{ijklm,NLS} = e_{ijklm,nls} = \tau_{ijk} + s_{ijk}(x_{ijklm}) + p_{l} + s_{l}(x_{ijklm})
\end{equation}
\noindent where

\begin{itemize}
\tightlist
\item
  \(y_{ijklm,drawn}\) is the drawn y-value for the \(l^{th}\) participant, \(m^{th}\) increment, and \(ijk^{th}\) treatment combination
\item
  \(\hat y_{ijklm,NLS}\) is the NLS fitted y-value for the \(l^{th}\) participant, \(m^{th}\) increment, and \(ijk^{th}\) treatment combination
\item
  \(e_{ijklm,NLS}\) is the residual between the drawn y-value and fitted y-value for the \(l^{th}\) participant, \(m^{th}\) increment, and \(ijk^{th}\) treatment combination
\item
  \(\tau_{ijk}\) is the intercept for the \(i^{th}\) growth rate, \(j^{th}\) point truncation, and \(k^{th}\) scale treatment combination
\item
  \(s_{ijk}\) is the smoothing spline for the \(ijk^{th}\) treatment combination
\item
  \(x_{ijklm}\) is the \(x\)-value for the \(l^{th}\) participant, \(m^{th}\) increment, and \(ijk^{th}\) treatment combination
\item
  \(p_{l} \sim N(0, \sigma^2_\text{participant})\) is the error due to the \(l^{th}\) participant's characteristics
\item
  \(s_{l}\) is the random smoothing spline for the \(l^{th}\) participant.
\end{itemize}

\cref{fig:exponential-prediction-gamm-preds} shows the estimated trends of the residuals (vertical deviation of participant drawn points from NLS fitted points) as modeled by the GAMM.
Examining the plots, the estimated trends of residuals for predictions made on the linear scale (blue) appear to deviate from the \(y=0\) horizontal (dashed) line indicating underestimation of exponential growth.
In comparisons, the estimated trends of residuals for predictions made on the log scale (orange) follow closely to the \(y=0\) horizontal (dashed) line, implying exponential trends predicted on the log scale are more accurate than those predicted on the linear scale.
In particular, this trend is more prominent in high exponential growth rates where underestimation becomes prominent after the aid of points is removed.

\begin{figure}[tbp]

{\centering \includegraphics[width=1\linewidth,]{thesis_files/figure-latex/exponential-prediction-gamm-preds-1} 

}

\caption[Exponential prediction GAMM results]{Estimated trends of residuals (vertical deviation of participant drawn points from NLS fitted points) as fit by the generalized additive mixed model. Deviation for visual trends predicted on the linear scale are shown in blue and deviation for visual trends predicted on the log scale are shown in orange. A random sample of 75 participants was selected to display the individual participant residuals behind the overall trend.}\label{fig:exponential-prediction-gamm-preds}
\end{figure}

\hypertarget{discussion-and-conclusion-1}{%
\section{Discussion and Conclusion}\label{discussion-and-conclusion-1}}

The intent of this chapter was to establish `You Draw It' as a method and tool for testing graphics then use this tool to determine the cognitive implications of displaying data on the log scale.
Eye Fitting Straight Lines in the Modern Era replicated the results found in Mosteller et al. (1981).
When shown points following a linear trend, participants tended to fit the slope of the first principal component over the slope of the least squares regression line.
This trend was most prominent when shown data simulated with larger variances.
The reproducibility of these results serve as evidence of the reliability of the `You Draw It' method.

In Prediction of Exponential Trends, the `You Draw It' method was used to test an individual's ability to make predictions for exponentially increasing data.
Results indicate that underestimation of exponential growth occurs when participants were asked to draw trend lines on the linear scale and that there was an improvement in accuracy when trends were drawn on the log scale.
This phenomena is strongly supported for high exponential growth rates.
Improvement in predictions are made when points along the exponential trend are shown as indicated by the discrepancy in results for treatments with points truncated at 50\% compared to 75\% of the domain.

The results of this study suggest that there are cognitive advantages to log scales when making predictions of exponential trends.
Participants' predictions were more accurate at high growth rates when participants drew trend lines on the log scale compared to the linear scale.
Further investigation is necessary to determine the implications of using log scales when translating exponential graphs to numerical values; we address this problem in the next chapter.

\hypertarget{estimation}{%
\chapter{Numerical Translation and Estimation}\label{estimation}}

\hypertarget{introduction-2}{%
\section{Introduction}\label{introduction-2}}

The previous two chapters explored the use of log scales through differentiation and visual prediction of trends.
These graphical tasks were conducted independent of context - no information about the data itself or even numerical scale values were provided to participants; instead, participants focused how our visual system perceives and identifies patterns in exponential growth.
In order to understand the cognitive implications of displaying exponentially increasing data on a log scale, it is essential to evaluate graph comprehension as it relates to the contextual scenario of the data shown.
This is a complex inferential process which requires participants to engage with the data by quantitatively transforming information in the chart (Cleveland \& McGill, 1984, 1985).
In this study, we asked participants to translate a graph of exponentially increasing data into real value quantities and extend their estimations by comparing two data points.

\hypertarget{graph-comprehension-1}{%
\subsection{Graph Comprehension}\label{graph-comprehension-1}}

Graph comprehension is heavily dependent on the questions being asked of the viewer; therefore, how these questions are phrased is an important aspect of comprehension and must be given deliberate consideration (Graesser et al., 2014).
Evaluation of how viewers explore a new and complex graphic requires long-term interaction with the chart displaying the data (Becker, Moore, \& Lawrence, 2019).
While it is difficult to obtain an accurate representation of a viewers understanding of the graphic with a fixed set of numerical estimates, three levels of graph comprehension have emerged from literature (Curcio, 1987; Friel et al., 2001; Glazer, 2011; Jolliffe, 1991; R. Wood, 1968).
The three behaviors related to graph comprehension involve (1) literal reading of the data (elementary level), (2) reading between the data (intermediate level), and (3) reading beyond the data (advanced level).
We present examples of questions associated with the three levels of questioning in \protect\hyperlink{lit-questioning}{Section 1.6.1}.
For instance, if shown a line graph of the value of a certain stock over time, an elementary level question might prompt the viewer to answer, ``what was the value of stock X on June 15th?'' and an intermediate level question would extend these estimates to ask the viewer, ``over the first five days, how did the value of stock X change'' (Friel et al., 2001).
In addition to the graph's visual features and questioning, it is important for researchers to give careful consideration to the context of the graphic on the viewers comprehension.

\hypertarget{estimation-biases-1}{%
\subsection{Estimation Biases}\label{estimation-biases-1}}

Certain well-known biases such as the tendency to round to multiples of five or ten or to anchor estimates to visual cues arise from open-ended estimation tasks (Tan \& Benbasat, 1990).
Viewers may anchor their estimates to grid lines or round their approximations to rounded figures due to natural preference (Godlonton et al., 2018; Myers, 1954; Tan, 1994).
Estimation accuracy is also affected by scale and axis labels (Dunham \& Osborne, 1991); when alternate grid lines are labeled, viewers often read unlabeled grid lines as halves (Beeby \& Taylor, 1973).
This misrepresentation is highlighted for asymmetric scales, such as a log scale, since spatial distance does not equate to numerical or quantitative difference.
Therefore, careful consideration must be given to the choice of scale for the graphic and how the viewer will interpret the data and information displayed.

\hypertarget{study-design-2}{%
\section{Study Design}\label{study-design-2}}

Participants in this study were asked to answer six questions related to each of two contextual scenarios and an associated scatter plot shown for a total of twelve questions.
The text for each scenario is presented below; the context of both scenarios was selected to be similar.
Each text describes a situation in which a fictional intergalactic species is exponentially increasing in population over a time chosen to reflect the popular culture media depiction of that species (\emph{Star trek}, 1967; \emph{Star wars}, 1977, 1983).
For simplicity, we will refer to these fictional time components as a year throughout the rest of the chapter.

\begin{quote}
\textbf{\textit{Tribble scenario.}} Hi, we're Tribbles! We were taken from our native planet, Iota Germinorum IV, and brought abroad Starfleet in stardate 4500. A Starfleet scientist, Edward Larkin, genetically engineered us to increase our reproductive rate in an attempt to solve a planetary food shortage. The Tribble population on Starfleet over the next 50 Stardates (equivalent to 1 week universe time) is illustrated in the graph. We need your help answering a few questions regarding the population of Tribbles.

\textbf{\textit{Ewok scenario.}} Hi, we're Ewoks! We are native to the forest moon of Endor. After the Galactic Civil War, some Ewoks traveled offworld to help Rebel veterans as 'therapy Ewoks' and began to repopulate. The Ewok population After the Battle of Yavin (ABY) is illustrated in the graph. We need your help answering a few questions regarding the population of Ewoks offworld.
\end{quote}

Fictional illustrations of the figures used in context were modified from artwork by Horst (2021) and included on the main page for each scenario.
The scale of the graphic and data set displayed was randomly assigned to scenarios for each individual.
For instance, a participant may have seen a scatter plot of data set two displayed on the linear scale paired with the Ewok scenario text and a scatter plot of data set one displayed on the log scale paired with the Tribble scenario text.
The order of the two scenarios and their assigned data set and scale was randomly assigned to each individual.

We selected the six questions \pcref{tab:estimation-questions-table} for graph comprehension based on the three defined levels of questioning.
In each scenario, participants were first asked an open ended question, which required them to spend time exploring the data displayed in the graphic, followed by a random order of two elementary level questions and three intermediate level questions.
We did not focus on advanced level questioning since extrapolation and interpolation was addressed in \protect\hyperlink{youdrawit}{Chapter 2}.

\begin{table}

\caption{\label{tab:estimation-questions-table}Estimation questions}
\centering
\begin{tabular}[t]{l>{\raggedright\arraybackslash}p{10em}>{\raggedright\arraybackslash}p{10em}}
\toprule
Question type & Tribble scenario & Ewok scenario\\
\midrule
Open Ended & Between stardates 4530 and 4540, how does the population of Tribbles change? & Between 30 and 40 ABY, how does the population of Ewoks change?\\
Elementary Q1 & What is the population of Tribbles in stardate 4510? & What is the population of Ewoks in 10 ABY?\\
Elementary Q2 & In what stardate does the population of Tribbles reach 4,000? & In what ABY does the population of Ewoks reach 4,000?\\
Intermediate Q1 & From 4520 to 4540, the population increases by \_\_\_\_ Tribbles. & From 20 ABY to 40 ABY, the population increases by \_\_\_\_ Ewoks.\\
Intermediate Q2 & How many times more Tribbles are there in 4540 than in 4520? & How many times more Ewoks are there in 40 ABY than in 20 ABY?\\
\addlinespace
Intermediate Q3 & How long does it take for the population of Tribbles in stardate 4510 to double? & How long does it take for the population of Ewoks in 10 ABY to double?\\
\bottomrule
\end{tabular}
\end{table}

The estimation study in this chapter was completed last in the series of the three graphical studies and took about fifteen minutes for participants to answer all twelve estimation questions.
Participants completed the series of graphical tests using a R Shiny application found \href{https://shiny.srvanderplas.com/perception-of-statistical-graphics/}{here}.
For each of the quantitative translation questions, participants were provided a basic calculator and scratchpad to aid in their estimation of values.
We recorded the inputted and evaluated calculations and scratch work of each participant in order to better understand participant strategies for estimation.

\hypertarget{data-generation-3}{%
\section{Data Generation}\label{data-generation-3}}

We generated two unique data sets with the same underlying parameter coefficients, but different errors randomly generated from the same error distribution.
For each data set, a total of \(N = 50\) points \((x_i, y_i), i = 1,...N\) were generated for single increments of \(x_i\in [0, 50]\) where \(x\) and \(y\) have an exponential relationship.
Data were simulated based on a three parameter exponential model with multiplicative errors:
\begin{align}
y_i & = \alpha e^{\beta x_i + e_i} + \theta \\
\text{with } e_i & \sim N(0, \sigma^2). \nonumber
\end{align}
The underlying parameter coefficients were selected to follow a similar growth rate and shape as the previous two studies by visual inspection while ensuring in a maximum magnitude of around 50,000.
The resulting parameters selected for data generation were \(\alpha = 130\), \(\beta = 0.12\), \(\theta = 50\), and \(\sigma = 1.5\).

\begin{figure}[tbp]

{\centering \includegraphics[width=1\linewidth,]{thesis_files/figure-latex/estimation-simulated-data-1} 

}

\caption[Estimation simulated data]{Scatter plots of the two unique data sets displayed on both the linear and log base two scales.}\label{fig:estimation-simulated-data}
\end{figure}

\cref{fig:estimation-simulated-data} display scatter plots of the two unique data sets on both the linear and log base two scales; a log of base two was selected in order to aid in participants estimation of time until the population doubled in `Intermediate Q3' \pcref{tab:estimation-questions-table}.
Participants were shown the graphic of both data sets on either the linear or log scale with labels adjusted to reflect the associated scenario context and scale.
Grid lines for the \(y\)-axis were set to be consistent for the same scale across both data sets with the linear scale increasing by 5,000 and the log base two scale doubling, thus demonstrating the additive and multiplicative contextual appearance and interpretation of each scale respectively.
Minor \(y\)-axis grid lines were removed to avoid participants anchoring to the midway point between grid lines; this is particularly important on the log scale since a half-way grid line spatially does not correspond to a half-way point numerically.
Grid lines for the \(x\)-axis spanned a range of 50 years with major grid lines every ten years apart and minor grid lines indicating every five years.
The time unit labels on the \(x\)-axis reflected 0 to 50 ABY (After Battle of Yavin) for the Ewok scenario and were adjusted to 4500 to 4550 Stardates for the Tribble Scenario to align with the associated popular media depiction of each figure as well as disguise the use of the same underlying data simulation model and estimation questions across both scenarios.

\hypertarget{results-3}{%
\section{Results}\label{results-3}}

Participant recruitment and study deployment was conducted via Prolific, a crowd sourcing website, on Wednesday, March 23, 2022 during which 302 individuals each completed all six estimation questions for each scenario (total of twelve questions per individual).
The data set used for analysis contained the unique participant identification and indicated the scenario, scale, data set, and estimation question along with the participant text response or quantitative estimate, calculation input and evaluation, and associated scratch work.
A total of 145 participants answered questions related to data set one on the linear scale and data set two on the log scale with 157 participants answering questions related to data set one on the log scale and data set two on the linear scale.
Sketches for each question are used to demonstrate the estimation tasks participants were asked to conduct.
An array of graphical displays allow for visual inspection of participant responses and provide suggestions about the cognitive implications of displaying exponentially increasing data on the log scale.

\hypertarget{open-ended}{%
\subsection{Open Ended}\label{open-ended}}

Before participants were asked to estimate numeric quantities, they were asked to provide an open ended response and describe how the population changed over time.
This required participants to spend time exploring the graphic and reflect upon how the data displayed related to the contextual application.
The \texttt{tidytext} and \texttt{corpus} packages in R (Perry, 2021; Silge \& Robinson, 2016) were used to extract and stem words from participant text responses; stop words such as `the' and `is' as well as numbers were removed from the cleaned word responses.
The \texttt{wordcloud} package (Fellows, 2018) was used to create a cloud comparing frequencies of words across the two scales \pcref{fig:estimation-word-cloud}.
The comparison word cloud is generated by defining \(p_{i,j}\) as the rate in which word \(i\) occurs when describing the data on scale \(j\) where \(p_j\) is the average rate across the scales \(\sum_i{\frac{p_{i,j}}{\text{N scales}}}\).
The maximum deviation for each word is calculated by \(max_i(p_{i,j} - p_j)\) and mapped to the size of the word with the position of the word determined by the scale in which the maximum occurs.

\begin{figure}[tbp]

{\centering \includegraphics[width=0.75\linewidth,]{thesis_files/figure-latex/estimation-word-cloud-1} 

}

\caption[Estimation word cloud]{The open ended question results are displayed in a comparison word cloud which compares frequencies of words across the two scales. The maximum deviation in frequency is mapped to the size of the word and the position and color of the word is determined by the scale - linear (blue), log (orange).}\label{fig:estimation-word-cloud}
\end{figure}

The comparison word cloud illustrates the general terminology participants used when describing the scatter plots shown on each scale.
Participants more frequently referred to terms such as `exponential' and `rapid' when shown the scatter plot on the linear scale while `double' and `quadruple' were often used to describe the graphic when shown on the log scale; indicating participants read the \(y\)-axis labels and noticed the doubling grid lines.
Participants often used ``triple'' to describe the data when displayed on the linear scale; one explanation might be that participants were roughly estimating the multiplicative change between grid lines.
For example, in year 40, the trend lands roughly around 15,000 and ends near 45,000 (three times as large) in year 50.
The use of the term `linear' when participants are describing the appearance of the data displayed on the log scale suggests that a portion of participants described the visual appearance of the data independent of the axis labels; without further context, we do not have enough information to determine whether this implies participants were not recognizing the data was exponentially increasing rather than linearly increasing due to the change in contextual appearance caused by the choice of scale.

\hypertarget{eq1-results}{%
\subsection{Elementary Q1: Estimation of population}\label{eq1-results}}

In order to examine the effect of scale on literal reading of the data, participants were asked, \textit{"What is the population in year 10?"} \pcref{fig:qe1-sketch}.
The true estimated population in year 10 based on the underlying parameter estimates was 481.61 with simulated points of 445.48 and 466.9 for data sets one and two respectively.
The median participant estimate across both scales and data sets was 500, with innerquartile ranges of 500 and 400 for data set one and data set two respectively when displayed on the linear scale and 48 and 12 for data set one and data set two respectively when displayed on the log scale.

\begin{figure}[tbp]

{\centering \includegraphics[width=1\linewidth,]{images/03-estimation/qe1-sketch} 

}

\caption[Elementary Q1 sketch]{Sketch of the estimation procedure asked in Elementary Q1. Participants first locate 10 along the $x$-axis and move upward until they beleive they have found the correct location on the curve; then participants look to the $y$-axis for their estimated population.}\label{fig:qe1-sketch}
\end{figure}

Density plots were used to illustrate the distribution of the quantitative estimates provided by participants.
\pcref{fig:qe1-density-plot-10-all} reveals a larger variance in quantitative population estimates made on the linear scale compared to the log scale.
As expected, it is clear that participants were anchoring to grid lines and base ten values as highlighted by the high density of estimates at 512 and 500 on the log scale as well as local maximums near multiples of ten such as 500 and 1000.

\begin{figure}[tbp]

{\centering \includegraphics[width=1\linewidth,]{thesis_files/figure-latex/qe1-density-plot-10-all-1} 

}

\caption[Elementary Q1 density in year 10]{Density of the participant estimates for the population in year 10. Colors are associated to scale - linear (blue) and log (orange) - and vertical lines indicate the true value based on the underlying model equation (black solid), closest point value based on the simulated data set (black dashed), and grid lines shown on the graphs (blue/orange dotted). A jittered rug plot along the $x$-axis shows where participant estimates were made. The two unique data sets are shown separately.}\label{fig:qe1-density-plot-10-all}
\end{figure}

During the study, participants were explicitly asked to estimate the population during year 10; this value corresponds to a low magnitude where the population is condensed in a small region on the linear scale as opposed to later in time when larger magnitudes in population can be seen.
While the results provided support for less variability in the estimated population in year 10 on the log scale, it is important to evaluate the accuracy of estimates along the full domain.
In two estimation questions related to intermediate level reading between the data, participants are asked to provide an increase and change in population between years 20 and 40, thus requiring participants to make first level estimates at these locations (\cref{fig:qi1-sketch} and \cref{fig:qi2-sketch}).
In order to understand the effect of the location along the domain and in turn the magnitude of the population being estimated, we extracted first level estimates for years 20 and 40 from participant calculations and scratch work.

In order to examine whether participants who used the provided resources for estimation differed in their numerical estimations from those who did not, we first compared population estimates from the explicitly asked year 10 location; these comparisons are provided in \protect\hyperlink{estimation-comparison}{Appendix 3b}.
About half of the participants fell into the category which provided scratch work and half did not.
It was determined there was no substantial difference or bias in estimates between the two groups, therefore, we proceeded to examine the estimated populations across scales from the first level estimates.

The true population from the underlying parameters in year 20 was 1483.01 with closest simulated point values of 1529.19 and 1288.9 for data sets one and two respectively; this location still results in a relatively low magnitude of population, but is closer to the crux of the exponential curve.
In year 40, the true population from the underlying parameters in year 40 is 15846.35 with closest simulated point values of 17046.94 and 24186.34 for data sets one and two respectively.
It is important to note that there is a difference in simulated point values in year 40 between the two data sets; as a result, the multiplicative error causes larger variance in simulated points for later years and larger population magnitudes.

\begin{figure}[tbp]

{\centering \includegraphics[width=1\linewidth,]{thesis_files/figure-latex/spaghetti-dataset1-1} 

}

\caption[Estimated population spaghetti plot: data set 1]{Visual evaluation of participants estimates of the population at years 10, 20, and 40 on data set 1. When work was shown, first level estimates were extracted from participant calculations and scratch pad notes for years 20 and 40. Spaghetti plots are displayed on the linear scale (top) and log scale (bottom) with both scale estimates shown on each - linear (blue), log (orange). The year was calculated from the underlying model equation based on the population estimate provided by the participant. Gray arrows indicate the true value and closest point value as demonstrated in \cref{fig:qe1-sketch}.}\label{fig:spaghetti-dataset1}
\end{figure}

\begin{figure}[tbp]

{\centering \includegraphics[width=1\linewidth,]{thesis_files/figure-latex/spaghetti-dataset2-1} 

}

\caption[Estimated population spaghetti plot: data set 2]{Visual evaluation of participants estimates of the population at years 10, 20, and 40 on data set 2. When work was shown, first level estimates were extracted from participant calculations and scratch pad notes for years 20 and 40. Spaghetti plots are displayed on the linear scale (top) and log scale (bottom) with both scale estimates shown on each - linear (blue), log (orange). The year was calculated from the underlying model equation based on the population estimate provided by the participant. Gray arrows indicate the true value and closest point value as demonstrated in \cref{fig:qe1-sketch}.}\label{fig:spaghetti-dataset2}
\end{figure}

Population estimates for year 10 from participants who used the scratchpad and first level estimates for years 20 and 40 are shown with spaghetti plots in \cref{fig:spaghetti-dataset1} and \cref{fig:spaghetti-dataset2} displayed on both the linear and log scale to aid in visual evaluation.
The scale in which the estimate was made is indicated blue for linear and orange for log with the segments mapped from the participant estimated population to the true year based on the underlying data equation.
Previously noted, the simulated point corresponding to year 40 in data set two has a large deviation from the true underlying data equation; \cref{fig:spaghetti-dataset2} highlights that some participants were reading the data points as opposed to first detecting the underlying trend and making estimates based on the identified trend.
This provides argument that estimates are highly subjective to the particular data set.
As the year increases, we observe an increased accuracy in estimates made on the linear scale while estimates made on the log scale suffer in accuracy due to strong anchoring to grid lines and the larger quantitative difference between grid lines as population magnitudes increase.
For instance, on the log scale, there was a tendency to overestimate the population for year 20 from data set one, underestimate the population for year 20 from data set two, and overestimate the population for year 40 from data set two.
Inaccurate first level estimations can lead to consequences in estimations which require participants to make comparisons between two points (e.g.~Intermediate Q1 and Q2).

In extracting participant first level estimates from their calculation and scratch work, we observed participants were resistant to estimating between grid lines and had a greater tendency to anchor their estimates to the grid line estimates on the log scale.
\cref{fig:common-population-estimates} illustrates the number of participants who provided that estimate on either the linear or log base two scale.
True values are based on the underlying model equation, closest simulated point values, and grid line breaks are indicated by the horizontal line types.
In particular, for year 40 in data set one, the closest point (17046.94) falls close to the log grid line (16384); participants greatly anchored to the grid line of 16384 with some participants adjusting to 16500 or 17000, anchoring again to a base ten value.
In a similar situation, for year 40 in data set two, the closest point (24186.34) falls close to the linear grid line (25000); more participants adjusted their estimates to 24500 or 24000 rather than anchoring to the grid line.
This suggests that participants were more likely to provide estimates which deviated from grid lines when making estimates on the linear scale, indicating they are more comfortable with interpreting values on a linear scale as opposed to the log scale.
When participants made estimates between grid lines on the log scale as indicated by their scratch work, they tended to estimate ``halfway'' between the two values indicated by the grid line breaks.
For example, 1536 was a common population estimate for year 20 in data set one because visually the location of estimation lands about halfway between grid lines 1024 and 2048 (\cref{fig:qi1-sketch} and \cref{fig:qi2-sketch}).
Another common halfway point on the log scale occurred at 24576 which visually lands between grid lines 16384 and 32768 for year 40 in data set two.
Participant calculations and scratch work provides support that participants equated these as halfway numerically as indicated by the selected work provided below:

\begin{align}
\textit{Sample work 1} \nonumber\\
2048-1024 &= 1024 \nonumber \\
1024/2 &= 512 \nonumber\\
512+1024 &= 1536 \nonumber\\
\nonumber\\
\textit{Sample work 2} \nonumber\\
2048 + 1024 & =3072 \nonumber\\
3072/2 & =1536 \nonumber\\
\nonumber\\
\textit{Sample work 3} \nonumber\\
32768-16384&=16384  \nonumber\\
32768-16384&=16384  \nonumber\\
16384*2&=32768  \nonumber\\
16384/2&=8192  \nonumber\\
8192+16384&=24576.  \nonumber
\end{align}
In particular, sample work 3 demonstrates the participant processing the log base two mapping as they repeatedly calculate the distance between two grid lines by subtraction and multiplication; they however then go on to estimate halfway between the two grid lines by equating spatial distance and quantitative difference.
This indicates a lack of understanding of log mapping where the spatial equivalence does not correspond to numeric equivalence; in other words, spatially halfway between two grid lines does not result in a numeric value halfway between the quantitative grid line labels.

\begin{figure}[tbp]

{\centering \includegraphics[width=1\linewidth,]{thesis_files/figure-latex/common-population-estimates-1} 

}

\caption[Estimated population: common responses for year 40]{Estimated populations in year 40 provided by more than three participants are shown in the dot chart. The $x$-axis indicates the number of participants who provided the estimate marked on the $y$-axis in assending numerical order. Colors are associated to scale - linear (blue) and log (orange) - and horizontal lines indicate the true value based on the underlying model equation (black solid), closest point value based on the simulated data set (black dashed), and grid lines shown on the graphs (blue/orange dotted). The two unique data sets are shown separately.}\label{fig:common-population-estimates}
\end{figure}

In conclusion, Elementary Q1 and the first level population estimates extracted from participant calculations and scratch work indicate that accuracy for low magnitudes are more accurate with lower variance in those estimates on the log scale than on the linear scale.
Accuracy of population estimates made on the linear scale improve as the magnitude of the population increases.
The results also provided support for the idea\}that participants have a strong tendency to anchor their estimates to both grid lines and a base ten framework with resistance to estimating between grid lines on the log scale in particular, leading to a sacrifice in accuracy for larger magnitudes.
Participant calculations and scratch work revealed a lack of understanding of logarithmic mapping due to considering spatial distance as indicative of numerical distance.

\hypertarget{elementary-q2-estimation-of-time}{%
\subsection{Elementary Q2: Estimation of time}\label{elementary-q2-estimation-of-time}}

In addition to estimating the population from a given year, participants were asked, \textit{"In what year does the population reach 4000?"} \pcref{fig:qe2-sketch}.
This required literal reading of the data by mapping a value given on the \(y\)-axis to its corresponding value on the \(x\)-axis.
The true estimated year based on the underlying equation in which the population reached exactly 4000 was 28.45.
Unlike the previous question, there was no exact simulated point that aligned with the quantity to be estimated; the closest points for data set one occured at years 24 (population 3774.9) and 30 (population 5174.12) and for data set two at years 27 (population 3859.22) and 28 (population 4099.69).
The median year estimated by participants for data set one was 24 on both scales with innerquartile ranges of 1 and 3 for the linear and log scale respectively; the median for data set two occurred at 27 for both data sets with innerquartile ranges of 2 and 1 for the linear and log scale respectively.

\begin{figure}[tbp]

{\centering \includegraphics[width=1\linewidth,]{images/03-estimation/qe2-sketch} 

}

\caption[Elementary Q2 sketch]{Sketch of the estimation procedure asked in Elementary Q2. Participants first locate 4000 along the $y$-axis and move to the right until they beleive they have found the correct location on the curve; then participants look down to the $x$-axis for their estimated year.}\label{fig:qe2-sketch}
\end{figure}

While a small portion of participants provided estimates of years 5, 10 and 15, the density plots in \cref{fig:qe2-density-plot} focus on reasonable participant estimates between years 20 and 35.
A population of 4000 occurs around a medium magnitude and is thus distinguishable on the linear scale, making the estimated location more visible.
Participants were consistently accurate across both the linear and log scales with a larger variance for data set one when estimates were made on the log scale.
One possible explanation for the difference in variation between data sets is that some participants were first visually fitting a trend on on the log scale (results in a visually linear trend) while some participants were basing their estimates off the closest point (year 24).
These competing strategies are clearly visible on the plot: some participants overestimated the closest point, while others made estimates more consistent with the true value based on the underlying (mean) equation.
On the log scale, participants were able to strongly anchor their estimates to the grid line break of 4096 and provide accurate year estimates by counting between grid lines on the \(x\)-axis with few participants making estimates between years (for example, 27.5).
However, participants still had a tendency to anchor to a base ten framework as indicated by an increase in the density of estimates occurring at year 30.

\begin{figure}[tbp]

{\centering \includegraphics[width=1\linewidth,]{thesis_files/figure-latex/qe2-density-plot-1} 

}

\caption[Elementary Q2 density]{Density of the participant estimates for the year in which the population reaches 4000. Colors are associated to scale - linear (blue) and log (orange) - and vertical lines indicate the true value based on the underlying model equation (black solid) and closest point value based on the simulated data set (black dashed). A jittered rug plot along the $x$-axis shows where participant estimates were made. The two unique data sets are shown separately. The plot shows anchoring occured to the closest point as shown by an increase in density around the dashed line. Density peaks occurred at whole values indicating rounding errors.}\label{fig:qe2-density-plot}
\end{figure}

Results from Elementary Q2 provide support that participants accurately estimated the year in which the population reaches 4000 on both scales.
The accuracy on the linear scale can be explained by the visibility of a medium magnitude along with participant ability to make accurate estimates between grid lines on a linear scale.
The population given aligned closely with grid line 4096 on the log scale, allowing participants to strongly anchor to the grid line for their estimation.
In particular, for data set one, participants were slightly more likely to base their estimates off the underlying trend line on the log scale than on the linear scale.
Estimated years were often provided in whole numbers and few participants showed an understanding that the population of interest could occur between years.

\hypertarget{intermediate-q1-additive-increase-in-population}{%
\subsection{Intermediate Q1: Additive increase in population}\label{intermediate-q1-additive-increase-in-population}}

Intermediate level questions required participants to read between the data and make comparisons between points.
Participants were asked, \textit{"From 20 to 40, the population increases by \_\_\_\_ [creatures]."} \pcref{fig:qi1-sketch}.
The questioning was selected carefully to prompt participants to make an additive comparison of populations between two years.
In order to make this comparison, participants must have first made an accurate first level estimate in both years and then subtract the two estimates.
Sample participant work below shows correct logic on both the linear and log scales:

\begin{align}
\textit{Sample work 4: correct logic (linear)} \nonumber\\
15000 - 2500 & = 12500\nonumber\\
\text{Scratchpad: } &\text{In 20 ABY the population of Ewoks was 2500,}\nonumber\\
                   &\text{in 40 ABY the population was 15 000,}\nonumber\\
                   &\text{i would make a substraction}\nonumber\\
\nonumber\\
\textit{Sample work 5: correct logic (log)} \nonumber\\
2048 - 1024  & = 1024 \nonumber\\
1024 - 512   & = 512 \nonumber\\
1024 + 512   & = 1536 \nonumber\\
16384 - 1536 & = 14848 \nonumber\\
\text{Scratchpad: }  & \text{20 aby 1536} \nonumber\\
             & \text{40 16384.} \nonumber
\end{align}
The true estimated increase in population from year 20 to 40 based on the underlying equation is 14363.34 (15846.35 - 1483.01) with increases based on the closest points of 15517.75 (17046.94 - 1529.18) and 22897.45 (24186.34 - 1288.91) for data sets one and two respectively.
The median estimated increase for data set one was 15000 (IQR = 3000) for the linear scale and 14784 (IQR = 2000) for the log scale while data set two resulted in larger estimates and variability with a median increase of 17500 (IQR = 10625) and 16500 (IQR = 8952) for the linear and log scale respectively.
The discrepancy in the summary between the two data sets provides further support that participants were inspecting the simulated data points in order to make their estimates.

\begin{figure}[tbp]

{\centering \includegraphics[width=1\linewidth,]{images/03-estimation/qi1-sketch} 

}

\caption[Intermediate Q1 sketch]{Sketch of the estimation procedure asked in Intermediate Q1. Participants make first level population estimates at years 20 and 40, then calculate the difference between the two values.}\label{fig:qi1-sketch}
\end{figure}

\begin{figure}[tbp]

{\centering \includegraphics[width=1\linewidth,]{thesis_files/figure-latex/qi1-density-1-1} 

}

\caption[Intermediate Q1 density (data set 1)]{Density of the participant estimates for the difference in population between years 20 and 40 for data set 1. Colors are associated to scale - linear (blue) and log (orange) - and vertical lines indicate the true difference based on the underlying model equation (black solid) and closest point difference based on the simulated data set (black dashed). A jittered rug plot along the $x$-axis shows where participant estimates were made. The plot shows an improvment in accuracy when estimates are made on the linear scale as opposed to the log scale as indicated by the linear peak at the closest point.}\label{fig:qi1-density-1}
\end{figure}

\begin{figure}[tbp]

{\centering \includegraphics[width=1\linewidth,]{thesis_files/figure-latex/qi1-density-2-1} 

}

\caption[Intermediate Q1 density (data set 2)]{Density of the participant estimates for the difference in population between years 20 and 40 for data set 2. Colors are associated to scale - linear (blue) and log (orange) - and vertical lines indicate the true difference based on the underlying model equation (black solid) and closest point difference based on the simulated data set (black dashed). A jittered rug plot along the $x$-axis shows where participant estimates were made. The plot shows an improvment in accuracy when estimates are made on the linear scale as opposed to the log scale as indicated by the linear peaks at the true difference and closest point. The two peaks illustrate how participants were reading reading the data points and not only the underlying trend.}\label{fig:qi1-density-2}
\end{figure}

\cref{fig:qi1-density-1} and \cref{fig:qi1-density-2} display the density for estimated increases in population as made by participants for data set one and two respectively.
There were a considerable amount of estimated increases near zero indicating that some participants were misinterpreting the value they were asked to estimate.
Sample participant work below shows common incorrect logic on both the linear and log scales:

\begin{align}
\textit{Sample work 6: incorrect logic (linear)} \nonumber\\
24000/2000&=12 \nonumber\\
\nonumber \\ 
\textit{Sample work 7: incorrect logic (log)} \nonumber\\
16380/1026&=15.96\nonumber\\
\nonumber \\ 
\textit{Sample work 8: changed logic (log)} \nonumber\\
2048-1024&=1024\nonumber\\
1024+512&=1536\nonumber\\
16384-1536&=14848\nonumber\\
14848/1536&=9.67.\nonumber
\end{align}

In particular, sample work 8 shows how the participant first estimated halfway between the log grid lines and correctly subtracted the populations for the two given years before incorrectly changing their logic to divide the two populations.
One potential source of misinterpretation of this questions might be the particular order in which participants were asked the questions.
For example, if participants were asked to provide an estimated increase in population after having been asked Intermediate Q2 which prompts participants to provide a multiplicative change in population, they may be more likely to misinterpret Intermediate Q1.
However, participants answering questions on the second scenario would have seen both questioning frameworks in the previous scenario context.

Estimates for the increase in population between year 20 and year 40 was distinctly more accurate for estimates made on the linear scale as indicated by the peak density occurring near the closest point and true value vertical lines.
The slight shifts in the density on the log scale suggest participants are making inaccurate first level estimates.
One explanation might be that participants were anchoring to the grid lines much stronger on the log scale as opposed to being more likely to adjust their estimates between grid lines on the linear scale.
Common responses \pcref{fig:qi1-common-responses} on the log scale come from anchoring to grid lines (16384 - 1024 = 15360), halfway numerically between grid lines (16384 - 1536 = 14848; 24576 - 1536 = 2340), and base ten (16384 - 2000 = 14784) while participants on the linear scale anchored to multiples of 500 and 1000.
This was dependent on the location of simulated points in relation to the grid lines and lead to an underestimation in difference for data set one and an overestimation in difference for data set two.
Variance in estimates appeared to be consistent across both scales for data set two with a smaller variance on the log scale for data set one.

\begin{figure}[tbp]

{\centering \includegraphics[width=1\linewidth,]{thesis_files/figure-latex/qi1-common-responses-1} 

}

\caption[Intermediate Q1 common responses]{The dot charts show estimates for the difference in population between years 20 and 40 provided by more than three participants. The $x$-axis indicates the number of participants who provided the estimate marked on the $y$-axis in assending numerical order. Colors are associated to scale - linear (blue) and log (orange) - and horizontal lines indicate the true difference based on the underlying model equation (black solid) and closest point difference based on the simulated data set (black dashed). The two unique data sets are shown separately.}\label{fig:qi1-common-responses}
\end{figure}

Responses from Intermediate Q1 required participants to use their first level estimates in order to make an additive comparison of populations between two years.
Some participants misinterpreted the question, making a multiplicative comparison, thus providing estimates closer to zero.
This was supported by examining select participant calculation and scratchpad work.
The estimated increase in population was more accurate on the linear scale with the lack of accuracy on the log scale affected by participant resistance to and misunderstanding of making estimates between log grid lines.

\hypertarget{intermediate-q2-multiplicative-change-in-population}{%
\subsection{Intermediate Q2: Multiplicative change in population}\label{intermediate-q2-multiplicative-change-in-population}}

Previously, we explored how participants made an additive comparison of populations between two years.
In addition, participants were asked, \textit{"How many times more [creatures] are there in 40 than in 20?"} \pcref{fig:qi2-sketch}.
The questioning was selected carefully to prompt participants to make a multiplicative comparison between two years.
Similar to Intermediate Q1, in order to make this comparison, participants must have made accurate first level estimates in both years and then divide the two estimates.
Participants may also have made this comparison on the log scale by understanding the multiplicative nature of the grid lines. Sample participant work below shows correct logic on both the linear and log scales:

\begin{align}
\textit{Sample work 9: correct logic (linear)} \nonumber\\
17500/1400&=12.5 \nonumber\\    
\text{Scratchpad: } & \text{same as before, but a division} \nonumber\\
\nonumber\\
\textit{Sample work 10: correct logic (linear)} \nonumber\\
17000-1000&=16000 \nonumber\\
17000/1000&=17  \nonumber\\
\nonumber\\
\textit{Sample work 11: correct logic (log)} \nonumber\\
24/1.4&=17.14    \nonumber\\ 
\text{Scratchpad: } & \text{around 24k tribbles were at 4540, and} \nonumber\\
                   & \text{1.4k at 4520, make a division and thats}\nonumber\\
                   & \text{how many times (without the k)}\nonumber\\
\nonumber \\
\textit{Sample work 12: correct logic (log)} \nonumber\\                   
2048*5&=10240 \nonumber\\   
2048*6&=12288 \nonumber\\   
2048*7&=14336 \nonumber\\   
2048*8&=16384  \nonumber\\   
2048*8&=16384. \nonumber
\end{align}

\begin{figure}[tbp]

{\centering \includegraphics[width=1\linewidth,]{images/03-estimation/qi2-sketch} 

}

\caption[Intermediate Q2 sketch]{Sketch of the estimation procedure asked in Intermediate Q2. Participants make first level population estimates at years 20 and 40, then calculate the ratio between the two values.}\label{fig:qi2-sketch}
\end{figure}

The scratch work from participants gave insight about the estimation strategy participants followed when determining the estimated change in population.
For instance, sample work 10 shows the participant first incorrectly calculated the additive increase in population before correcting their calculation through division while sample work 12 shows how the participant used a trial and error method.
The true change in population based on the underlying equation was 10.69 times as many (15846.35/1483.01) with changes based on the closest points of 11.1 (17046.94/1529.18) and 18.8 (24186.34/1288.91) for data sets one and two respectively.
The median estimated change for data set one was 11.7 (IQR = 8.5) for the linear scale and 10.7 (IQR = 6) for the log scale while data set two resulted in larger estimates and variability with a median change of 15.3 (IQR = 14) and 16 (IQR = 8.5) for the linear and log scale respectively.
The inconsistency between the two data sets aligns with previous evidence that participants were making estimates by reading the simulated data rather than based on the underlying trend.

\begin{figure}[tbp]

{\centering \includegraphics[width=1\linewidth,]{thesis_files/figure-latex/qi2-plots-1} 

}

\caption[Intermediate Q2 observed plot]{Displays the observed estimated change in population for Intermediate Q2. The colors indicate scale - linear (blue) and log (orange) with participants dodged. The plot shows a substantial number of participants provided estimates that more closely reflected that of the additive increase in population rather than the multiplicative change.}\label{fig:qi2-plots}
\end{figure}

As seen in the results for Intermediate Q1, some participants struggled to understand the value they were being asked to estimate.
Similarly, \cref{fig:qi2-plots} illustrates a substantial number of participants provided estimates that more closely reflected that of the additive increase in population rather than the multiplicative change.
\cref{fig:qi2-common-responses} highlights that 15000 was still a common participant response.
Sample work below demonstrate common incorrect logic and calculations conducted by participants:

\begin{align}
\textit{Sample work 13: incorrect logic (linear)} \nonumber\\
23800-1100&=22700 \nonumber\\
\nonumber\\
\textit{Sample work 14: incorrect logic (log)} \nonumber\\
16384-1536&=14848. \nonumber
\end{align}

Evaluating reasonable participant responses for the change in population between 0 times as many and 35 times as many, \cref{fig:qi2-density} indicates participants tended to be make more accurate and less variable estimates on the log scale than on the linear scale.
\cref{fig:qi2-plots} shows common responses provided by participants.

\begin{figure}[tbp]

{\centering \includegraphics[width=1\linewidth,]{thesis_files/figure-latex/qi2-density-1} 

}

\caption[Intermediate Q2 density]{Density of the participant estimates for the multiplicative change in population between years 20 and 40. Colors are associated to scale - linear (blue) and log (orange) - and vertical lines indicate the true change based on the underlying model equation (black solid) and closest point change based on the simulated data set (black dashed). A jittered rug plot along the $x$-axis shows where participant estimates were made. Data sets are plotted separately. The density plots show participants tended to be make more accurate and less variable estimates on the log scale than on the linear scale.}\label{fig:qi2-density}
\end{figure}

\begin{figure}[tbp]

{\centering \includegraphics[width=1\linewidth,]{thesis_files/figure-latex/qi2-common-responses-1} 

}

\caption[Intermediate Q2 common responses]{The dot charts show estimates for the multiplicative change in population between years 20 and 40 provided by more than three participants. The $x$-axis indicates the number of participants who provided the estimate marked on the $y$-axis in assending numerical order. Colors are associated to scale - linear (blue) and log (orange) - and horizontal lines indicate the true change based on the underlying model equation (black solid) and closest point change based on the simulated data set (black dashed). The two unique data sets are shown separately. We see that 15000 is still a common response, demonstrating a common misunderstanding of the value being asked.}\label{fig:qi2-common-responses}
\end{figure}

Overall, responses for Intermediate Q2 provided further support that participants tended to misinterpret the quantity they were being asked to estimate.
The density plots of responses suggest that the log scale has a slight advantage over the linear scale for estimating the multiplicative change in population.
While anchoring to grid lines and base ten values for first level estimates still occurred, many participants further anchored their responses to whole values.

\hypertarget{intermediate-q3-time-until-population-doubles}{%
\subsection{Intermediate Q3: Time until population doubles}\label{intermediate-q3-time-until-population-doubles}}

An alternative multiplicative comparison between two points was to determine the amount of time it took for a value to double.
Participants were asked, \emph{``How long does it take for the population in year 10 to double?''} \pcref{fig:qi3-sketch}.
In order to accurately evaluate this comparison, participants must have made a first level estimate for the population in year 10, asked in Elementary Q1.
Participants then needed to double their first level estimate in order to extract the year in which this value occurred and finally subtract year 10.
Alternatively, on the log scale, participants could have made this comparison without actually extracting the numeric values and instead relied on their spatial distance equating one increase in grid lines to a double in population.
This estimation strategy would have required keen understanding of the log base two scale.
Making a judgement based on participant calculations and scratch work, most participants selected the former approach when they estimated the number of years it took for the population to double.
One participant stated, ``4510 has 512 Tribble Population and to double it it needs to have 1,024 so it would take approximately 5 years. In 4515 they would have double the population.'' while another participant indicated ``10Aby near to 460.8 so double is 921.6; aprox. 5 years.''

\begin{figure}[tbp]

{\centering \includegraphics[width=1\linewidth,]{images/03-estimation/qi3-sketch} 

}

\caption[Intermediate Q3 sketch]{Sketch of the estimation procedure asked in Intermediate Q3. Participants make a first level population estimate at year 10, then double this population and estimate the year in which that occurs. Lastly, participants subtracted year 10 to determine how long it took for the population to double. Another estimation strategy could involve participants spatially judging the double of population on the log base 2 scale.}\label{fig:qi3-sketch}
\end{figure}

Based on the true underlying equation, the population in year 10 (481.62) doubled to 963.24 in year 16.25, thus it took 6.25 years for the population to double.
Closest simulated points resulted in the population doubling in 4 years (445 x 2 = 890.97 in 14) for data set one and 6 years (466.90 x 2 = 933.79 in 16) for data set two.
The median participant response for data set one was 5 (IQR = 5) and 5 (IQR = 2) for the linear and log scale respectively with a median for data set two of 8 (IQR = 6) and 6 (IQR = 2) for the linear and log scale respectively.
\cref{fig:qi3-density} illustrates the estimated number of years until the population in year 10 doubled.
While there appears to be similar accuracy across both scales, the variance in estimates was considerably smaller for the log scale.
The large variance in the linear scale may be explained by the location of reference year 10 which resulted in a population of low magnitude and is visually difficult to estimate.
As indicated by peaks in density, there was strong anchoring which occurs at multiples of five.

\begin{figure}[tbp]

{\centering \includegraphics[width=1\linewidth,]{thesis_files/figure-latex/qi3-density-1} 

}

\caption[Intermediate Q3 density]{Density of the participant estimates for how long it took for the population in year 10 to double. Colors are associated to scale - linear (blue) and log (orange) - and vertical lines indicate the true number of years based on the underlying model equation (black solid) and closest point number of years based on the simulated data set (black dashed). A jittered rug plot along the $x$-axis shows where participant estimates were made. The two unique data sets are shown separately. The plot shows larger variability for estimates made on the linear scale.}\label{fig:qi3-density}
\end{figure}

In summary, as indicated by participant scratch work, they tended to make a first level estimates for the reference year 10 rather than visually judging the distance between grid lines on the log scale.
The estimated number of years until the population doubled from a reference year of 10 resulted in lower variability on the log scale as opposed to a larger variability on the linear scale.
One explanation might be the low magnitude of population in year 10 and further exploration would be needed to justify for other reference years.
Common responses strongly suggest participants were anchoring to multiples of 5 years.

\hypertarget{discussion-and-conclusion-2}{%
\section{Discussion and Conclusion}\label{discussion-and-conclusion-2}}

This study was intended to help inform and aid in understanding the cognitive implications of displaying exponentially increasing data on a log scale.
We evaluated graph comprehension as it relates to two contextual scenarios by asking participants a series of six questions (one open ended, two elementary level, and three intermediate) which required them to quantitatively transform information in the chart.
Results provided an understanding of the advantages and disadvantages of the log scale.

In general, results suggest that understanding log logic is difficult as indicated by the misunderstanding in Intermediate Q1 and Q2.
This is also supported in following participants estimation strategies for making first level estimates in Intermediate Q3 rather than relying on their spatial awareness between grid lines on the log scale.
The accuracy of estimates greatly depends on the location of the value being estimated in relation to the magnitude.
For example, accuracy and variability of population estimates made on the linear scale improved as the year of interest increased and thus the magnitude of population increased, making the point more visible.
Alternatively, there was a slight sacrifice in the accuracy of population estimates on the log scale as the year of interest increased.
This was due to participant resistance to estimate between grid lines on the log scale and inaccurate representation of equating spatial distance to quantitative difference.
As the magnitude of population increases, there was a more noticeable effect of the resistance and lack of understanding.
Inaccurate first level estimations can lead to consequences in estimations which require participants
to make comparisons between two points.
Density plots showed an advantage of the linear scale when estimating an additive increase in population and a slight advantage of the log scale when estimating a multiplicative change in population.

It was also found that estimates were subjective to the simulated data set as shown by the discrepancy between data set one and data set two.
This implies a large portion of participants were reading the actual simulated data points as opposed to basing estimates on the underlying visual trend of the data.
Common responses revealed participants bias to anchoring their estimates to the grid lines, particularly on the log scale, as well as to base ten values.
The strong tendency to anchor to grid lines on the log scale resulted in a sacrifice in accuracy as quantitative differences between grid lines increased.
This also implies that accuracy strongly depended on the location of the simulated point in relation to the grid lines.

Understanding graph comprehension is a complex process and requires long-term interaction with the chart and information being presented.
With a fixed number of estimates and assessment of participant scratch work, we outlined a variety of situations in which displaying exponentially increasing data on the log scale resulted in both advantages and disadvantages, thus providing a better understanding of the cognitive implications of the use of log scales.

\hypertarget{conclusion}{%
\chapter{Conclusion}\label{conclusion}}

This research evaluated the use of log scales to display exponentially increasing data from three different angles and levels of complexity: perception, prediction, and estimation.
Each study provided us insight into the advantages and disadvantages of displaying exponentially increasing data on a log scale and in what context each choice of scale might be more appropriate.

The first study laid the foundation for future studies by testing participants ability to perceptually distinguish between different rates of exponential growth on a linear and log scale.
This study utilized statistical lineups and did not require participants to understand exponential growth, identify log scales, or have any mathematical training; instead, any findings rely on participants ability to perceive differences in curvature and slope.
An analysis on the accuracy of participants identification of the target plot provides us with insight to the perceptual implications of the choice of scale.
Results from the lineup analysis indicate that the perceptual difference results from the contextual appearance of the trend.
The choice of scale changes this contextual appearance leading to slight perceptual advantages for both scales depending on the curvatures of the trend lines being compared.
In general, when there were large curvature differences, the choice of scale had no impact and perceptual differences were easily identified on both scales.
However, when minor differences in curvature occurred, there was a perceptual advantage for the scale which resulted in identified the trend which contextually appeared as a curve from the trends that appeared as a line.
This revealed an advantage for the log scale when differences were subtle with an exception of the the linear scale leading to a perceptual advantage for identifying a trend with more curvature from one which appears to be more linear - contextually appears opposite on the log scale to identify a trend that contextually appears linear from ones that appear curved.

`You Draw It' interactive graphics were adapted from the New York Times and used to test the ability to make forecast predictions for exponentially increasing trends on both scales by drawing a visually fit line with a computer mouse.

The second study required interpretative processes to extend the pattern recognition and construct meaning.
This study was supported by a sub-study which validated `You Draw It' as a tool and method for testing statistical graphics and introduced an appropriate statistical analysis method using generalized additive mixed models for comparing visually fitted trend lines to statistical regression results.
This new method was then used to test participants ability to make forecast predictions for exponentially increasing trends on both scales.
The results from the analysis showed a clear underestimation of forecasting trends with high exponential growth rates when participants were asked to make predictions on the linear scale.
Improvement in forecasts were made when participants were asked to make predictions on the log scale as well as when participants were provided visual aids of points along the trend line.
These improvements can be explained by the change in contextual appearance of the data between the two scales; participants visually extended a linear trend on the log scale and an exponential trend on the linear scale.
The graphical tasks from the first two studies were conducted independent of scenarios or contextual applications of log scales; instead, they focused how our visual system perceives and identifies patterns in exponential growth.
This did not require participants to understand or read log scales.

In order to understand the cognitive implications of displaying exponentially increasing data on a log scale, the third study evaluated graph comprehension as it relates to the contextual scenario of the data shown.
This required participants to integrate information from the scenario, chart title, axes, and graphical forms.
In the study, participants were asked to quantitatively transform a graph of exponentially increasing data and extend their estimates to compare two points.
We evaluated graph comprehension as it relates to two contextual scenarios by asking participants a series of questions based on the elementary (literally reading the data) and intermediate (reading between the data) level questions.
The results of this study inform our understanding of the cognitive implications of displaying exponentially increasing data on a log scale.

Overall, our results suggested that log logic is difficult and that we often misinterpret and miscalculate multiplicative reasoning.
However, further investigation is necessary to determine if the misunderstanding occurs due to the ambiguity of language or scale.
By collecting information from the calculation and scratchpad inputs, we were able to better understand the strategies used in visual estimation.
Participants often made first level estimates when comparing two points rather than relying on their spatial awareness between grid lines on the log scale.
They were resistant to make estimates between grid lines (anchored to grid lines) and tended to inaccurately equate spatial distance to numerical difference on the log scale, sacrificing estimation accuracy for cognitive efficiency; this resistance and misinterpretation leads to greater consequences for larger magnitudes when distances between grid lines is large.
Further testing is required to evaluate this impact on log scales of different bases, such as base ten where this spatial distance has a much larger effect than on base two.
Extension studies could also provide information about the impact including minor grid lines has on anchoring; for example, ten visually unequal spaced minor grid lines (equal numerical difference) could be added to a log base ten scale to potentially aid in participant estimation.
Overall, estimation accuracy for small magnitudes was improved by the use of the log scale, but sacrifices in accuracy on the log scale became apparent as magnitudes increased leading to advantages on the linear scale.

The studies conducted in this research relied on graphical tasks of varying complexity in order to help us understand the perceptual and cognitive advantages and disadvantages of displaying exponentially increasing data on the log scale.
Our results showed there are perceptual advantages of the use of log scales due to the change in contextual appearance; however, our understanding of log logic is flawed when translating the information into context.
We recommend consideration of both user needs and graph specific tasks when presenting data on the log scale; caution should be taken when interpretation of large magnitudes is required, but advantages may appear when it is necessary to visually identify and interpret small magnitudes on the chart.
In addition to this research, further investigation is necessary to expand the use of log scales to data which does not follow an exponential trend, but where log scales are otherwise appropriate - such as polynomial curves spanning multiple orders of magnitude.
It is also necessary to evaluate the implications of transforming or displaying the \(x\)-axis on a log scale, as this work focused on the transformation of the \(y\)-axis.
Follow-up studies for this research could provide further insight into the strategies of estimation by providing users the ability to interact with the graph with visual aids such as the arrows shown in sketches from \protect\hyperlink{estimation}{Chapter 4} and how we make decisions based on data displayed on a log scale in order to evaluate the effect of scale on risk adversity.
This research stands as a model for conducting an extensive series of graphical tasks on the same type of data and plot in order to gain a comprehensive understanding of both the perceptual and cognitive implications of the design choices.

\appendix

\hypertarget{youdrawit-with-shiny}{%
\chapter{You Draw It Setup with Shiny}\label{youdrawit-with-shiny}}

Interactive plots for the you draw it study were created using the \texttt{r2d3} package and integrating D3 source code with an R shiny application.
I conducted all data simulation and processing in R and outputted two data sets - point data and line data - containing (x, y) coordinates corresponding to either a simulated point or fitted value predicted by a statistical model respectively.
Then, the r2d3 package converted the data sets in R to JSON to be interpreted by the D3.js code.
I define functions in D3.js to draw the initial plot and set up drawable points for the user drawn line.
Drag events in D3.js were utilized to react to observe and react to user input.
Shiny Messages were used to communicate the user interaction between the D3 code and the R environment.
The plot was then rendered and updated on user interaction into the R shiny application with the RenderD3 and d3Output functions.
Parameters for aesthetic design choices were defined in a list of options and r2d3 passes these to the D3.js code.
For instance, I specified the buffer space allowed for the \(x\) and \(y\) axes to avoid users anchoring their lines to the axes limits. For D3.js source code, visit GitHub \href{https://github.com/srvanderplas/Perception-of-Log-Scales/blob/master/you-draw-it-development/you-draw-it-test-app/main.js}{here}.
\cref{fig:r2d3-shiny-flowchart} provides a visual aid of the process of creating the you draw it experimental study in \protect\hyperlink{youdrawit}{Chapter 3}.

\begin{figure}[tbp]

{\centering \includegraphics[width=1\linewidth,]{images/02-you-draw-it/r2d3+shiny} 

}

\caption{Interactive plot development}\label{fig:r2d3-shiny-flowchart}
\end{figure}

\hypertarget{exponential-prediction-plots}{%
\chapter{Exponential Prediction Interactive Plots}\label{exponential-prediction-plots}}

The figures below illustrate the 8 interactive plots used to test exponential prediction.
Two data sets were simulated with low and high exponential growth rates and shown four times each by truncating the points shown at both 50\% and 75\% of the domain as well as on both the log and linear scales following a 2 x 2 x 2 factorial treatment design.

\begin{figure}[tbp]

{\centering \includegraphics[width=0.65\linewidth,]{images/02-you-draw-it/low-10-linear} 

}

\caption[Exponential prediction plot (low; 50\%; linear)]{Exponential Prediction: low growth rate, points truncated at 50\%, linear scale}\label{fig:low-10-linear}
\end{figure}

\begin{figure}[tbp]

{\centering \includegraphics[width=0.65\linewidth,]{images/02-you-draw-it/low-10-log} 

}

\caption[Exponential prediction plot (low; 50\%; log)]{Exponential Prediction: low growth rate, points truncated at 50\%, log scale}\label{fig:low-10-log}
\end{figure}

\begin{figure}[tbp]

{\centering \includegraphics[width=0.65\linewidth,]{images/02-you-draw-it/low-15-linear} 

}

\caption[Exponential prediction plot (low; 75\%; linear)]{Exponential Prediction: low growth rate, points truncated at 75\%, linear scale}\label{fig:low-15-linear}
\end{figure}

\begin{figure}[tbp]

{\centering \includegraphics[width=0.65\linewidth,]{images/02-you-draw-it/low-15-log} 

}

\caption[Exponential prediction plot (low; 75\%; log)]{Exponential Prediction: low growth rate, points truncated at 75\%, log scale}\label{fig:low-15-log}
\end{figure}

\begin{figure}[tbp]

{\centering \includegraphics[width=0.65\linewidth,]{images/02-you-draw-it/high-10-linear} 

}

\caption[Exponential prediction plot (high; 50\%; linear)]{Exponential Prediction: high growth rate, points truncated at 50\%, linear scale}\label{fig:high-10-linear}
\end{figure}

\begin{figure}[tbp]

{\centering \includegraphics[width=0.65\linewidth,]{images/02-you-draw-it/high-10-log} 

}

\caption[Exponential prediction plot (high; 50\%; log)]{Exponential Prediction: high growth rate, points truncated at 50\%, log scale}\label{fig:high-10-log}
\end{figure}

\begin{figure}[tbp]

{\centering \includegraphics[width=0.65\linewidth,]{images/02-you-draw-it/high-15-linear} 

}

\caption[Exponential prediction plot (high; 75\%; linear)]{Exponential Prediction: high growth rate, points truncated at 75\%, linear scale}\label{fig:high-15-linear}
\end{figure}

\begin{figure}[tbp]

{\centering \includegraphics[width=0.65\linewidth,]{images/02-you-draw-it/high-15-log} 

}

\caption[Exponential prediction plot (high; 75\%; log)]{Exponential Prediction: high growth rate, points truncated at 75\%, log scale}\label{fig:high-15-log}
\end{figure}

\hypertarget{estimation-comparison}{%
\chapter{Scratchwork participant comparison}\label{estimation-comparison}}

In order to examine whether participants who used the provided resources for estimation differed in their numerical estimations from those who did not, we first compared population estimates from the explicitly asked year 10 location.
About half of the participants fell into the category which provided scratch work and half did not \pcref{tab:showed-work-comparison-table}.
The was determined there was no substantial difference or bias in estimates between the two groups, therefore, we proceeded to examine the estimated populations across scales from the first level estimates \pcref{fig:showed-work-comparison}.
See \protect\hyperlink{eq1-results}{4.4.2} for follow-up comparisons.

\begin{table}

\caption{\label{tab:showed-work-comparison-table}Estimation showed work summary}
\centering
\begin{tabular}[t]{lllr}
\toprule
Data set & Scale & Showed work & N\\
\midrule
dataset1 & linear & no & 73\\
dataset1 & linear & yes & 72\\
dataset1 & log2 & no & 79\\
dataset1 & log2 & yes & 78\\
dataset2 & linear & no & 79\\
\addlinespace
dataset2 & linear & yes & 78\\
dataset2 & log2 & no & 73\\
dataset2 & log2 & yes & 72\\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[tbp]

{\centering \includegraphics[width=1\linewidth,]{thesis_files/figure-latex/showed-work-comparison-1} 

}

\caption{Estimation showed work density plot comparison}\label{fig:showed-work-comparison}
\end{figure}

\backmatter

\hypertarget{references}{%
\chapter*{References}\label{references}}
\addcontentsline{toc}{chapter}{References}

\noindent

\setlength{\parindent}{-0.20in}
\setlength{\leftskip}{0.20in}
\setlength{\parskip}{8pt}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-aisch_cox_quealy_2015}{}}%
Aisch, G., Cox, A., \& Quealy, K. (2015). You draw it: How family income predicts children's college chances. \emph{The New York Times}. Retrieved from \url{https://www.nytimes.com/interactive/2015/05/28/upshot/you-draw-it-how-family-income-affects-childrens-college-chances.html}

\leavevmode\vadjust pre{\hypertarget{ref-amer2005bias}{}}%
Amer, T. (2005). Bias due to visual illusion in the graphical presentation of accounting information. \emph{Journal of Information Systems}, \emph{19}(1), 1--18.

\leavevmode\vadjust pre{\hypertarget{ref-anderson_design_1974}{}}%
Anderson, V., \& McLean, R. (1974). \emph{Design of experiments: A realistic approach}. New York: M. Dekker.

\leavevmode\vadjust pre{\hypertarget{ref-andrews_2022}{}}%
Andrews, R. (2022, January). Standards for graphic presentation. \emph{Info We Trust}. Retrieved from \url{https://infowetrust.com/project/1915-standards}

\leavevmode\vadjust pre{\hypertarget{ref-NYTrememberinglives}{}}%
Barry, D., Buchanan, L., Cargill, C., Daniel, A., Delaquérière, A., Gamio, L., \ldots{} al., et. (2020, May). Remembering the 100,000 lives lost to coronavirus in america. Retrieved from \url{https://www.nytimes.com/interactive/2020/05/24/us/us-coronavirus-deaths-100000.html}

\leavevmode\vadjust pre{\hypertarget{ref-lme4}{}}%
Bates, D., Mächler, M., Bolker, B., \& Walker, S. (2015). Fitting {Linear} {Mixed}-{Effects} {Models} {Using} lme4. \emph{Journal of Statistical Software}, \emph{67}, 1--48. http://doi.org/\href{https://doi.org/10.18637/jss.v067.i01}{10.18637/jss.v067.i01}

\leavevmode\vadjust pre{\hypertarget{ref-baumer2021texts}{}}%
Baumer, B., Kaplan, D., \& Horton, N. (2021). \emph{Texts in statistical science: Modern data science with r}. Chapman; Hall/CRC.

\leavevmode\vadjust pre{\hypertarget{ref-bavel_using_2020}{}}%
Bavel, J., Baicker, K., Boggio, P., Capraro, V., Cichocka, A., Cikara, M., \ldots{} others. (2020). Using social and behavioural science to support COVID-19 pandemic response. \emph{Nature Human Behaviour}, \emph{4}(5), 460--471.

\leavevmode\vadjust pre{\hypertarget{ref-becker2019trackr}{}}%
Becker, G., Moore, S., \& Lawrence, M. (2019). Trackr: A framework for enhancing discoverability and reproducibility of data visualizations and other artifacts in r. \emph{Journal of Computational and Graphical Statistics}, \emph{28}(3), 644--658.

\leavevmode\vadjust pre{\hypertarget{ref-beeby1973well}{}}%
Beeby, A., \& Taylor, H. (1973). How well can we use graphs. \emph{The Communicator of Scientific and Technical Information}, \emph{17}, 7--11.

\leavevmode\vadjust pre{\hypertarget{ref-reddit_CrappyDesign}{}}%
Benjamin-Cat. (2018, November 13). European pet dog, cat, and bird ownership per capita with average age of leaving parental home (of human beings) {[}Reddit \{Post\}{]}. Retrieved May 19, 2022, from \href{https://www.reddit.com/r/europe/comments/9wmu3x/european_pet_dog_cat_and_bird_ownership_per/}{www.reddit.com/r/europe/comments/9wmu3x/european\_pet\_dog\_cat\_and\_bird\_ownership\_per/}

\leavevmode\vadjust pre{\hypertarget{ref-best_perception_2007}{}}%
Best, L., Smith, L., \& Stubbs, D. (2007). Perception of linear and nonlinear trends: Using slope and curvature information to make trend discriminations. \emph{Perceptual and Motor Skills}, \emph{104}(3), 707--721.

\leavevmode\vadjust pre{\hypertarget{ref-ryanabest_2021}{}}%
Best, R., \& Boice, J. (2021, June). Build an NBA contender with our roster-shuffling machine. Retrieved from \url{https://projects.fivethirtyeight.com/nba-trades-2021/}

\leavevmode\vadjust pre{\hypertarget{ref-broersma1985graphical}{}}%
Broersma, H., \& Molenaar, I. (1985). Graphical perception of distributional aspects of data. \emph{Computational Statistics Quarterly}, \emph{2}(1), 53--72.

\leavevmode\vadjust pre{\hypertarget{ref-buchanan_park_pearce_2017}{}}%
Buchanan, L., Park, H., \& Pearce, A. (2017). You draw it: What got better or worse during obama's presidency. \emph{The New York Times}. Retrieved from \url{https://www.nytimes.com/interactive/2017/01/15/us/politics/you-draw-obama-legacy.html}

\leavevmode\vadjust pre{\hypertarget{ref-buja_statistical_2009}{}}%
Buja, A., Cook, D., Hofmann, H., Lawrence, M., Lee, E.-K., Swayne, D., \& Wickham, H. (2009a). Statistical inference for exploratory data analysis and model diagnostics. \emph{Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences}, \emph{367}(1906), 4361--4383.

\leavevmode\vadjust pre{\hypertarget{ref-nullabor}{}}%
Buja, A., Cook, D., Hofmann, H., Lawrence, M., Lee, E.-K., Swayne, D., \& Wickham, H. (2009b). Statistical inference for exploratory data analysis and model diagnostics. \emph{Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences}, \emph{367}(1906), 4361--4383.

\leavevmode\vadjust pre{\hypertarget{ref-burnmurdoch_2020}{}}%
Burn-Murdoch, J., Nevitt, C., Tilford, C., Rininsland, A., Kao, J., Elliott, O., \ldots{} Stabe, M. (2020). Coronavirus tracked: Has the epidemic peaked near you? \emph{Coronavirus chart: see how your country compares}. Financial Times. Retrieved from \url{https://ig.ft.com/coronavirus-chart/?areas=eur}

\leavevmode\vadjust pre{\hypertarget{ref-reddit_dataisugly}{}}%
BusyAd6668. (2022, March 17). {ARGH}?! {[}Reddit \{Post\}{]}. Retrieved May 19, 2022, from \href{https://www.reddit.com/r/dataisugly/comments/tgn4vq/argh/}{www.reddit.com/r/dataisugly/comments/tgn4vq/argh/}

\leavevmode\vadjust pre{\hypertarget{ref-carpenter1998model}{}}%
Carpenter, P., \& Shah, P. (1998). A model of the perceptual and conceptual processes in graph comprehension. \emph{Journal of Experimental Psychology: Applied}, \emph{4}(2), 75.

\leavevmode\vadjust pre{\hypertarget{ref-ciccione2021can}{}}%
Ciccione, L., \& Dehaene, S. (2021). Can humans perform mental regression on a graph? Accuracy and bias in the perception of scatterplots. \emph{Cognitive Psychology}, \emph{128}, 101406.

\leavevmode\vadjust pre{\hypertarget{ref-cleveland1988shape}{}}%
Cleveland, W., McGill, M., \& McGill, R. (1988). The shape parameter of a two-variable graph. \emph{Journal of the American Statistical Association}, \emph{83}(402), 289--300.

\leavevmode\vadjust pre{\hypertarget{ref-cleveland_graphical_1984}{}}%
Cleveland, W., \& McGill, R. (1984). Graphical perception: Theory, experimentation, and application to the development of graphical methods. \emph{Journal of the American Statistical Association}, \emph{79}(387), 531--554.

\leavevmode\vadjust pre{\hypertarget{ref-cleveland_graphical_1985}{}}%
Cleveland, W., \& McGill, R. (1985). Graphical perception and graphical methods for analyzing scientific data. \emph{Science}, \emph{229}(4716), 828--833.

\leavevmode\vadjust pre{\hypertarget{ref-cleveland1987graphical}{}}%
Cleveland, W., \& McGill, R. (1987). Graphical perception: The visual decoding of quantitative information on graphical displays of data. \emph{Journal of the Royal Statistical Society: Series A (General)}, \emph{150}(3), 192--210.

\leavevmode\vadjust pre{\hypertarget{ref-croxton1932graphic}{}}%
Croxton, F., \& Stein, H. (1932). Graphic comparisons by bars, squares, circles, and cubes. \emph{Journal of the American Statistical Association}, \emph{27}(177), 54--60.

\leavevmode\vadjust pre{\hypertarget{ref-croxton1927bar}{}}%
Croxton, F., \& Stryker, R. (1927). Bar charts versus circle diagrams. \emph{Journal of the American Statistical Association}, \emph{22}(160), 473--482.

\leavevmode\vadjust pre{\hypertarget{ref-curcio1987comprehension}{}}%
Curcio, F. (1987). Comprehension of mathematical relationships expressed in graphs. \emph{Journal for Research in Mathematics Education}, \emph{18}(5), 382--393.

\leavevmode\vadjust pre{\hypertarget{ref-dehaene2008log}{}}%
Dehaene, S., Izard, V., Spelke, E., \& Pica, P. (2008). Log or linear? Distinct intuitions of the number scale in western and amazonian indigene cultures. \emph{Science}, \emph{320}(5880), 1217--1220.

\leavevmode\vadjust pre{\hypertarget{ref-deming1943statistical}{}}%
Deming, W. E., DEMING, W. E. A., Sons, J. W. \&., Chapman, \& (Londyn)., H. (1943). \emph{Statistical adjustment of data}. J. Wiley \& Sons, Incorporated. Retrieved from \url{https://books.google.com/books?id=9awgAAAAMAAJ}

\leavevmode\vadjust pre{\hypertarget{ref-dongarra1997top500}{}}%
Dongarra, J. J., Meuer, H. W., Strohmaier, E.others. (1997). TOP500 supercomputer sites. \emph{Supercomputer}, \emph{13}, 89--111.

\leavevmode\vadjust pre{\hypertarget{ref-dunham1991learning}{}}%
Dunham, P., \& Osborne, A. (1991). Learning how to see: Students graphing difficulties. \emph{Focus on Learning Problems in Mathematics}, \emph{13}(4), 35--49.

\leavevmode\vadjust pre{\hypertarget{ref-dunn1988framed}{}}%
Dunn, R. (1988). Framed rectangle charts or statistical maps with shading: An experiment in graphical perception. \emph{The American Statistician}, \emph{42}(2), 123--129.

\leavevmode\vadjust pre{\hypertarget{ref-eells1926relative}{}}%
Eells, W. C. (1926). The relative merits of circles and bars for representing component parts. \emph{Journal of the American Statistical Association}, \emph{21}(154), 119--132.

\leavevmode\vadjust pre{\hypertarget{ref-fagen-ulmschneider_2020}{}}%
Fagen-Ulmschneider, W. (2020). 91-DIVOC. \emph{An interactive visualization of COVID-19}. University of Illinois. Retrieved from \url{https://91-divoc.com/pages/covid-visualization/}

\leavevmode\vadjust pre{\hypertarget{ref-fechner1860elemente}{}}%
Fechner, G. T. (1860). \emph{Elemente der psychophysik} (Vol. 2). Breitkopf u. H{ä}rtel.

\leavevmode\vadjust pre{\hypertarget{ref-wordcloud_pkg}{}}%
Fellows, I. (2018). \emph{Wordcloud: Word clouds}. Retrieved from \url{https://CRAN.R-project.org/package=wordcloud}

\leavevmode\vadjust pre{\hypertarget{ref-finney_subjective_1951}{}}%
Finney, D. (1951). Subjective judgment in statistical analysis: An experimental study. \emph{Journal of the Royal Statistical Society: Series B (Methodological)}, \emph{13}(2), 284--297.

\leavevmode\vadjust pre{\hypertarget{ref-finney1948table}{}}%
Finney, D., \& Stevens, W. (1948). A table for the calculation of working probits and weights in probit analysis. \emph{Biometrika}, \emph{35}(1/2), 191--201.

\leavevmode\vadjust pre{\hypertarget{ref-forrester1990exploring}{}}%
Forrester, M., Latham, J., \& Shire, B. (1990). Exploring estimation in young primary school children. \emph{Educational Psychology}, \emph{10}(4), 283--300.

\leavevmode\vadjust pre{\hypertarget{ref-friel2001making}{}}%
Friel, S., Curcio, F., \& Bright, G. (2001). Making sense of graphs: Critical factors influencing comprehension and instructional implications. \emph{Journal for Research in Mathematics Education}, \emph{32}(2), 124--158.

\leavevmode\vadjust pre{\hypertarget{ref-glazer2011challenges}{}}%
Glazer, N. (2011). Challenges with graph interpretation: A review of the literature. \emph{Studies in Science Education}, \emph{47}(2), 183--210.

\leavevmode\vadjust pre{\hypertarget{ref-godlonton2018anchoring}{}}%
Godlonton, S., Hernandez, M., \& Murphy, M. (2018). Anchoring bias in recall data: Evidence from central america. \emph{American Journal of Agricultural Economics}, \emph{100}(2), 479--501.

\leavevmode\vadjust pre{\hypertarget{ref-goldstein2021sensation}{}}%
Goldstein, B., \& Cacciamani, L. (2021). \emph{Sensation and perception}. Cengage Learning.

\leavevmode\vadjust pre{\hypertarget{ref-gordon_statistician_2015}{}}%
Gordon, I., \& Finch, S. (2015). Statistician heal thyself: Have we lost the plot? \emph{Journal of Computational and Graphical Statistics}, \emph{24}(4), 1210--1229.

\leavevmode\vadjust pre{\hypertarget{ref-graesser2014new}{}}%
Graesser, A., Swamer, S., Baggett, W., \& Sell, M. (2014). New models of deep comprehension. In \emph{Models of understanding text} (pp. 9--40). Psychology Press.

\leavevmode\vadjust pre{\hypertarget{ref-green2009personal}{}}%
Green, T., \& Fisher, B. (2009). The personal equation of complex individual cognition during visual interface interaction. In \emph{Workshop on human-computer interaction and visualization} (pp. 38--57). Springer.

\leavevmode\vadjust pre{\hypertarget{ref-haemer_presentation_1949}{}}%
Haemer, K., \& Kelley, T. (1949). Presentation problems: Suiting the chart to the audience: Common graphic devices classified according to ease of reading. \emph{The American Statistician}, \emph{3}(5), 11--11.

\leavevmode\vadjust pre{\hypertarget{ref-heckler_student_2013}{}}%
Heckler, A., Mikula, B., \& Rosenblatt, R. (2013). Student accuracy in reading logarithmic plots: The problem and how to fix it. In \emph{2013 IEEE frontiers in education conference (FIE)} (pp. 1066--1071). IEEE.

\leavevmode\vadjust pre{\hypertarget{ref-heer2006multi}{}}%
Heer, J., \& Agrawala, M. (2006). Multi-scale banking to 45 degrees. \emph{IEEE Transactions on Visualization and Computer Graphics}, \emph{12}(5), 701--708.

\leavevmode\vadjust pre{\hypertarget{ref-hofmann_graphical_2012}{}}%
Hofmann, H., Follett, L., Majumder, M., \& Cook, D. (2012). Graphical tests for power comparison of competing designs. \emph{IEEE Transactions on Visualization and Computer Graphics}, \emph{18}(12), 2441--2448.

\leavevmode\vadjust pre{\hypertarget{ref-hogan2003quantitative}{}}%
Hogan, T., \& Brezinski, K. (2003). Quantitative estimation: One, two, or three abilities? \emph{Mathematical Thinking and Learning}, \emph{5}(4), 259--280.

\leavevmode\vadjust pre{\hypertarget{ref-allison_horst}{}}%
Horst, A. (2021, September 2). R \& stats illustrations {[}Github \{Repository\}{]}. Retrieved May 19, 2022, from \url{https://github.com/allisonhorst/stats-illustrations/}

\leavevmode\vadjust pre{\hypertarget{ref-jerne1949validity}{}}%
Jerne, N., \& Wood, E. (1949). The validity and meaning of the results of biological assays. \emph{Biometrics}, \emph{5}(4), 273--299.

\leavevmode\vadjust pre{\hypertarget{ref-global_epidemics_2021}{}}%
Jha, A. K., Allen, D., Tsai, T., Friedhoff, S., Ranney, M., \& Figueroa, J. (2021, July 20). Risk levels. Retrieved May 19, 2022, from \url{https://globalepidemics.org/key-metrics-for-covid-suppression/}

\leavevmode\vadjust pre{\hypertarget{ref-jolliffe1991assessment}{}}%
Jolliffe, F. (1991). Assessment of the understanding of statistical concepts. In \emph{Proceedings of the third international conference on teaching statistics} (Vol. 1, pp. 461--466).

\leavevmode\vadjust pre{\hypertarget{ref-jones_polynomial_1977}{}}%
Jones, Gregory. (1977). Polynomial perception of exponential growth. \emph{Perception \& Psychophysics}.

\leavevmode\vadjust pre{\hypertarget{ref-jones2012students}{}}%
Jones, Gail, Gardner, G., Taylor, A., Forrester, J., \& Andre, T. (2012). Students' accuracy of measurement estimation: Context, units, and logical thinking. \emph{School Science and Mathematics}, \emph{112}(3), 171--178.

\leavevmode\vadjust pre{\hypertarget{ref-joram2005children}{}}%
Joram, E., Gabriele, A., Bertheau, M., Gelman, R., \& Subrahmanyam, K. (2005). Children's use of the reference point strategy for measurement estimation. \emph{Journal for Research in Mathematics Education}, \emph{36}(1), 4--23.

\leavevmode\vadjust pre{\hypertarget{ref-katz_2017}{}}%
Katz, J. (2017). You draw it: Just how bad is the drug overdose epidemic? \emph{The New York Times}. Retrieved from \url{https://www.nytimes.com/interactive/2017/04/14/upshot/drug-overdose-epidemic-you-draw-it.html}

\leavevmode\vadjust pre{\hypertarget{ref-kruskal1975visions}{}}%
Kruskal, W. (1975). Visions of maps and graphs. In \emph{Proceedings of the international symposium on computer-assisted cartography, auto-carto II} (pp. 27--36). Citeseer.

\leavevmode\vadjust pre{\hypertarget{ref-leinhardt1990functions}{}}%
Leinhardt, G., Zaslavsky, O., \& Stein, M. (1990). Functions, graphs, and graphing: Tasks, learning, and teaching. \emph{Review of Educational Research}, \emph{60}(1), 1--64.

\leavevmode\vadjust pre{\hypertarget{ref-emmeans}{}}%
Lenth, R. (2021). \emph{Emmeans: Estimated marginal means, aka least-squares means}. Retrieved from \url{https://CRAN.R-project.org/package=emmeans}

\leavevmode\vadjust pre{\hypertarget{ref-lewandowsky_perception_1989}{}}%
Lewandowsky, S., \& Spence, I. (1989). The perception of statistical graphs. \emph{Sociological Methods \& Research}, \emph{18}(2-3), 200--242.

\leavevmode\vadjust pre{\hypertarget{ref-loy_variations_2016}{}}%
Loy, A., Follett, L., \& Hofmann, H. (2016). Variations of q--q plots: The power of our eyes! \emph{The American Statistician}, \emph{70}(2), 202--214.

\leavevmode\vadjust pre{\hypertarget{ref-loy_model_2017}{}}%
Loy, A., Hofmann, H., \& Cook, D. (2017). Model choice and diagnostics for linear mixed-effects models using statistics on street corners. \emph{Journal of Computational and Graphical Statistics}, \emph{26}(3), 478--492.

\leavevmode\vadjust pre{\hypertarget{ref-r2d3}{}}%
Luraschi, J., \& Allaire, J. (2018). \emph{r2d3: Interface to 'D3' visualizations}. Retrieved from \url{https://CRAN.R-project.org/package=r2d3}

\leavevmode\vadjust pre{\hypertarget{ref-macdonald1977numbers}{}}%
Macdonald-Ross, M. (1977). How numbers are shown. \emph{AV Communication Review}, \emph{25}(4), 359--409.

\leavevmode\vadjust pre{\hypertarget{ref-mackinnon_feedback_1991}{}}%
MacKinnon, A., \& Wearing, A. (1991). Feedback and the forecasting of exponential change. \emph{Acta Psychologica}, \emph{76}(2), 177--191.

\leavevmode\vadjust pre{\hypertarget{ref-majumder_validation_2013}{}}%
Majumder, M., Hofmann, H., \& Cook, D. (2013). Validation of visual statistical inference, applied to linear models. \emph{Journal of the American Statistical Association}, \emph{108}(503), 942--956.

\leavevmode\vadjust pre{\hypertarget{ref-menge_logarithmic_2018}{}}%
Menge, D., MacPherson, A., Bytnerowicz, T., Quebbeman, A., Schwartz, N., Taylor, B., \& Wolf, A. (2018). Logarithmic scales in ecological data presentation may cause misinterpretation. \emph{Nature Ecology \& Evolution}, \emph{2}(9), 1393--1402.

\leavevmode\vadjust pre{\hypertarget{ref-mosteller_eye_1981}{}}%
Mosteller, F., Siegel, A., Trapido, E., \& Youtz, C. (1981). Eye fitting straight lines. \emph{The American Statistician}, \emph{35}(3), 150--152.

\leavevmode\vadjust pre{\hypertarget{ref-myers1954accuracy}{}}%
Myers, R. (1954). Accuracy of age reporting in the 1950 united states census. \emph{Journal of the American Statistical Association}, \emph{49}(268), 826--831.

\leavevmode\vadjust pre{\hypertarget{ref-nieder2003coding}{}}%
Nieder, A., \& Miller, E. (2003). Coding of cognitive magnitude: Compressed scaling of numerical information in the primate prefrontal cortex. \emph{Neuron}, \emph{37}(1), 149--157.

\leavevmode\vadjust pre{\hypertarget{ref-corpus_pkg}{}}%
Perry, P. (2021). \emph{Corpus: Text corpus analysis}. Retrieved from \url{https://CRAN.R-project.org/package=corpus}

\leavevmode\vadjust pre{\hypertarget{ref-peterson1954accurately}{}}%
Peterson, L., \& Schramm, W. (1954). How accurately are different kinds of graphs read? \emph{Audio Visual Communication Review}, 178--189.

\leavevmode\vadjust pre{\hypertarget{ref-romano_scale_2020}{}}%
Romano, A., Sotis, C., Dominioni, G., \& Guidi, S. (2020). The scale of COVID-19 graphs affects understanding, attitudes, and policy preferences. \emph{Health Economics}, \emph{29}(11), 1482--1494.

\leavevmode\vadjust pre{\hypertarget{ref-rost_2020}{}}%
Rost, L. C. (2020, December). You've informed the public with visualizations about the coronavirus. Thank you. \emph{Datawrapper Blog}. Retrieved from \url{https://blog.datawrapper.de/coronavirus-data-visualization-effect-datawrapper/}

\leavevmode\vadjust pre{\hypertarget{ref-schneeweiss2010symmetric}{}}%
Schneeweiss, H., Komlos, J., \& Ahmad, A. (2010). Symmetric and asymmetric rounding: A review and some new results. \emph{AStA Advances in Statistical Analysis}, \emph{94}(3), 247--271.

\leavevmode\vadjust pre{\hypertarget{ref-mcr_pkg}{}}%
Schuetzenmeister, A., \& Model, F. (2021). \emph{Mcr: Method comparison regression}. Retrieved from \url{https://CRAN.R-project.org/package=mcr}

\leavevmode\vadjust pre{\hypertarget{ref-shah1999graphs}{}}%
Shah, P., Mayer, R., \& Hegarty, M. (1999). Graphs as aids to knowledge construction: Signaling techniques for guiding the process of graph comprehension. \emph{Journal of Educational Psychology}, \emph{91}(4), 690.

\leavevmode\vadjust pre{\hypertarget{ref-siegler_numerical_2017}{}}%
Siegler, R., \& Braithwaite, D. (2017). Numerical development. \emph{Annual Review of Psychology}, \emph{68}, 187--213.

\leavevmode\vadjust pre{\hypertarget{ref-tidytext_pkg}{}}%
Silge, J., \& Robinson, D. (2016). Tidytext: Text mining and analysis using tidy data principles in r. \emph{Journal of Open Source Software}, \emph{1}(3), 37.

\leavevmode\vadjust pre{\hypertarget{ref-lauren_2018}{}}%
Smith, M. (2017, March 6). Does pineapple belong on a pizza? Retrieved May 19, 2022, from \url{https://yougov.co.uk/topics/politics/articles-reports/2017/03/06/does-pineapple-belong-pizza}

\leavevmode\vadjust pre{\hypertarget{ref-spence_visual_1990}{}}%
Spence, I. (1990). Visual psychophysics of simple graphical elements. \emph{Journal of Experimental Psychology: Human Perception and Performance}, \emph{16}(4), 683.

\leavevmode\vadjust pre{\hypertarget{ref-star_trek}{}}%
\emph{Star trek: The original series. "The trouble with tribbles". Season 2. Episode 15.} (1967). Broadcast on NBC.

\leavevmode\vadjust pre{\hypertarget{ref-star_wars1}{}}%
\emph{Star wars: Episode IV -- a new hope}. (1977). 20th Century Fox.

\leavevmode\vadjust pre{\hypertarget{ref-star_wars2}{}}%
\emph{Star wars: Episode VI - return of the jedi}. (1983). 20th Century Fox.

\leavevmode\vadjust pre{\hypertarget{ref-lowndes2017our}{}}%
Stewart-Lowndes, J., Best, B., Scarborough, C., Afflerbach, J., Frazier, M., O'Hara, C., \ldots{} Halpern, B. (2017). Our path to better science in less time using open data science tools. \emph{Nature Ecology \& Evolution}, \emph{1}(6), 1--7.

\leavevmode\vadjust pre{\hypertarget{ref-szafir2018good}{}}%
Szafir, D. (2018). The good, the bad, and the biased: Five ways visualizations can mislead (and how to fix them). \emph{Interactions}, \emph{25}(4), 26--33.

\leavevmode\vadjust pre{\hypertarget{ref-tan1994human}{}}%
Tan, J. (1994). Human processing of two-dimensional graphics: Information-volume concepts and effects in graph-task fit anchoring frameworks. \emph{International Journal of Human-Computer Interaction}, \emph{6}(4), 414--456.

\leavevmode\vadjust pre{\hypertarget{ref-tan1990processing}{}}%
Tan, J., \& Benbasat, I. (1990). Processing of graphical information: A decomposition taxonomy to match data extraction tasks and graphical representations. \emph{Information Systems Research}, 416--439.

\leavevmode\vadjust pre{\hypertarget{ref-teghtsoonian1965judgment}{}}%
Teghtsoonian, M. (1965). The judgment of size. \emph{The American Journal of Psychology}, \emph{78}(3), 392--402.

\leavevmode\vadjust pre{\hypertarget{ref-raster_vs_svg}{}}%
Tol. (2021). Bitmap VS SVG. Retrieved from \url{https://commons.wikimedia.org/wiki/File:Bitmap_VS_SVG.svg}

\leavevmode\vadjust pre{\hypertarget{ref-tory2004human}{}}%
Tory, M., \& Moller, T. (2004). Human factors in visualization research. \emph{IEEE Transactions on Visualization and Computer Graphics}, \emph{10}(1), 72--84.

\leavevmode\vadjust pre{\hypertarget{ref-tufte1985visual}{}}%
Tufte, E. R. (1985). The visual display of quantitative information. \emph{The Journal for Healthcare Quality (JHQ)}, \emph{7}(3), 15.

\leavevmode\vadjust pre{\hypertarget{ref-walker1874statistical}{}}%
United States Census Office. 9Th Census, 1870, \& Walker, F. A. (1874). \emph{Statistical atlas of the united states based on the results of the ninth censuswith contributions from many eminent men of science and several departments of the government}. New York: J. Bien. Retrieved from \url{https://www.loc.gov/item/05019329/}

\leavevmode\vadjust pre{\hypertarget{ref-vanderplas_testing_2020}{}}%
Vanderplas, S., Cook, D., \& Hofmann, H. (2020). Testing statistical charts: What makes a good graph? \emph{Annual Review of Statistics and Its Application}, \emph{7}, 61--88.

\leavevmode\vadjust pre{\hypertarget{ref-vanderplas2015spatial}{}}%
VanderPlas, S., \& Hofmann, H. (2015). Spatial reasoning and data displays. \emph{IEEE Transactions on Visualization and Computer Graphics}, \emph{22}(1), 459--468.

\leavevmode\vadjust pre{\hypertarget{ref-vanderplas_clusters_2017}{}}%
VanderPlas, S., \& Hofmann, H. (2017). Clusters beat trend!? Testing feature hierarchy in statistical graphics. \emph{Journal of Computational and Graphical Statistics}, \emph{26}(2), 231--242.

\leavevmode\vadjust pre{\hypertarget{ref-varshney_why_2013}{}}%
Varshney, L., \& Sun, J. (2013). Why do we perceive logarithmically? \emph{Significance}, \emph{10}(1), 28--31.

\leavevmode\vadjust pre{\hypertarget{ref-vessey1991cognitive}{}}%
Vessey, I. (1991). Cognitive fit: A theory-based analysis of the graphs versus tables literature. \emph{Decision Sciences}, \emph{22}(2), 219--240.

\leavevmode\vadjust pre{\hypertarget{ref-villa_2022}{}}%
Villa, M. (2022, February). The rouble collapse against US dollar. Retrieved from \url{https://twitter.com/emmevilla/status/1498365717720223751}

\leavevmode\vadjust pre{\hypertarget{ref-vonbergmann_2021}{}}%
Von Bergmann, J. (2021, March 17). Xkcd\_exponential: Public health vs scientists. Retrieved from \url{https://github.com/mountainMath/xkcd_exponential}

\leavevmode\vadjust pre{\hypertarget{ref-von1927further}{}}%
Von Huhn, R. (1927). Further studies in the graphic use of circles and bars: I: A discussion of the eells' experiment. \emph{Journal of the American Statistical Association}, \emph{22}(157), 31--36.

\leavevmode\vadjust pre{\hypertarget{ref-waddell2005comparisons}{}}%
Waddell, W. (2005). Comparisons of thresholds for carcinogenicity on linear and logarithmic dosage scales. \emph{Human \& Experimental Toxicology}, \emph{24}(6), 325--332.

\leavevmode\vadjust pre{\hypertarget{ref-wagenaar_misperception_1975}{}}%
Wagenaar, W., \& Sagaria, S. (1975). Misperception of exponential growth. \emph{Perception \& Psychophysics}, \emph{18}(6), 416--422.

\leavevmode\vadjust pre{\hypertarget{ref-wickham2013graphical}{}}%
Wickham, H. (2013). Graphical criticism: Some historical notes. \emph{Journal of Computational and Graphical Statistics}, \emph{22}(1), 38--44.

\leavevmode\vadjust pre{\hypertarget{ref-ggplot2}{}}%
Wickham, H. (2016). \emph{ggplot2: Elegant graphics for data analysis}. springer.

\leavevmode\vadjust pre{\hypertarget{ref-wickham2010graphical}{}}%
Wickham, H., Cook, D., Hofmann, H., \& Buja, A. (2010). Graphical inference for infovis. \emph{IEEE Transactions on Visualization and Computer Graphics}, \emph{16}(6), 973--979.

\leavevmode\vadjust pre{\hypertarget{ref-wickham2016r}{}}%
Wickham, H., \& Grolemund, G. (2016). \emph{R for data science: Import, tidy, transform, visualize, and model data}. " O'Reilly Media, Inc.".

\leavevmode\vadjust pre{\hypertarget{ref-wilkinson2013grammar}{}}%
Wilkinson, L. (2013). \emph{The grammar of graphics}. Springer Science \& Business Media.

\leavevmode\vadjust pre{\hypertarget{ref-wood1968objectives}{}}%
Wood, R. (1968). Objectives in the teaching of mathematics. \emph{Educational Research}, \emph{10}(2), 83--98.

\leavevmode\vadjust pre{\hypertarget{ref-mgcv5}{}}%
Wood, S. (2003). Thin plate regression splines. \emph{Journal of the Royal Statistical Society: Series B (Statistical Methodology)}, \emph{65}(1), 95--114.

\leavevmode\vadjust pre{\hypertarget{ref-mgcv3}{}}%
Wood, S. (2004). Stable and efficient multiple smoothing parameter estimation for generalized additive models. \emph{Journal of the American Statistical Association}, \emph{99}(467), 673--686.

\leavevmode\vadjust pre{\hypertarget{ref-mgcv1}{}}%
Wood, S. (2011). Fast stable restricted maximum likelihood and marginal likelihood estimation of semiparametric generalized linear models. \emph{Journal of the Royal Statistical Society: Series B (Statistical Methodology)}, \emph{73}(1), 3--36.

\leavevmode\vadjust pre{\hypertarget{ref-mgcv4}{}}%
Wood, S. (2017). \emph{Generalized additive models: An introduction with r} (2nd ed.). Chapman; Hall/CRC.

\leavevmode\vadjust pre{\hypertarget{ref-mgcv2}{}}%
Wood, S., Pya, N., \& Säfken, B. (2016). Smoothing parameter and model selection for general smooth models. \emph{Journal of the American Statistical Association}, \emph{111}(516), 1548--1563.

\leavevmode\vadjust pre{\hypertarget{ref-graphcrimes_2022}{}}%
Wordtips. (2022, January). Where in the {World} {Is} the {Best} at {Solving} {Wordle}? Retrieved May 19, 2022, from \url{https://word.tips/wordle-wizards/}

\end{CSLReferences}


%% backmatter is needed at the end of the main body of your thesis to
%% set up page numbering correctly for the remainder of the thesis
\backmatter

%% Start the correct formatting for the appendices
% \appendix
%% Input each appendix here
% \input{./appendix_a}

%% Bibliography goes here (You better have one)
%% BibTeX is your friend

% \bibliographystyle{alpha}  % or use  abbrv to abbreviate first names and use numerical indices
\bibliographystyle{abbrv}  % or use  abbrv to abbreviate first names and use numerical indices
%% Add your BibTex file here (don't include the .bib)
\bibliography{./references}



%% Index go here (if you have one)
\end{document}
