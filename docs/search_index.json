[["index.html", "HUMAN PERCEPTION OF EXPONENTIALLY INCREASING DATA DISPLAYED ON A LOG SCALE EVALUATED THROUGH EXPERIMENTAL GRAPHICS TASKS CHAPTER 1 Literature Review 1.1 Introduction to Graphics 1.2 Perception and Psychophysics 1.3 Graphical Experiments 1.4 Logarithmic Scales and Mapping 1.5 Underestimation of Exponential Growth 1.6 Research Objectives", " HUMAN PERCEPTION OF EXPONENTIALLY INCREASING DATA DISPLAYED ON A LOG SCALE EVALUATED THROUGH EXPERIMENTAL GRAPHICS TASKS Emily Anna Robinson Abstract Log scales are often used to display data over several orders of magnitude within one graph. Three graphical experimental tasks were conducted to evaluate the impact displaying data on the log scale has on human perception of exponentially increasing trends compared to displaying data on the linear scale. The first experiment evaluates whether our ability to perceptually notice differences in exponentially increasing trends is impacted by the choice of scale. Participants were shown a set of plots and asked to identify which plot appeared to differ most from the other plots. Results indicated that when there was a large difference in exponential curves, the choice of scale had no impact and participants accurately differentiated between the two curves on both the linear and log scale. However, displaying exponentially increasing data on a log scale improved the accuracy of differentiating between exponentially increasing curves with slight differences. An exception occurred when identifying an exponential curve from curves closely resembling a linear trend, indicating it is easy to identify a curve in a group of lines but much harder to identify a line in a group of curves. The other experimental tasks focus on determining whether there are cognitive disadvantages to log scales: do log scales make it harder to make use of graphical information? To test an individuals ability to make predictions for exponentially increasing data, participants were asked to draw a line using their computer mouse through an exponentially increasing trend shown on both the linear and log scale. Results showed that when exponential growth rate is large, underestimation of exponential growth occurs when making predictions on a linear scale and there is an improvement in accuracy of predictions made on the log scale. The last experimental task is designed to test an individuals ability to translate a graph of exponentially increasing data into real value quantities and make comparisons of estimates. The results of the three experimental tasks provide guidelines for readers to actively choose which of many possible graphics to draw in order to ensure their charts are effective at communicating the intended result. CHAPTER 1 Literature Review 1.1 Introduction to Graphics Advanced technology and computing power have promoted data visualization as a central tool in modern data science. Unwin (2020) defines data visualization as the art of drawing graphical charts in order to display data. Graphics are useful for data cleaning, exploring data structure, and have been an essential component in communicating information for the last 200 years (Lewandowsky &amp; Spence, 1989) During the \\(\\text{18}^{\\text{th}}\\) and \\(\\text{19}^{\\text{th}}\\) centuries, governments began using graphics in order to better understand their population and economic interests (Harms, 1991; Playfair, 1801; Walker et al., 2013). In , Playfair (1801) visually represents the power and economic status of each European Nation in the early \\(\\text{19}^{\\text{th}}\\) century. Circle size and numeric annotation within the circle indicate the number of square miles in each country with the number of people per square mile indicated above the circle. The vertical bar to the left side represents the number of inhabitants (millions), and the line to the right side represents the revenue (million pounds). Color in the original figure, not shown, identifies countries as maritime powers or powerful by land only. The Statistical atlas of the United States (Walker et al., 2013) used charts and graphics to display data compiled from the 1870 US census. displays the population of each state where square size represents the proportion of the states population separated into three regions representing the origin and race of the population. The rectangle shown to the right represents the proportion of residents born in the state who have become residents of other states. In the \\(\\text{20}^{\\text{th}}\\) century, companies began utilizing graphics to understand their mechanics and support business decisions and news sources began displaying graphics of weather forecasts as a means to communicate critical information and aid in decision-making (Chandar, Collier, &amp; Miranti, 2012; Yates, 1985). Chandar et al. (2012) illustrates how AT&amp;T used graphics to demonstrate managements ability to optimize utilization of assets by making comparisons between their annual net telephone revenues and their returns on total assets . Today, we encounter data visualizations everywhere; researchers include graphics to communicate their results in scientific publications and mass media present graphics in order to convey news stories to the public through newspapers, TV, and the Web (Aisch et al., 2016; Gouretski &amp; Koltermann, 2007; Silver, 2020). Figure 1.1: William Playfairs balance of trate Figure 1.2: Statistical Atlas 1870 state population Figure 1.3: AT&amp;T utilization of assets Although statistical graphics have become widely used and valued in science, business, and in many other aspects of life, as creators of graphics, we are too accepting of them as default without asking critical questions about the graphics we create or view (Unwin, 2020). Vanderplas, Cook, &amp; Hofmann (2020) poses the general question we must ask ourselves, how effective is this graph at communicating useful information? An effective graphic accurately shows the data through the appropriate chart selection, axes and scales, and aesthetic design choices in order to successfully communicate the intended result. Higher quality of technology has influenced the creation, replication, and complexity of graphics as there are an infinitely many number of graphical displays and design choices that can be implemented at faster speeds with more flexibility. The creator of a graphic makes decisions about the variables displayed, the type of graphic, the size of the graphic and the aspect ratio, the colors and symbols used, the scales and limits, and the ordering of categorical variables. In response to the increasing number of design choices, consistent themes and higher standards are being placed on graphics. Selecting from an extensive list of styles and choices of graphics in order to effectively communicate insights into the data is a challenging task. A consistent concern is the lack of theory of graphics available to build on; better theory should result in better graphics. Creators of graphics need an established set of concepts and terminology to build their graphics from so they can actively choose which of many possible graphics to draw in order to ensure their charts are effective at communicating the intended result. Many efforts have been made to provide guidelines for graphical designs including Wilkinsons Grammar of Graphics (Wilkinson, 2012). The grammar of graphics serves as the fundamental framework for data visualization with the notion that graphics are built from the ground up by specifying exactly how to create a particular graph from a given data set. Visual representations are constructed through the use of tidy data which is characterized as a data set in which each variable is in its own column, each observation is in its own row, and each value is in its own cell (Hadley Wickham &amp; Grolemund, 2016). Graphics are viewed as a mapping from variables in a data set (or statistics computed from the data) to visual attributes such as the axes, colors, shapes, or facets on the canvas in which the chart is displayed. illustrates the process of creating a graphic from a data set through the use of variable mapping, data transformations, coordinate systems, and aesthetic features (Vanderplas et al., 2020) Software, such as Hadley Wickhams ggplot2 (Hadley Wickham, 2011), aims to implement the framework of creating charts and graphics as the grammar of graphics recommends. Figure 1.4: Graphic flowchart Despite past attempts to improve the use of graphics in science, graphics displayed in academic research are still falling short of the standards. Gordon &amp; Finch (2015) evaluated 97 graphs for overall quality, based on five principles of graphical excellence including: (1) show the data clearly (2) use simplicity in design (3) use good alignment on a common scale for quantities to be compared (4) keep the visual encoding transparent (5) use graphical forms consistent with principles 1 and 4. The authors randomly sampled 97 graphs published in A* (top 5%) journals with work in statistics and applied science disciplines. There were 50 graphs sampled from the most recently available issues of A* journals in applied sciences such as environmental sciences, agricultural and veterinary sciences, medical and health sciences, education, economics, and psychology. The additional 47 graphs were randomly sampled from A* statistics journals. Each graph was scored based on 60 features related to the five principles, such as proper axes labels. Both authors assigned an overall quality rating (poor, adequate, good, or exemplary) to each of the graphs sampled; discussion between authors settled any discrepancies in ratings. The authors rated 39% of the 97 graphs sampled as poor, indicating there is still an astonishing lack in the quality of graphics. More startling is the fact that the source of the graphic from an applied science or a graphic from statistics had no effect on the quality of the graphic. In efforts to achieve a higher standard of the graphics being presented, future work must be done to implement the academic research being conducted in graphics into practice. 1.2 Perception and Psychophysics 1.2.1 Perceptual Process In order to develop guiding principles for generating graphics effective in communication, we must first understand the basic mechanics of the human perceptual system and the biases we are vulnerable to (Goldstein &amp; Brockmole, 2017). The perceptual process is a sequence of steps used to describe a how a stimulus in the environment leads to our perception of the stimulus and action in response to the stimulus . This process is separated into sensation (Carlson, 2010) - involving simple processes that occur right at the beginning of a sensory system - and perception (Myers &amp; DeWall, 2021) - involving higher-order mechanisms and identified with more complex processes. The perceptual process begins when there is a stimulus in the environment and light is reflected and focused back into the viewers eyes. Within the eye, the light reflected is transformed and focused by the eyes optical system and an image is formed on the receptors of the viewers retina. It is important to note that everything a person perceives is based not on direct contact with stimuli but on representations of stimuli that are formed on the receptors and the resulting activity in the persons nervous system. Once light is reflected and focused, our visual receptors respond to the light and transform the light energy into electrical energy through a process called transduction. Signals from the receptors are then transmitted through the retina, to the brain, and then within the brain where perception (what do you see?) and recognition (what is it called?) occur. After recognition, viewers take some sort of motor action; for example, the viewer might move closer to the object. The perceptual process is not direct and instead takes on more of a cyclic nature where a person may go through many iterations of stimuli, perception, recognition, and action before the final image is identified and understood (M. A. Peterson, 1994). Figure 1.5: Perceptual process When perception occurs, we first experience the preattentive stage in which we observe color, shape, size, and other basic information about the stimuli being perceived. Preattentive perception effects are automatically processed within the first 500 milliseconds of viewing and do not depend on sustained cognitive attention (Vanderplas et al., 2020). Following the preattentive stage, direct attention is required for additional processing to allow us to draw connections between components that assist in our interpretation of the stimuli. When viewing a chart or graph, most insights we gain are due to the cognitive processes that occur after attention is focused on specific aspects of the graph. The relationship between physiology and perception can provide us information about how graphics may be understood and interpreted. Through experimentation, the physiological response (automatic reaction) is related to the behavioral response (perception, recognition, and action). For example, Furmanski &amp; Engel (2000) tested behavioral responses with functional magnetic resonance imaging (fMRI) techniques to show that the the human visual system is more sensitive to horizontal and vertical stimuli than to stimuli at other orientations. According to a cognitive analysis, graph interpretation involves (a) relatively simple pattern perception and association processes in which viewers can associate graphic patterns to quantitative referents and (b) more complex and error-prone inferential processes in which viewers must mentally transform data (Shah, Mayer, &amp; Hegarty, 1999). Shah &amp; Carpenter (1995) established the process in which viewers interact with charts by first perceptually observing the visual features and later translating to cognitive processing of the information depicted by those features. A viewer must first encode the visual array by identifying meaningful visual features (for example, a straight light slanting downward). Next, the viewer must classify the quantitative measures and relationships in which those visual features illustrate (for example, a decreasing linear relationship between x and y). The last step involves translating the quantitative measures and relationships to the variables defined in the data set (for example, a population decreasing over years). Psychophysics, the branch of psychology that deals with the relationships between physical stimuli (for example, light) and mental phenomena, aims to provide explanations of the relationship between physiology and perception and point out human perceptual biases. By examining both behavior and physiology together, we are able to understand the mechanisms responsible for perception. 1.2.2 Logarithmic Perception Ernst Weber, an early psychophysics researcher, discovered a phenomenon known as Webers law, by determining the relationship between the difference threshold (smallest detectable difference between two sensory stimuli; known as the Just Noticeable Difference) and the magnitude of a stimulus (Fechner, 1860). This holds true for a variety of stimuli such as weight, light, and sound as well as for a range of magnitudes; larger numbers require a proportional larger difference in order to remain equally discriminate (Dehaene, Izard, Spelke, &amp; Pica, 2008). Known as Webers law, it was established that we do not notice absolute changes in stimuli, but instead that we notice the relative change (Sun, Wang, Goyal, &amp; Varshney, 2012). Numerically, Webers Law is defined as \\[\\begin{equation} \\frac{\\Delta S}{S} = K \\end{equation}\\] where \\(\\Delta S\\) represents the difference threshold, S represents the initial stimulus intensity, and K is called Webers contrast which remains constant as the magnitude of S changes. Gustav Fechner, a founder of psychophysics, provided further extension to Webers law by discovering the relationship between the perceived intensity is logarithmic to the stimulus intensity when observed above a minimal threshold of perception (Sun et al., 2012). Formally known as the Weber-Fechner law, it is derived from Webers law as \\[\\begin{equation} P = K\\ln \\frac{S}{S_0} \\end{equation}\\] where P represents the perceived stimulus, K represents Webers contrast, S represents the initial stimulus intensity, and \\(S_0\\) represents the minimal threshold of perception. 1.3 Graphical Experiments One way in which we determine the relationship between behavior and physiology is through the use of graphical tests (Cleveland &amp; McGill, 1984; Lewandowsky &amp; Spence, 1989; Spence, 1990; VanderPlas &amp; Hofmann, 2015). These tests may take many forms: identifying differences in graphs, accurately reading information off a chart, using data to make correct real-world decisions, or predicting the next few observations. All of these types of tests require different levels of use and manipulation of the information presented in the chart. The initial push to develop classification and recommendation systems for charts was grounded on heuristics rather than on experimentation (Kruskal, 1975; Macdonald-Ross, 1977). Request were made for the validation of the perception and utility of statistical charts through graphical experiments. Initial experiments struggled with methodological issues (Croxton &amp; Stein, 1932; Croxton &amp; Stryker, 1927; Eells, 1926) with most early experimentation stemming from psychophysics research on the perception of size and shape (Teghtsoonian, 1965). In attempts to understand the human perception and judgment of component parts, Eells (1926) instructed students to think of each circle diagram as representing 100% and write their best estimate of the percentage of the whole in each sector. Participants were told not to hurry, but to work steadily in order to determine efficiency of judgment. Students were then asked to analyze their mental processes used to make their estimates and indicate the method that best matches: by areas of sectors, by central angles, by arcs on the circumference, by subtending chords. This process was repeated three days later by presenting students the same data represented in bar diagrams . Results of the study led the authors to argue for the use of circle diagrams to show component parts based on both participant accuracy and speed. In response, Croxton &amp; Stryker (1927) evaluated the accuracy of judgment of two types of charts (bars and circles) in efforts to reach a consistent conclusion. During class, students were individually presented pairs of diagrams (without scales) on cards and asked to estimate the percentages displayed in the diagram. It was found that the bar was preferable to the circle when shown percentages that deviate from quarters, but that the circle is strongly preferred when shown percentages separating the diagrams into 25% or 50%. Figure 1.6: Eells (1926) component parts diagrams While a typical psychophysics experiment focuses on whether an effect is detectable and whether the magnitude of the effect can be accurately estimated, these early experiments instead depended on speed and accuracy for plot evaluation (Lewandowsky &amp; Spence, 1989; Spence, 1990; Teghtsoonian, 1965). In attempts to understand the visual psychophysics of simple graphical elements, Spence (1990), presented stimuli (tables, lines - horizontal and vertical, bars, boxes, cylinders, pie charts, and disk charts) to participants on a monitor screen in a computer lab. Participants were asked to use their cursor to position the marker to indicate the proportion to the apparent sizes of the elements . Results found that the table elements (numbers), pie elements, and bar elements led to the most accurate proportion estimates; boxes and disk elements resulted in the least accurate estimates. Measuring the speed at which participants made their judgments, it was found that two- and three- dimensional stimuli (for example, pie charts and box charts) assisted in faster judgment than zero- or one- dimensional stimuli (for example, tables and lines). Figure 1.7: Spence (1990) task display Cognitive psychologists and statisticians made progress by conducting experiments to identify perceptual errors associated with different styles of graphics and charts (Cleveland &amp; McGill, 1984, 1985; Shah et al., 1999). Cleveland &amp; McGill (1984) provides a basis for perceptual judgment, still utilized today, by examining six basic aesthetic design choices: position along a common scale, position along nonaligned scales, length, angle, slope, and area. Shah et al. (1999) established the notion that redesigning graphs can result in the improvement of the viewers interpretation of the data. For example, the use of gestalt principles (Goldstein &amp; Brockmole, 2017) such as proximity, similarity, and good continuation can help minimize the inferential processes and maximize the pattern association processes required to interpret relevant information. These later experiments followed similar methodology as early studies by asking participants to read information directly from the charts and provide a quantitative estimate or answer a predefined question; as with the early studies, accuracy and response time were evaluated (Amer, 2005; Broersma &amp; Molenaar, 1985; Dunn, 1988; L. V. Peterson &amp; Schramm, 1954; Tan, 1994). Spence (1990) presents four example questions for comparing the sizes of individual graphical elements: (1) How much greater was the rainfall in September than May? (2) Is the price of oil in constant dollars increasing or decreasing from year to year? (3) Do more people subscribe to Time than Newsweek? and (4) Did the ABC Corporation pay the largest dividends last year, or did XYZ? Amer (2005) demonstrates that visual illusion may bias decision making and graph comprehension, even if the graphs are constructed according to best practice. Participants were presented a cost volume profit graph with two crossing lines (revenue and cost) and asked to estimate three values: (1) the amount of total revenues on the ordinate corresponding to the endpoint of the total-revenue line plotted on the graph (2) the amount of total costs on the ordinate corresponding to the endpoint of the total-cost line plotted on the graph and (3) the amount of costs/revenues on the ordinate at the break even pointthe point where the two lines cross. Results indicate that decision makers may consistently underestimate or overestimate the values displayed on line graphs due to what is called the Poggendorff illusion. In Dunn (1988), participants were shown two maps, an unclassed choropleth map and a framed rectangle chart, indicating the murder rate of each US state . The goal of the study was to assess the relative accuracy with which quantitative information is extracted from both types of charts. Participants were strictly informed that the experiment was designed to test the ability of individuals to read or decode statistical maps and asked to write down their estimate of the murder rate as accurately as possible beside the 24 named states. Results indicate that subjects found it easier to extract quantitative information from the framed rectangle chart than from the unclassed choropleth map and that the between individual variability in the choropleth map was related to the area of the state. Figure 1.8: Amer (2005) cost volume profit graph Figure 1.9: Dunn (1988) maps During the \\(\\text{21}^{\\text{st}}\\) century, there have been advancements in the methodology used to investigate the effectiveness of statistical charts (Majumder, Hofmann, &amp; Cook, 2013). Buja et al. (2009a) introduced the lineup protocol in which data plots are depicted and interpreted as statistics. Supported by the grammar of graphics, a data plot can be characterized as a statistic, defined as, a functional mapping of a variable or set of variables (Vanderplas et al., 2020). This allows the data plot to be tested similar to other statistics, by comparing the actual data plot to a set of plots with the absence of any data structure we can test the likelihood of any perceived structure being significant. The construction of data plots as statistics allow for easy experimentation, granting researchers the ability to compare the effectiveness of and understand the perception of different types of charts. While the lineup protocol differs from methodology used in earlier studies, the focus is still on initial perception and graph comprehension with a relatively small amount of work conducted to understand the effect of design choices on higher cognitive processes such as learning or analysis (Green &amp; Fisher, 2009). Lineups serve as a powerful tool for testing perceived differences by eliminating ambiguous questions. However, the lineup protocol is constrained by the inability to test higher order cognitive skills such as accurately reading information off of a graph or drawing conclusions from the graph, limiting their ability to be used for testing real-world applications. 1.4 Logarithmic Scales and Mapping Figure 1.10: 91-DOVIC New Daily Case Counts as of July 2021 Figure 1.11: COVID-19 Risk Level Map as of July 2020 We have recently experienced the impact graphics and charts have on a large scale through the SARSNCOV-2 pandemic (COVID-19). At the beginning of 2020, we saw an influx of dashboards being developed to display case counts, transmission rates, and outbreak regions (Rost, 2020); mass media routinely showed charts to share information with the public about the progression of the pandemic (Romano, Sotis, Dominioni, &amp; Guidi, 2020). Fagen-Ulmschneider (2020) began the 91-DIVOC project to explore the global growth of COVID-19 through interactive graphics updated daily. The interactive graphics allowed viewers to explore the current status of COVID-19 by selecting their desired regions, axes, axis scale, and measure of interest (for example, case count, death count, and vaccine count); (Fagen-Ulmschneider, 2020) shows the new confirmed COVID-19 cases per day, normalized by population, as of July 2021. Other graphics displayed COVID-19 data as maps with color indicating the severity and risk in each US county (Risk levels, 2021). People began seeking out graphical displays of COVID-19 data as a direct result of these pieces of work (Rost, 2020); providing increased and ongoing exposure to these graphics over time. illustrates the increased views Datawrapper, a user-friendly web tool used to create basic interactive charts, had during the COVID-19 pandemic (Rost, 2020). Many of these graphics helped guide decision makers to implement policies such as shut-downs or mandated mask wearing, as well as facilitated communication with the public to increase compliance (Bavel et al., 2020). As graphics began to play an important role in the communication of the pandemic, creators of graphics were faced with design choices in order to ensure their charts were effective at accurately communicating the current status of the pandemic. Figure 1.12: Datawrapper daily chart views during COVID-19 Figure 1.13: Usefulness of the log scale in science When faced with data which spans several orders of magnitude, we must decide whether to show the data on its original scale (compressing the smaller magnitudes into relatively little area) or to transform the scale and alter the contextual appearance of the data. One common solution is to use a log scale transformation to display data over several orders of magnitude within one graph. The usefulness of the log scale in science is illustrated in (Munroe, 2005) showing the challenge of displaying the fuel energy density of Uranium along side other sources of fuel due to differences in magnitude of density. Exponential curves are a common source of data in which smaller magnitudes are compressed into a smaller area; presents an exponential curve displayed on both the linear and log scale illustrating the use of the log scale when displaying data which spans several magnitudes. Logarithms convert multiplicative relationships (for example, 1 &amp; 10 displayed 10 units apart and 10 &amp; 100 displayed 90 units apart) to additive relationships (for example, 1 &amp; 10 and 10 &amp; 100 both equally spaced along the axis), showing proportional relationships and linearizing power functions (Menge et al., 2018). They also have practical purposes, easing the computation of small numbers such as likelihoods and transforming data to fit statistical assumptions. When presenting log scaled data, it is possible to use either un-transformed scale labels (for example, values of 1, 10 and 100 are equally spaced along the axis) or log transformed scale labels (for example, 0, 1, and 2, showing the corresponding powers of 10). Figure 1.14: Linear scale verses log scale We have recently experienced the benefits and pitfalls of using log scales as COVID-19 dashboards displayed case count data on both the log and linear scale (Burn-Murdoch et al., 2020; Fagen-Ulmschneider, 2020). In spring 2020, during the early stages of the COVID-19 pandemic, there were large magnitude discrepancies in case counts at a given time point between different geographic regions (for example states and provinces as well as countries and continents). During this time, we saw the usefulness of log scale transformations showing case count curves for areas with few cases and areas with many cases within one chart. The usefulness of log scales in comparing deaths attributed to COVID-19 between countries as of March 2020 is illustrated in ; the diagonal reference lines provide a visual aid useful for interpretation (Burn-Murdoch et al., 2020). As the pandemic evolved, and the case counts were no longer spreading exponentially, graphs with linear scales seemed more effective at spotting early increases in case counts that signaled more localized outbreaks. In and , the daily case counts as of June 30, 2020 are displayed on both the linear and log scales respectively (Burn-Murdoch et al., 2020). The effect of the linear scale appears to evoke a stronger reaction from the public than the log scale as daily case counts are clearly rising rapidly during the summer wave. This is only one recent example of a situation in which both log and linear scales are useful for showing different aspects of the same data. There are long histories of using log scales to display results in ecology, psychophysics, engineering, and physics (Heckler, Mikula, &amp; Rosenblatt, 2013; Menge et al., 2018) In Waddell (2005), comparisons were made between the linear and logarithmic scales for the relationship between dosage and carcinogenicity in rodents. Results favored the use of logarithmic scales for doses in order to put the relative doses into perspective whereas using a linear scale to administer doses to animals with the same chemicals to which humans are exposed does not provide useful, comparative information. Given the widespread use of logarithmic scales, it is important to understand the implications of their use in order to provide guidelines for best use. Figure 1.15: Covid 19 Deaths (log scale) as of March 23, 2020 Figure 1.16: Covid 19 Case Counts (linear scale) as of June 30, 2020 Figure 1.17: Covid 19 Case Counts (log scale) as of June 30, 2020 When we first learn to count, we begin counting by ones (for example, 1, 2, 3, etc.), then by tens (for example, 10, 20, 30, etc.), and advancing to hundreds (for example, 100, 200, 300, etc.), following the base10 order of magnitude system (for example, 1, 10, 100, etc.). Research suggests our perception and mapping of numbers to a number line is logarithmic at first, but transitions to a linear scale later in development, with formal mathematics education (Dehaene et al., 2008; Siegler &amp; Braithwaite, 2017, 2017; Varshney &amp; Sun, 2013). For example, a kindergartner asked to place numbers one through ten along a number line would place three close to the middle, following the logarithmic perspective (Varshney &amp; Sun, 2013); demonstrates how a kindergartner might map numbers along a number line. Dehaene et al. (2008) found that with basic training, members of remote cultures with a basic vocabulary and minimal education understood the concept that numbers can be mapped into a spacial space; for example, numbers can be mapped to a number line or numbers can be mapped onto a clock. There was a gradual transition from logarithmic to linear scale as the mapping of whole number magnitude representations transitioned from a compressed (approximately logarithmic) distribution to an approximately linear one. These results indicate the universal and cultural-dependent characteristics of the sense of number. Figure 1.18: Kindergarten example of mapping numbers 1-10 along a number line Assuming there is a direct relationship between perceptual and cognitive processes, it is reasonable to assume numerical representations should also be displayed on a nonlinear, compressed number scale. Therefore, if we perceive logarithmically by default, it is a natural (and presumably low effort) way to display information and should be easy to read and understand/use. The idea is compression enlarges the coding space, thus increasing the dynamic range of perception and firing neurons within our visual system (Nieder &amp; Miller, 2003). Similar to the training and education required to transition from logarithmic mapping to linear mapping, there is also necessary training required in the assessment of graphical displays associated with logarithmic scales. Haemer &amp; Kelley (1949) identify semi-logarithmic charts for temporal series as requiring a certain degree of technical training. 1.5 Underestimation of Exponential Growth Figure 1.19: Log scale comic People with diverse backgrounds can interpret the same display of data in vastly different ways; (von Bergmann, 2021) illustrates how individuals in public health interpret exponential growth distinctly different from scientists during early, middle, and late stages of growth. Exponential growth is often misjudged in early stages, appearing to have a small growth rate. As exponential growth continues, the middle stage appears to be growing, but not at an astounding rate, appearing more quadratic. It is not until late stages of exponential growth when it is quite apparent that there is exponential growth occurring. This misinterpretation can lead to decisions made under inaccurate understanding causing future consequences. Early studies explored the estimation and prediction of exponential growth and found that growth is underestimated when presented both numerically and graphically (Wagenaar &amp; Sagaria, 1975). Results indicated that numerical estimation is more accurate than graphical estimation for exponential curves. Experimental studies were conducted in order to determine strategies to improve the accuracy of estimation of exponential growth (Jones, 1977; Mackinnon &amp; Wearing, 1991; Wagenaar &amp; Sagaria, 1975). There was no improvement in estimation found when participants had contextual knowledge or experience with exponential growth, but instruction on exponential growth reduced the underestimation; participants adjusted their initial starting value but not their perception of the growth rate (Jones, 1977; Wagenaar &amp; Sagaria, 1975). Mackinnon &amp; Wearing (1991) found that estimation was improved by providing immediate feedback to participants about the accuracy of their current predictions. Our inability to accurately predict exponential growth might also be addressed by log transforming the data, however, this transformation introduces new complexities; most readers are not mathematically sophisticated enough to intuitively understand logarithmic math and translate that back into real-world effects. In Menge et al. (2018), ecologists were surveyed to determine how often ecologists encounter log scaled data and how well ecologists understand log scaled data when they see it in the literature. Participants were presented three relationships displayed on linear-linear scales, log-log scales with untransformed values, or loglog scales with log transformed values . The authors propose three types of misconceptions participants encountered when presented data on log-log scales: hand-hold fallacy, Zenos zero fallacy, and watch out for curves fallacies. These misconceptions are a result of linear extrapolation assuming that a line in log-log space represents a line instead of the power law (which is an exponential relationship) in linear-linear space. Figure 1.20: Graphs viewed in Menge (2018) survey The hand-hold fallacy stems from the misconception that steeper slopes in log-log relationships are steeper slopes in linear-linear space, illustrated in d-f.  In fact, it is not only the slope that matters, but also the intercept and the location on the horizontal axis since a line in log-log space represents a power law in linear-linear space (linear extrapolation). Emerging from Zenos zero fallacy is the misconception that positively sloped lines in log-log space can imply a non-zero value of y when x is zero, illustrated in a-c and d-f. This is never true as positively sloped lines in log-log space actually imply that y = 0 when x = 0. This misconception again is a result of linear extrapolation assuming that a line in log-log space represents a line instead of the power law in linear-linear space. The last misconception, watch out for curves fallacies encompasses three faults: (1) lines in log-log space are lines in linear-linear space, illustrated in d-f, (2) lines in log-log space curve upward in linear-linear space, illustrated in d-f, and (3) curves in log-log space have the same curvature in linear-linear space, illustrated in g-i. Linear extrapolation is again responsible for the first and third faults while the second fault is a result of error in thinking that log-log lines represent power laws, and all exponential relationships curve upward; this is only true when the log-log slope is greater than one. Menge et al. (2018) found that in each of these scenarios, participants were confident in their incorrect responses, indicating incorrect knowledge rather than a lack of knowledge. 1.6 Research Objectives In my research, I conduct three graphical experimental tasks to evaluate the impact displaying data on the log scale has on human perception of exponentially increasing trends compared to displaying data on the linear scale. The first experiment evaluates whether our ability to perceptually notice differences in exponentially increasing trends is impacted by the choice of scale. I conducted a visual inference experiment in which participants were shown a series of lineups and asked to identify the plot that differed most from the surrounding plots. The other experimental tasks focus on determining whether there are cognitive disadvantages to log scales: do log scales make it harder to make use of graphical information? To test an individuals ability to make predictions for exponentially increasing data, participants were asked to draw a line using their computer mouse through an exponentially increasing trend shown on both the linear and log scale. In addition to differentiation and prediction of exponentially increasing data, an experimental task was conducted to test an individuals ability to translate a graph of exponentially increasing data into real value quantities and extend their estimations by making comparisons. The results of the three experimental tasks provide guidelines for readers to actively choose which of many possible graphics to draw, according to some set of design choices, to ensure their charts are effective. "],["2-lineups.html", "CHAPTER 2 Perception through lineups 2.1 Introduction 2.2 Visual Inference 2.3 Data Generation 2.4 Parameter Selection 2.5 Lineup Setup 2.6 Study Design 2.7 Results 2.8 Discussion and Conclusion", " CHAPTER 2 Perception through lineups 2.1 Introduction To lay a foundation for future exploration of the use of log scales, I begin with the most fundamental ability to identify differences in charts; this does not require that participants understand exponential growth, identify log scales, or have any mathematical training. Instead, I am simply testing the change in perceptual sensitivity resulting from visualization choices. The study in this chapter is conducted through visual inference and the use of statistical lineups (Buja et al., 2009a) to differentiate between exponentially increasing curves with differing levels of curvature, using linear and log scales. 2.2 Visual Inference Previously, I explained how a data plot can be evaluated and treated as a visual statistic, a numerical function which summarizes the data. To evaluate a graph, the statistic (data plot) must be run through a visual evaluation - a person. We can conclude the visual statistics are significantly different if two different methods of presenting data result in qualitatively different results when evaluated visually. Recent graphical experiments have utilized statistical lineups to quantify the perception of graphical design choices (Hofmann, Follett, Majumder, &amp; Cook, 2012; Loy, Follett, &amp; Hofmann, 2016; Loy, Hofmann, &amp; Cook, 2017; VanderPlas &amp; Hofmann, 2017). Statistical lineups provide an elegant way of combining perception and statistical hypothesis testing using graphical experiments (Majumder et al., 2013; Vanderplas et al., 2020; H. Wickham, Cook, Hofmann, &amp; Buja, 2010). Lineups are named after the police lineup of criminal investigations where witnesses are asked to identify the criminal from a set of individuals. Similarly, a statistical lineup is a plot consisting of smaller panels; the viewer is asked to identify the panel containing the real data from among a set of decoy null plots. Null plots display data under the assumption there is no relationship and can be generated by permutation or simulation. A statistical lineup typically consists of 20 panels - one target panel and 19 null panels. If the viewer can identify the target panel randomly embedded within the set of null panels, this suggests that the real data is visually distinct from data generated under the null model. provides examples of statistical lineups. The lineup plot on the left displays increasing exponential data displayed on a linear scale with panel 13 as the target; the lineup plot on the right displays increasing exponential data on the log base ten scale with panel 4 as the target. Figure 2.1: Example Lineups While explicit graphical tests direct the participant to a specific feature of a plot to answer a specific question, implicit graphical tests require the user to identify both the purpose and function of the plot in order to evaluate the plots shown (Vanderplas et al., 2020). Implicit graphical tests, such as lineups, have the advantage of simultaneously visually testing for multiple visual features including outliers, clusters, linear and nonlinear relationships. Responses from multiple viewers are collected through crowd sourcing websites such as Prolific, Amazon Mechanical Turk, and Reddit. 2.3 Data Generation In this study, both the target and null data sets were generated by simulating data from an exponential model; the models differ in the parameters selected for the null and target panels. In order to guarantee the simulated data spans the same domain and range of values, I implemented a domain constraint of \\(x\\in [0,20]\\) and a range constraint of \\(y\\in [10,100]\\) with \\(N = 50\\) points randomly assigned throughout the domain and mapped to the \\(y\\)-axis using the exponential model with the selected parameters. These constraints provide some assurance that participants who select the target plot are doing so because of their visual perception differentiating between curvature or growth rate rather than different starting or ending values. Data were simulated based on a three-parameter exponential model with multiplicative errors: \\[\\begin{align} y_i &amp; = \\alpha\\cdot e^{\\beta\\cdot x_i + \\epsilon_i} + \\theta \\\\ \\text{with } \\epsilon_i &amp; \\sim N(0, \\sigma^2). \\nonumber \\end{align}\\] The parameters \\(\\alpha\\) and \\(\\theta\\) are adjusted based on \\(\\beta\\) and \\(\\sigma^2\\) to guarantee the range and domain constraints are met. The model generated \\(N = 50\\) points \\((x_i, y_i), i = 1,...,N\\) where \\(x\\) and \\(y\\) have an increasing exponential relationship. The heuristic data generation procedure is described in and . 2.4 Parameter Selection I followed a Goldilocks inspired procedure to choose three levels of curvature (low curvature, medium curvature, and high curvature). For each curvature level, I simulated 1000 data sets of \\((x_{ij}, y_{ij})\\) points for \\(i = 1,...,50\\) \\(x\\)-values and \\(j = 1...10\\) corresponding \\(y\\)-values per \\(x\\)-value. Each generated \\(x_i\\) point from was replicated ten times. On each of the individual data sets, I conducted a linear regression model and computed the lack of fit statistic (LOF) which measures the deviation of the data from the linear regression model. The density curves of the LOF statistics for each level of curvature are plotted to to provide a metric for differentiating between the curvature levels and thus detecting the target plot. While the LOF statistic provides a numerical value for discriminating between the difficulty levels, it cannot be directly related to the perceptual discriminability; it serves primarily as an approximation to ensure that we are testing parameters at several distinct curvature levels. Final parameters used for data simulation are shown in . Figure 2.2: Lack of fit statistic density curves 2.5 Lineup Setup Lineup plots were generated by mapping one simulated data set corresponding to curvature level A to a scatter plot to be identified as the target panel while multiple simulated data sets corresponding to curvature level B were individually mapped to scatter plots for the null panels. The nullabor package in R (Buja et al., 2009b) was used to randomly assign the target plot to one of the panels surrounded by panels containing null plots. For example, a target plot with simulated data following an increasing exponential curve with high curvature is randomly embedded within null plots with simulated data following an increasing exponential trend with low curvature. By the implemented constraints, the target panel and null panels will span a similar domain and range. There are a total of six lineup curvature combinations; illustrates the six lineup curvature combinations (top: linear scale; bottom: log scale) where the green line indicates the curvature level designated to the target plot while the black line indicates the curvature level assigned to the null plots. Two sets of each lineup curvature combination were simulated (total of 12 test data sets) and plotted on both the linear scale and the log scale (total of 24 test lineup plots). In addition, there are three curvature combinations which generate homogeneous Rorschach lineups, where all panels are from the same distribution. Each participant evaluated one of these lineups, but for simplicity, these evaluations are not described in this chapter. Figure 2.3: Lineup curvature combinations 2.6 Study Design Each participant was shown a total of thirteen lineup plots (twelve test lineup plots and one Rorschach lineup plot). Participants were randomly assigned one of the two replicate data sets for each of the six unique lineup curvature combinations. For each assigned test data set, the participant was shown the lineup plot corresponding to both the linear scale and the log scale. For the additional Rorschach lineup plot, participants were randomly assigned one data set shown on either the linear or the log scale. The order of the thirteen lineup plots shown was randomized for each participant. Participants above the age of majority in their region were recruited from Prolific, a survey site that connects researchers to study participants. Participants were compensated for their time and participated in all three related graphical studies consecutively. The lineup study in this chapter was completed first in the series of graphical studies. Previous literature suggests that prior mathematical knowledge or experience with exponential data is not associated with the outcome of graphical experiments (VanderPlas &amp; Hofmann, 2015). Participants completed the series of graphical tests using a R Shiny application found here. Participants were shown a series of lineup plots and asked to identify the plot that was most different from the others. On each plot, participants were asked to justify their choice and provide their level of confidence in their choice. The goal of this graphical task is to test an individuals ability to perceptually differentiate exponentially increasing trends with differing levels of curvature on both the linear and log scale. 2.7 Results Participant recruitment and study deployment was conducted via Prolific, a crowd sourcing website, on Wednesday, March 23, 2022 during which 325 individuals completed 4492 unique test lineup evaluations. Only participants who completed the lineup study were included in the final data set which included a total of 311 participants and 3958 lineup evaluations. Each plot was evaluated between 141 and 203 times (Mean: 164.92, SD: 14.9). Participants correctly identified the target panel in 47% of the 1981 lineup evaluations made on the linear scale and 65.3% of the 1977 lineup evaluations made on the log scale. Target plot identification was analyzed using the glmer function in the lme4 R package (Bates, Mächler, Bolker, &amp; Walker, 2015). Estimates and odds ratio comparisons between the log and linear scales were calculated using the emmeans R package (Lenth, 2021). Each lineup plot evaluated was assigned a binary value based on the participant response (correct = 1, not correct = 0). Define \\(Y_{ijkl}\\) to be the event that participant \\(l = 1,...,N_{participant}\\) correctly identifies the target plot for data set \\(k = 1,2\\) with curvature combination \\(j = 1,2,3,4,5,6\\) plotted on scale \\(i = 1,2\\). The binary response was analyzed using a generalized linear mixed model following a binomial distribution with a logit link function with a row-column blocking design accounting for the variation due to participant and data set respectively as \\[\\begin{equation} \\text{logit }P(Y_{ijk}) = \\eta + \\delta_i + \\gamma_j + \\delta \\gamma_{ij} + s_l + d_k \\end{equation}\\] where We assume that random effects for data set and participant are independent. Results indicate a strong interaction between the curvature combination and scale (\\(\\chi^2_5 = 294.443\\); \\(\\text{p} &lt;0.0001\\)). Variance due to participant and data set were estimated to be \\(\\hat\\sigma^2_{\\text{participant}} = 1.19\\) (s.e. 1.09) and \\(\\hat\\sigma^2_{\\text{data}} = 0.433\\) (s.e. 0.66) respectively. On both the log and linear scales, the highest accuracy occurred in lineup plots where the target model and null model had a large curvature difference and the target plot had more curvature than the null plots (high curvature target plot embedded in low curvature null plots). There is a decrease in accuracy on the linear scale when comparing a target plot with less curvature to null plots with more curvature (medium curvature target plot embedded in high curvature null plots; low curvature target plot embedded in medium curvature null plots; low curvature target plot embedded in high curvature null plots). Best, Smith, &amp; Stubbs (2007) found that accuracy of identifying the correct curve type was higher when nonlinear trends were presented indicating that it is hard to say something is linear (something has less curvature), but easy to say that it is not linear; our results concur with this observation. displays the estimated (log) odds ratio of successfully identifying the target panel on the log scale compared to the linear scale. The thumbnail figures to the right of the plot illustrate the curvature combination on both the linear (left thumbnail) and log base ten (right thumbnail) scales associated with the \\(y\\)-axis label. The choice of scale has no impact if curvature differences are large and the target plot had more curvature than the null plots (high curvature target plot embedded in low curvature null plots). However, presenting data on the log scale makes us more sensitive to slight changes in curvature (low or high curvature target plot embedded in medium curvature null plots; medium curvature target plot embedded in high curvature null plots) and large differences in curvature when the target plot has less curvature than the null plots (low curvature target plot embedded in high curvature null plots). An exception occurs when identifying a plot with curvature embedded in null plots close to a linear trend (medium curvature target panel embedded in low curvature null panels). The results indicate that participants were more accurate at detecting the target panel on the linear scale than the log scale. When examining this curvature combination, the same perceptual effect occurs as what we previously saw, but in a different context of scales. On the linear scale, participants are perceptually identifying a curved trend from close to a linear trend whereas after the logarithmic transformation, participants are perceptually identifying a trend close to linear from a curved trend. This again supports the claim that it is easy to identify a curve in a bunch of lines but much harder to identify a line in a bunch of curves (Best et al., 2007). Figure 2.4: Lineups log(odds) results 2.8 Discussion and Conclusion The overall goal of this chapter is to provide basic research to support the principles used to guide design decisions in scientific visualizations of exponential data. In this study, I explored the use of linear and log scales to determine whether our ability to notice differences in exponentially increasing trends is impacted by the choice of scale. The results indicated that when there was a large difference in curvature between the target plot and null plots and the target plot had more curvature than the null plots, the choice of scale had no impact and participants accurately differentiated between the two curves on both the linear and log scale. However, displaying exponentially increasing data on a log scale improved the accuracy of differentiating between models with slight curvature differences or large curvature differences when the target plot had less curvature than the null plots. An exception occurred when identifying a plot with curvature embedded in surrounding plots closely relating to a linear trend, indicating that it is easy to identify a curve in a group of lines but much harder to identify a line in a group of curves. The use of visual inference to identify these guidelines suggests that there are advantages to log scales when differences are subtle. What remains to be seen is whether there are cognitive disadvantages to log scales: do log scales make it harder to make use of graphical information? "],["3-youdrawit.html", "CHAPTER 3 Prediction with you draw it 3.1 Introduction 3.2 Study Design 3.3 Eye Fitting Straight Lines in the Modern Era 3.4 Prediction of Exponential Trends 3.5 Discussion and Conclusion", " CHAPTER 3 Prediction with you draw it 3.1 Introduction In Chapter 2, a base foundation for future exploration of the use of log scales was established by evaluating participants ability to identify differences in charts through the use of lineups. This did not require that participants were able to understand exponential growth, identify log scales, or have any mathematical training; instead, it simply tested the change in perceptual sensitivity resulting from visualization choices. In order to determine whether there are cognitive disadvantages to log scales, I utilize interactive graphics to test an individuals ability to make predictions for exponentially increasing data. In this study, participants are asked to draw a line using their computer mouse through the exponentially increasing trend shown on both the log and linear scales. 3.1.1 Past Methodology Initial studies in the 20th century explored the use of fitting lines by eye through a set of points (D. J. Finney, 1951; Mosteller, Siegel, Trapido, &amp; Youtz, 1981). Common methods of fitting trends by eye involve maneuvering a string, black thread, or ruler until the fit is suitable, then drawing the line through the set of points. In D. J. Finney (1951), it was of interest to determine the effect of stopping iterative maximum likelihood calculations after one iteration. Many techniques in statistical analysis are performed with the aid of iterative calculations such as Newtons method or Fishers scoring. Guesses are made at the best estimates of certain parameters and these guesses are then used as the basis of a computation which yields a new set of approximation to the parameter estimates; this same procedure is the performed on the new parameter estimates and the computing cycle is repeated until convergence, as determined by the statistician, is reached. The author was interested in whether one iteration of calculations was sufficient in the estimation of parameters connected with dose-response relationships. One measure of interest is the relative potency between a test preparation of doses and standard preparation of does; relative potency is calculated as the ratio of two equally effective doses between the two preparation methods. shows a pair of parallel probit responses in a biological assay. The x-axis is the \\(\\log_{1.5}\\) dose level for four dose levels (for example, doses 4, 6, 9, and 13 correspond correspond to equally spaced values on a logarithmic scale, labeled 0, 1, 2, and 3) and the y-axis is the corresponding probit response as calculated in David John Finney &amp; Stevens (1948); circles correspond to the test preparation method while the crosses correspond to the standard preparation method. For these sort of assays, the does-response relationship follows a linear regression of the probit response on the logarithm of the dose levels; the two preparation methods can be constrained to be parallel (Jerne &amp; Wood, 1949), limiting the relative potency to one consistent value. In this study, twenty-one scientists were recruited via postal mail and asked to rule two lines in order to judge by eye the positions for a pair of parallel probit regression lines in a biological assay . The author then computed one iterative calculation of the relative potency based on starting values as indicated by the pair of lines provided by each participant and compared these relative potency estimates to that which was estimated by the full probit technique (reaching convergence through multiple iterations). Results indicated that one cycle of iterations for calculating the relative potency was sufficient based on the starting values provided by eye from the participants. Figure 3.1: Subjective Judgement in Statistical Analysis (1951) Parallel Probits Thirty years later, Mosteller et al. (1981), sought to understand the properties of least squares and other computed lines by establishing one systematic method of fitting lines by eye. The authors recruited 153 graduate students and post doctoral researchers in Introductory Biostatistics. Participants were asked to fit lines by eye to four sets of points using an 8.5 x 11 inch transparency with a straight line etched completely across the middle. A latin square design (Giesbrecht &amp; Gumpertz, 2004) with packets of the set of points stapled together in four different sequences was used to determine if there is an effect of order of presentation. It was found that order of presentation had no effect and that participants tended to fit the slope of the first principal component (error minimized orthogonally, both horizontal and vertical, to the regression line) over the slope of the least squares regression line (error minimized vertically to the regression line). Figure 3.2: Eye Fitting Straight Lines (1981) Data Sets In 2015, the New York Times introduced an interactive feature, called You Draw It (Aisch, Cox, &amp; Quealy, 2015; Buchanan, Park, &amp; Pearce, 2017; Katz, 2017). Readers are asked to input their own assumptions about various metrics and compare how these assumptions relate to reality. The New York Times team utilizes Data Driven Documents (D3) that allows readers to predict these metrics through the use of drawing a line on their computer screen with their computer mouse. (Katz, 2017) is one such example in which readers are asked to draw the line for the missing years providing what they estimate to be the number of Americans who have died every year from car accidents, since 1990. After the reader has completed drawing the line, the actual observed values are revealed and the reader may check their estimated knowledge against the actual reported data. Figure 3.3: New York Times You Draw It Feature 3.1.2 Data Driven Documents Major news and research organizations such as the New York Times, FiveThirtyEight, Washington Post, and the Pew Research Center create and customize graphics with Data Driven Documents (D3). In June 2020, the New York Times released a front page displaying figures that represent each of the 100,000 lives lost from the COVID-19 pandemic until this point in time (Barry et al., 2020); this visualization was meant to bring about a visceral reaction and resonate with readers. During 2021 March Madness, FiveThirtyEight created a roster-shuffling machine which allowed readers to build their own NBA contender through interactivity (Ryanabest, 2021). Data Driven Documents (D3) is an open-source JavaScript based graphing framework created by Mike Bostock during his time working on graphics at the New York Times. For readers familiar with R, it is notable to consider D3 in JavaScript equivalent to the ggplot2 package in R (Hadley Wickham, 2016). Similar to geometric objects and style choices in ggplot2, the grammar of D3 also includes elements such as circles, paths, and rectangles with choices of attributes and styles such as color and size. Data Driven Documents depend on Extensible Markup Language (XML) to generate graphics and images by binding objects and layers to the plotting area as Scalable Vector Graphics (SVG) in order to preserve the shapes rather than the pixels (Tol, 2021). Advantages of using D3 include animation and allowing for movement and user interaction such as hovering, clicking, and brushing. Figure 3.4: SVG vs Raster A challenge of working with D3 is the environment necessary to display the graphics and images. The r2d3 package in R provides an efficient integration of D3 visuals and R by displaying them in familiar HTML output formats such as RMarkdown or Shiny applications (Luraschi &amp; Allaire, 2018). The creator of the graphic applies D3.js source code to visualize data which has previously been processed within an R setting. The example R code illustrates the structure of the r2d3 function which includes specification of a data frame in R (converted to a JSON file), the D3.js source code file, and the D3 version that accompanies the source code. A default SVG container for layering elements is then generated by the r2d3 function which renders the plot using the source code. Appendix A outlines the development of the you draw interactive plots used in this study through the use of r2d3 and R shiny applications. provides an example of a you draw it interactive plot as seen by participants during the study. The first frame shows what the participant sees along with the prompt, Use your mouse to fill in the trend in the yellow box region. Next, the yellow box region moves along as the participant draws their trend-line until the yellow region disappears, indicating the participant has filled in the entire domain. r2d3(data = data, script = &quot;d3-source-code.js&quot;, d3_version = &quot;5&quot;) Figure 3.5: You Draw It Example 3.2 Study Design This chapter contains two sub-studies; the first aims to establish you draw it as a tool for measuring predictions of trends fitted by eye and a method for testing graphics, the second then applies you draw it to test an individuals ability to make predictions for exponentially increasing data. In order to validate you draw it as a method for testing graphics, the first sub-study, referred to as Eye Fitting Straight Lines in the Modern Era, replicated the experiment and results found in Mosteller et al. (1981). The second sub-study, referred to as Prediction of Exponential Trends, uses the established you draw it method to test an individuals ability to make predictions for exponentially increasing data on both the log and linear scales. All subjects participated in both sub-studies at the same time in conjunction with one another. A total of six data sets - four Eye Fitting Straight Lines in the Modern Era and two Prediction of Exponential Trends - are generated for each individual at the start of the experiment. The two simulated data sets corresponding to the simulated data models used in the Prediction of Exponential Trends sub-study are then plotted a total of four times each with different aesthetic and scale choices for a total of eight task plots. Participants in the study are first shown two you draw it practice plots followed by twelve you draw it task plots. The order of all twelve task plots was randomly assigned for each individual in a completely randomized design where users saw the four task plots from the Eye Fitting Straight Lines in the Modern Era sub-study interspersed with the eight task plots from the Prediction of Exponential Trends sub-study. Participants completed the experiment using a RShiny application found here. During May 2021, participants were recruited through Twitter, Reddit, and direct email. A total of 39 individuals completed 256 unique you draw it task plots; all completed you draw it task plots were included in the analysis. 3.3 Eye Fitting Straight Lines in the Modern Era D. J. Finney (1951) and Mosteller et al. (1981) use methods such as using a ruler, string, or transparency sheet to fit straight lines through a set of points. This section replicates the study found in Mosteller et al. (1981) in order to establish you draw it as a tool and method for testing graphics. 3.3.1 Data Generation All data processing was conducted in R before being passed to the D3.js source code. A total of \\(N = 30\\) points \\((x_i, y_i), i = 1,...N\\) were generated for \\(x_i \\in [x_{min}, x_{max}]\\) where \\(x\\) and \\(y\\) have a linear relationship. Data were simulated based on linear model with additive errors: \\[\\begin{align} y_i &amp; = \\beta_0 + \\beta_1 x_i + e_i \\\\ \\text{with } e_i &amp; \\sim N(0, \\sigma^2). \\nonumber \\end{align}\\] The parameters \\(\\beta_0\\) and \\(\\beta_1\\) are selected to replicate Mosteller et al. (1981) with \\(e_i\\) generated by rejection sampling in order to guarantee the points shown align with that of the fitted line. An ordinary least squares regression is then fit to the simulated points in order to obtain the best fit line and fitted values in 0.25 increments across the domain, \\((x_k, \\hat y_{k,OLS}), k = 1, ..., 4 x_{max} +1\\). The data simulation function then outputs a list of point data and line data both indicating the parameter identification, x-value, and corresponding simulated or fitted y value. The data simulation procedure is described in . Simulated model equation parameters were selected to reflect the four data sets (F, N, S, and V) used in Mosteller et al. (1981) . Parameter choices F, N, and S simulated data across a domain of 0 to 20. Parameter choice F produces a trend with a positive slope and a large variance while N has a negative slope and a large variance. In comparison, S shows a trend with a positive slope with a small variance and V yields a steep positive slope with a small variance over the domain of 4 to 16. illustrates an example of simulated data for all four parameter choices intended to reflect the trends seen in . Aesthetic design choices were made consistent across each of the interactive you draw it plots; the aspect ratio, defining the \\(x\\) to \\(y\\) axis ratio was set to one and the y-axis range extended 10% beyond (above and below) the range of the simulated data points to allow for users to draw outside the simulated data set range. Figure 3.6: Eye Fitting Straight Lines in the Modern Era Simulated Data Example 3.3.2 Results In addition to the participant drawn points, \\((x_k, y_{k,drawn})\\), and the ordinary least squares (OLS) regression fitted values, \\((x_k, \\hat y_{k,OLS})\\), a regression equation with a slope based on the first principal component (PCA) was used to calculate fitted values, \\((x_k, \\hat y_{k,PCA})\\). For each set of simulated data and parameter choice, the PCA regression equation was determined by using the princomp function in the stats package in base R to obtain the rotation of the coordinate axes from the first principal component (direction which captures the most variance). The estimated slope, \\(\\hat\\beta_{1,PCA}\\), is determined by the ratio of the axis rotation in y and axis rotation in x of the first principal component with the y-intercept, \\(\\hat\\beta_{0,PCA}\\) calculated by the point-slope equation of a line using the mean of of the simulated points, \\((\\bar x_i, \\bar y_i)\\). Fitted values, \\(\\hat y_{k,PCA}\\) are then obtained every 0.25 increment across the domain from the PCA regression equation, \\(\\hat y_{k,PCA} = \\hat\\beta_{0,PCA} + \\hat\\beta_{1,PCA} x_k\\). illustrates the difference between an OLS regression equation which minimizes the vertical distance of points from the line and a regression equation with a slope calculated by the first principal component which minimizes the smallest distance of points from the line. Figure 3.7: OLS vs PCA Regression Lines For each participant, the final data set used for analysis contains \\(x_{ijk}, y_{ijk,drawn}, \\hat y_{ijk,OLS}\\), and \\(\\hat y_{ijk,PCA}\\) for parameter choice \\(i = 1,2,3,4\\), j = \\(1,...N_{participant}\\), and \\(x_{ijk}\\) value \\(k = 1, ...,4 x_{max} + 1\\). Using both a linear mixed model and a generalized additive mixed model, comparisons of vertical residuals in relation to the OLS fitted values (\\(e_{ijk,OLS} = y_{ijk,drawn} - \\hat y_{ijk,OLS}\\)) and PCA fitted values (\\(e_{ijk,PCA} = y_{ijk,drawn} - \\hat y_{ijk,PCA}\\)) were made across the domain. displays an example of all three fitted trend lines for parameter choice F. Figure 3.8: Eye Fitting Straight Lines in the Modern Era Example Using the lmer function in the lme4 package (Bates et al., 2015), a linear mixed model (LMM) is fit separately to the OLS and PCA residuals, constraining the fit to a linear trend. Parameter choice, \\(x\\), and the interaction between \\(x\\) and parameter choice were treated as fixed effects with a random participant effect accounting for variation due to participant. The LMM equation for each fit (OLS and PCA) residuals is given by: \\[\\begin{equation} y_{ijk,drawn} - \\hat y_{ijk,fit} = e_{ijk,fit} = \\left[\\gamma_0 + \\alpha_i\\right] + \\left[\\gamma_{1} x_{ijk} + \\gamma_{2i} x_{ijk}\\right] + p_{j} + \\epsilon_{ijk} \\end{equation}\\] where \\(y_{ijk,drawn}\\) is the drawn y-value for the \\(i^{th}\\) parameter choice, \\(j^{th}\\) participant, and \\(k^{th}\\) increment of x-value \\(\\hat y_{ijk,fit}\\) is the fitted y-value for the \\(i^{th}\\) parameter choice, \\(j^{th}\\) participant, and \\(k^{th}\\) increment of x-value corresponding to either the OLS or PCA fit \\(e_{ijk,fit}\\) is the residual between the drawn and fitted y-values for the \\(i^{th}\\) parameter choice, \\(j^{th}\\) participant, and \\(k^{th}\\) increment of x-value corresponding to either the OLS or PCA fit \\(\\gamma_0\\) is the overall intercept \\(\\alpha_i\\) is the effect of the \\(i^{th}\\) parameter choice (F, S, V, N) on the intercept \\(\\gamma_1\\) is the overall slope for \\(x\\) \\(\\gamma_{2i}\\) is the effect of the parameter choice on the slope \\(x_{ijk}\\) is the x-value for the \\(i^{th}\\) parameter choice, \\(j^{th}\\) participant, and \\(k^{th}\\) increment \\(p_{j} \\sim N(0, \\sigma^2_{participant})\\) is the random error due to the \\(j^{th}\\) participants characteristics \\(\\epsilon_{ijk} \\sim N(0, \\sigma^2)\\) is the residual error. Eliminating the linear trend constraint, the bam function in the mgcv package (S. N. Wood, 2003, 2004, 2011; S. N. Wood, 2017; S. N. Wood, N., Pya, &amp; Saefken, 2016) is used to fit a generalized additive mixed model (GAMM) separately to the OLS and PCA residuals to allow for estimation of smoothing splines. Parameter choice was treated as a fixed effect with no estimated intercept and a separate smoothing spline for \\(x\\) was estimated for each parameter choice. A random participant effect accounting for variation due to participant and a random spline for each participant accounted for variation in spline for each participant. The GAMM equation for each fit (OLS and PCA) residuals is given by: \\[\\begin{equation} y_{ijk, drawn} - \\hat y_{ijk, fit} = e_{ijk,fit} = \\alpha_i + s_{i}(x_{ijk}) + p_{j} + s_{j}(x_{ijk}) \\end{equation}\\] where \\(y_{ijk,drawn}\\) is the drawn y-value for the \\(i^{th}\\) parameter choice, \\(j^{th}\\) participant, and \\(k^{th}\\) increment of x-value \\(\\hat y_{ijk,fit}\\) is the fitted y-value for the \\(i^{th}\\) parameter choice, \\(j^{th}\\) participant, and \\(k^{th}\\) increment of x-value corresponding to either the OLS or PCA fit \\(e_{ijk,fit}\\) is the residual between the drawn and fitted y-values for the \\(i^{th}\\) parameter choice, \\(j^{th}\\) participant, and \\(k^{th}\\) increment of x-value corresponding to either the OLS or PCA fit \\(\\alpha_i\\) is the intercept for the parameter choice \\(i\\) \\(s_{i}\\) is the smoothing spline for the \\(i^{th}\\) parameter choice \\(x_{ijk}\\) is the x-value for the \\(i^{th}\\) parameter choice, \\(j^{th}\\) participant, and \\(k^{th}\\) increment \\(p_{j} \\sim N(0, \\sigma^2_{participant})\\) is the error due to participant variation \\(s_{j}\\) is the random smoothing spline for each participant. and show the estimated trends of residuals (vertical deviation of participant drawn points from both the OLS and PCA fitted points) as modeled by a LMM and GAMM respectively. Examining the plots, the estimated trends of PCA residuals (orange) appear to align closer to the \\(y=0\\) horizontal (dashed) line than the OLS residuals (blue). In particular, this trend is more prominent in parameter choices with large variances (F and N). These results are consistent to those found in Mosteller et al. (1981) indicating participants fit a trend line closer to the estimated regression line with the slope of the first principal component than the estimated OLS regression line. Figure 3.9: Eye Fitting Straight Lines in the Modern Era LMM results Figure 3.10: Eye Fitting Straight Lines in the Modern Era GAMM results In addition to fitting trend lines over the residuals between the participant drawn values and fitted values, an OLS sum of squares and PCA sums of squares measure was calculated for each you draw it plot. Sums of squares between the two fits were compared using the lmer function in the lme4 package in R (Bates et al., 2015) to run a linear mixed model (LMM) with a log transformation. Parameter choice (S, F, V, N), fit (OLS, PCA), and the interaction between parameter choice and fit were treated as fixed effects with a random participant effect. Define \\(SS_{ijk}\\) as the sums of squares for parameter choice \\(i = 1,2,3,4\\), fit \\(j=1,2\\), and participant \\(k = 1,...,N_{participant}\\). The LMM equation is given by: \\[\\begin{equation} \\log\\left(SS_{ijk}\\right) = \\alpha_i + \\beta_j + \\alpha\\beta_{ij} + p_{j} + \\epsilon_{ijk} \\end{equation}\\] \\(\\alpha_i\\) denotes the effect of the \\(i^{th}\\) parameter choice \\(\\beta_j\\) denotes the effect of the \\(j^{th}\\) fit \\(\\alpha\\beta_{ij}\\) denotes the interaction between the \\(i^{th}\\) parameter choice and \\(j^{th}\\) fit \\(p_{j} \\sim N(0, \\sigma^2_{participant})\\) is the random error due to the \\(k^{th}\\) participants characteristics \\(\\epsilon_{ijk} \\sim N(0, \\sigma^2)\\) is the residual error. The estimated odds ratios between the OLS fit and PCA fit for each parameter choice are displayed in . While there is no significant effect of fit for any parameter choices, there is indication of the trend found previously in the residual regression models indicating that participants tend to fit the slope of the first principal component over the slope of the ordinary least squares regression line, particularly for trends with large variation (parameter choices N and F). Figure 3.11: Eye Fitting Straight Lines in the Modern Era Sum of Squares Results 3.4 Prediction of Exponential Trends The results from the first sub-study validate you draw it as a tool for testing graphics. This sub-study is designed to test an individuals ability to make predictions for exponentially increasing data on both the log and linear scales, addressing cognitive understanding of log scales. Participants are asked to draw a line using their computer mouse through the exponentially increasing trend shown on both the log and linear scale. 3.4.1 Data Generation All data processing was conducted in R before being passed to the D3.js source code. A total of \\(N = 30\\) points \\((x_i, y_i), i = 1,...N\\) were generated for \\(x_i\\in [x_{min}, x_{max}]\\) where \\(x\\) and \\(y\\) have an exponential relationship. Data were simulated based on a one parameter exponential model with multiplicative errors: \\[\\begin{align} y_i &amp; = e^{\\beta x_i + e_i} \\\\ \\text{with } e_i &amp; \\sim N(0, \\sigma^2). \\nonumber \\end{align}\\] The parameter, \\(\\beta\\), was selected to reflect the rate of exponential growth with \\(e_i\\) generated by rejection sampling in order to guarantee the points shown align with that of the fitted line displayed in the initial plot frame. A nonlinear least squares regression is then fit to the simulated points in order to obtain the best fit line and fitted values in 0.25 increments across the domain, \\((x_m, \\hat y_{m,NLS}), k = 1, ..., 4 x_{max} +1\\). The data simulation function then outputs a list of point data and line data both indicating the parameter identification, x value, and corresponding simulated or fitted y value. The data simulation procedure is described in . Model equation parameter, \\(\\beta\\), was selected to reflect two exponential growth rates (low: \\(\\beta = 0.10, \\sigma = 0.09\\) and high: \\(\\beta = 0.23, \\sigma = 0.25\\)) as determined by visual inspection with growth rate parameter selection from the lineup study in Chapter 2 used as a starting point. Each growth rate parameter was used to simulate data across a domain of 0 to 20. The two simulated data sets (low and high exponential growth rates) were then shown four times each by truncating the points shown at both 50% and 75% of the domain as well as on both the log and linear scales for a total of eight interactive plots reflecting a factorial treatment design. Appendix B displays visual examples of all eight interactive plots. Aesthetic design choices were made consistent across each of the interactive you draw it plots; the aspect ratio, defining the \\(x\\) to \\(y\\) axis ratio was set to one, the y-axis extended 50% below the lower limit of the simulated data range and 200% beyond the upper limit of the simulated data range to allow for users to draw outside the data set range, and participants were asked to start drawing at 50% of the domain (for example, at \\(x = 10\\)). Reflecting the treatment design for each plot, the y-axis was assigned to be displayed on either the linear scale or log scale. 3.4.2 Results A LOESS smoother (local regression) was fit to each user line to allow for visual inspection. For each participant \\(l = 1,...N_{participant}\\), the final data set used for analysis contains \\(x_{ijklm}, y_{ijklm,drawn}, \\hat y_{ijklm,loess}\\), and \\(\\hat y_{ijklm,NLS}\\) for growth rate \\(i = 1,2\\), points truncated \\(j = 1,2\\), scale \\(k = 1,2\\) and \\(x_{ijklm}\\) value for increment \\(m = 1, ...,81\\). displays spaghetti plots for each of the eight treatment combinations. The spaghetti plot with a high growth rate suggests participants underestimated the exponential trend when asked to draw a trend line on the linear scale compared to when asked to draw a trend line on the log scale. In particular, this suggestion is most noticeable when points are truncated at 50% with the underestimation beginning at a later \\(x\\) value when points are truncated at 75%. Figure 3.12: Exponential Prediction Spaghetti Plot Allowing for flexibility, the bam function in the mgcv package (S. N. Wood, 2003, 2004, 2011; S. N. Wood, 2017; S. N. Wood et al., 2016) is used to fit a GAMM to estimate trends of vertical residuals from the participant drawn line in relation to the NLS fitted values (\\(e_{ijklm,NLS} = y_{ijklm,drawn} - \\hat y_{ijklm,NLS}\\)) across the domain. The combination between growth rate, point truncation, and scale was treated as a fixed effect with no estimated intercept and a separate smoothing spline for \\(x\\) was estimated for each treatment combination. A random participant effect accounting for variation due to participant and a random spline for each participant accounted for variation in spline for each participant. The GAMM equation for residuals is given by: \\[\\begin{equation} y_{ijklm,drawn} - \\hat y_{ijklm,NLS} = e_{ijklm,nls} = \\tau_{ijk} + s_{ijk}(x_{ijklm}) + p_{l} + s_{l}(x_{ijklm}) \\end{equation}\\] where \\(y_{ijklm,drawn}\\) is the drawn y-value for the \\(l^{th}\\) participant, \\(m^{th}\\) increment, and \\(ijk^{th}\\) treatment combination \\(\\hat y_{ijklm,NLS}\\) is the NLS fitted y-value for the \\(l^{th}\\) participant, \\(m^{th}\\) increment, and \\(ijk^{th}\\) treatment combination \\(e_{ijklm,NLS}\\) is the residual between the drawn y-value and fitted y-value for the \\(l^{th}\\) participant, \\(m^{th}\\) increment, and \\(ijk^{th}\\) treatment combination \\(\\tau_{ijk}\\) is the intercept for the \\(i^{th}\\) growth rate, \\(j^{th}\\) point truncation, and \\(k^{th}\\) scale treatment combination \\(s_{ijk}\\) is the smoothing spline for the \\(ijk^{th}\\) treatment combination \\(x_{ijklm}\\) is the x-value for the \\(l^{th}\\) participant, \\(m^{th}\\) increment, and \\(ijk^{th}\\) treatment combination \\(p_{l} \\sim N(0, \\sigma^2_{participant})\\) is the error due to the \\(l^{th}\\) participants characteristics \\(s_{l}\\) is the random smoothing spline for the \\(l^{th}\\) participant. shows the estimated trends of the residuals (vertical deviation of participant drawn points from NLS fitted points) as modeled by the GAMM. Examining the plots, the estimated trends of residuals for predictions made on the linear scale (blue) appear to deviate from the \\(y=0\\) horizontal (dashed) line indicating underestimation of exponential growth. In comparisons, the estimated trends of residuals for predictions made on the log scale (orange) follow closely to the \\(y=0\\) horizontal (dashed) line, implying exponential trends predicted on the log scale are more accurate than those predicted on the linear scale. In particular, this trend is more prominent in high exponential growth rates where underestimation becomes prominent after the aid of points is removed. Figure 3.13: Exponential Prediction GAMM Results 3.5 Discussion and Conclusion The intent of this chapter was to establish you draw it as a tool for testing graphics then use this tool to determine the cognitive implications of displaying data on the log scale. Eye Fitting Straight Lines in the Modern Era replicated the results found in Mosteller et al. (1981). When shown points following a linear trend, participants tended to fit the slope of the first principal component over the slope of the least squares regression line. This trend was most prominent when shown data simulated with larger variances. The reproducibility of these results serve as evidence of the reliability of the you draw it method. In Prediction of Exponential Trends, the you draw it method was used to test an individuals ability to make predictions for exponentially increasing data. Results indicate that underestimation of exponential growth occurs when participants were asked to draw trend lines on the linear scale and that there was an improvement in accuracy when trends were drawn on the log scale. This phenomena is strongly supported for high exponential growth rates. Improvement in predictions are made when points along the exponential trend are shown as indicated by the discrepancy in results for treatments with points truncated at 50% compared to 75% of the domain. The results of this study suggest that there are cognitive advantages to log scales when making predictions of exponential trends. Improvement in predictions were made for trends with high exponential growth rates when participants were asked to draw a trend line on the log scale compared to the linear scale. Further investigation is necessary to determine the implications of using log scales when translating exponential graphs to numerical values. "],["4-estimation.html", "CHAPTER 4 Numerical Translation and Estimation 4.1 Introduction 4.2 Study Design 4.3 Data Generation 4.4 Results 4.5 Discussion and Conclusion", " CHAPTER 4 Numerical Translation and Estimation 4.1 Introduction The previous two chapters explored the use of log scales through differentiation and visual prediction of trends. These graphical tasks were conducted independent of scenarios or contextual applications of logarithmic scales; instead, they focused how our visual system perceives and identifies patterns in exponential growth. In order to understand the cognitive implications of displaying exponentially increasing data on a log scale, it is essential to evaluate graph comprehension as it relates to the contextual scenario of the data shown. This is a complex inferential process which requires participants to engage with the data by quantitatively transforming information in the chart (Cleveland &amp; McGill, 1984, 1985). In this study, I ask participants to translate a graph of exponentially increasing data into real value quantities and extend their estimations by comparing two data points. 4.1.1 Graph Comprehension Graph comprehension is heavily dependent on the questions being asked of the viewer; therefore, questioning is an important aspect of comprehension and must be given deliberate consideration (Graesser, Swamer, Baggett, &amp; Sell, 2014). Evaluation of how viewers explore a new and complex graphic requires long-term interaction with the chart displaying the data (Becker, Moore, &amp; Lawrence, 2019). While it is difficult to obtain an accurate representation of a viewers understanding of the graphic with a fixed set of numerical estimates, three levels of graph comprehension have emerged from literature (Curcio, 1987; Friel, Curcio, &amp; Bright, 2001; Glazer, 2011; Jolliffe, 1991; R. Wood, 1968). The three behaviors related to graph comprehension involve (1) literal reading of the data (elementary level), (2) reading between the data (intermediate level), and (3) reading beyond the data (advanced level). 4.1.2 Rounding Estimates &amp; Anchoring Open-ended estimation tasks elicit certain well-known biases such as the tendency to round to multiples of 5 or 10 (Becker et al., 2019) 4.2 Study Design Participants in this study were asked to answer six questions related to each of two contextual scenarios and an associated scatter plot shown for a total of twelve questions. The text for each scenario is presented below; the context of both scenarios was selected to be similar. Each text describes a situation in which a fictional intergalactic species is exponentially increasing in population over a time measure adjusted to reflect the popular culture media depiction of that species (Marquand, 1983; Star trek, 1967; Star wars, 1977). For simplicity, I will refer to this fictional time component as a year throughout. Fictional illustrations of the figures used in context were modified from artwork by Allison Horst and included on the main page for each scenario. The scale of the graphic and data set displayed was randomly assigned to scenarios for each individual. For instance, a participant may have seen a scatter plot of data set two displayed on the linear scale paired with the Ewok scenario text and a scatter plot of data set one displayed on the log scale paired with the Tribble scenario text. The order of the two scenarios and their assigned data set and scale was randomly assigned to each individual. I selected the six questions for graph comprehension based on the three defined levels of questioning. In each scenario, participants were first asked an open ended question, which required them to spend time exploring the data displayed in the graphic, followed by a random order of two elementary level questions and three intermediate level questions. I did not focus on advanced level questioning since extrapolation and interpolation was addressed in Chapter 2. The estimation study in this chapter was completed last in the series of the three graphical studies and took about fifteen minutes for participants to answer all twelve estimation questions. Participants completed the series of graphical tests using a R Shiny application found here. For each of the quantitative translation questions, participants were provided a basic calculator and scratchpad to aid in their estimation of values. I recorded the inputted and evaluated calculations and scratch work of each participant in order to understand participant strategies for estimation. 4.3 Data Generation I generated two unique data sets with the same underlying parameter coefficients, but different errors randomly generated from the same error distribution. For each data set, a total of \\(N = 50\\) points \\((x_i, y_i), i = 1,...N\\) were generated for single increments of \\(x_i\\in [0, 50]\\) where \\(x\\) and \\(y\\) have an exponential relationship. Data were simulated based on a three parameter exponential model with multiplicative errors: \\[\\begin{align} y_i &amp; = \\alpha e^{\\beta x_i + e_i} + \\theta \\\\ \\text{with } e_i &amp; \\sim N(0, \\sigma^2). \\nonumber \\end{align}\\] The underlying parameter coefficients were selected to follow a similar growth rate and shape as the previous two studies by visual inspection while ensuring in a maximum magnitude of around 50,000. The resulting parameters selected for data generation were \\(\\alpha = 130\\), \\(\\beta = 0.12\\), \\(\\theta = 50\\), and \\(\\sigma = 1.5\\). Figure 4.1: Estimation simulated data display scatter plots of the two unique data sets on both the linear and logarithmic base two scales; a logarithm of base two was selected in order to aid in participants estimation of time until the population doubled in Intermediate Q3 . Participants were shown the graphic of both data sets on either the linear or logarithmic base two scale with labels adjusted to reflect the associated scenario context and scale. Grid lines for the \\(y\\)-axis were set to be consistent for the same scale across both data sets with the linear scale increasing by 5,000 and the logarithmic base two scale doubling, thus demonstrating the additive and multiplicative contextual appearance and interpretation of each scale respectively. Minor \\(y\\)-axis grid lines were removed to avoid participants anchoring to the midway point between grid lines; this is particularly important on the logarithmic scale since a halfway grid line spatially does not correspond to a halfway point numerically. Grid lines for the \\(x\\)-axis spanned a range of 50 with major grid lines every ten units in time apart and minor grid lines indicating every five units in time. The time unit labels on the \\(x\\)-axis reflected 0 to 50 ABY (After Battle of Yavin) for the Ewok scenario and were adjusted to 4500 to 4550 Stardates for the Tribble Scenario to align with the associated popular media depiction of each figure as well as disguise the use of the same underlying data simulation model and estimation questions across both scenarios. 4.4 Results Participant recruitment and study deployment was conducted via Prolific, a crowd sourcing website, on Wednesday, March 23, 2022 during which 302 individuals each completed all six estimation questions for each scenario (total of twelve questions per individual). The data set used for analysis contained the unique participant identification and indicated the scenario, scale, data set, and estimation question along with the participant text response or quantitative estimate, calculation input and evaluation, and associated scratch work. A total of 145 participants answered questions related to data set one on the linear scale and data set two on the logarithmic base two scale with 157 participants answering questions related to data set one on the logarithmic scale and data set two on the linear scale. Sketches for each question are used to demonstrate the estimation tasks participants are asked to conduct. An array of graphical displays allow for visual inspection of participant responses and provides suggestions about the implications of displaying exponentially increasing data on the logarithmic scale. 4.4.1 Open Ended Before participants were asked to estimate numeric quantities, they were asked to provide an open ended response and describe how the population changes over time. This required participants to spend time exploring the graphic and reflect upon how the data displayed relates to the contextual application. The tidytext and corpus packages in R (Perry, 2021; Silge &amp; Robinson, 2016) were used to extract and stem words from participant text responses; stop words such as the and is as well as numbers were removed from the cleaned word responses. The wordcloud package (Fellows, 2018) was used to create a cloud comparing frequencies of words across the two scales . The comparison word cloud is generated by defining \\(p_{i,j}\\) as the rate in which word \\(i\\) occurs when describing the data on scale \\(j\\) where \\(p_j\\) is the average rate across the scales \\(\\sum_i{\\frac{p_{i,j}}{\\text{N scales}}}\\). The maximum deviation for each word is calculated by \\(max_i(p_{i,j} - p_j)\\) and mapped to the size of the word with the position of the word determined by the scale in which the maximum occurs. Figure 4.2: Estimation word cloud The comparison word cloud illustrates the general terminology participants used when describing the scatter plots shown on each scale. Participants more frequently referred to terms such as exponential and rapid when shown the scatter plot on the linear scale while double and quadruple were often used to describe the graphic when shown on the logarithmic scale; indicating participants read the \\(y\\)-axis labels and noticed the doubling grid lines. The use of the term linear when participants are describing the appearance of the data displayed on the logarithmic scale suggests that a portion of participants did not recognize the data was exponentially increasing rather than linearly increasing due to the change in the visual appearance of the data between the two scales. 4.4.2 Elementary Q1: Estimation of population In order to examine the effect of scale on literal reading of the data, participants were asked, . The true estimated population in year 10 based on the underlying parameter estimates is 481.61 with simulated points of 445.48 and 466.9 for data sets one and two respectively. The median participant estimate across both scales and data sets was 500 with innerquartile ranges of 500 and 400 for data set one and data set two respectively when displayed on the linear scale and 48 and 12 for data set one and data set two respectively when displayed on the logarithmic scale. Figure 4.3: Elementary Q1 Sketch Density plots are used to illustrate the distribution of the quantitative estimates provided by participants. reveals a larger variance in quantitative population estimates made on the linear scale compared to the logarithmic scale. There is strong support that participants were anchoring to grid lines and base ten values as highlighted by the high density of estimates at 512 and 500 on the log scale as well as local maximums near multiples of ten such as 500 and 1000. Figure 4.4: Elementary Q1 Density in year 10 During the study, participants were explicitly asked to estimate the population during year 10; this value corresponds to a low magnitude where the population is condensed in a small region on the linear scale as opposed to later in time when larger magnitudes in population can be seen. While the results provided support for less variability in the estimated population in year 10 on the logarithmic scale, it is important to evaluate the accuracy of estimates along the domain. In two estimation questions related to intermediate level reading between the data, participants are asked to provide an increase and change in population between years 20 and 40, thus requiring participants to make first level estimates at these locations (). In order to understand the effect of the location along the domain, I extracted first level estimates for years 20 and 40 from participant calculations and scratch-work. I first compared population estimates from the explicitly asked year 10 location between participants who used the calculator and scratchpad in two or more of the questions to determine whether the there were differences or biases between those who used resources for estimation compared to those who did not utilize the resources Appendix 3b. About half of the participants fell into the category which provided scratch work and half did not. The true population from the underlying parameters in year 20 is 1483.01 with closest simulated point values of 1529.19 and 1288.9 for data sets one and two respectively; this location still results in a relatively low magnitude of population, but is closer to the crux of the exponential curve. In year 40, the true population from the underlying parameters in year 40 is 15846.35 with closest simulated point values of 17046.94 and 24186.34 for data sets one and two respectively. It is important to note the discrepancy in simulated point values in year 40 between the two data sets as a result to a multiplicative error causing a larger variance in simulated points for later years and larger magnitudes. Figure 4.5: Estimated Population: Data set 1 Figure 4.6: Estimated Population: Data set 2 Population estimates for year 10 from participants who used the scratchpad and first level estimates for years 20 and 40 are shown with spaghetti plots in and displayed on both the linear and logarithmic base two scale. The scale in which the estimate was made is indicated blue for linear and orange for log with the segments mapped from the participant estimated population to the true year based on the underlying data equation. Previously noted, the simulated point corresponding to year 40 in data set 2 has a large deviation from the true underlying data equation; highlights that some participants were reading the data points as opposed to first detecting the underlying trend and making estimates based on the identified trend. This provides argument that estimates are highly subjective to the particular data set. As the domain increases, we observe an increased accuracy in estimates made on the linear scale while estimates made on the logarithmic scale suffer in accruacy due to strong anchoring to grid lines and the larger quantitative difference between grid lines as magnitudes increase. For instance, on the logarithmic scale, there was a tendency to overestimate the population for year 20 from data set one, underestimate the population for year 20 from data set two, and overestimate the population for year 40 from data set two. Inaccurate first level estimations can lead to consequences in estimations which require participants to make comparisons between two points (Intermediate Q1 and Q2). In extracting participant first level estimates from their calculation and scratch work, I observed participants were resistant to estimating between grid lines and had a greater tendency to anchor their estimates to the grid line estimates on the logarithmic scale. illustrates the number of participants who provided that estimate on either the linear or logarithmic base two scale. True values based on underlying estimates, closest simulated point values, and grid line breaks are indicated by the horizontal line types. In particular, for year 40 in data set 1, the closest point (17046.94) falls close to the logarithmic grid line (16384); participants greatly anchored to the grid line of 16384 with some participants adjusting to 16500 or 17000, anchoring again to a base ten value. In a similar situation, for year 40 in data set 2, the closest point (24186.34) falls close to the linear grid line (25000); more participants adjusted their estimates to 24500 or 24000 rather than anchoring to the grid line. This suggests that participants were more likely to provide estimates which deviated from grid lines when making estimates on the linear scale, indicating they are more comfortable with interpreting values on a linear scale as opposed to the logarithmic scale. When participants made estimates between grid lines on the logarithmic scale as indicated by their scratch work, they tended to estimate half-way between the two values indicated by the grid line breaks. For example, 1536 was a common population estimate for year 20 because visually the location of estimation lands about halfway between grid lines 1024 and 2048 (). Participant calculations and scratch-work provides support that participants equated this as half-way numerically as indicated by the selected work provided below: \\[\\begin{align} \\textit{Sample work 1} \\nonumber\\\\ 2048-1024 &amp;= 1024 \\nonumber \\\\ 1024/2 &amp;= 512 \\nonumber\\\\ 512+1024 &amp;= 1536 \\nonumber \\nonumber \\\\ \\nonumber \\\\ \\textit{Sample work 2} \\nonumber\\\\ 2048 + 1024 &amp; =3072 \\nonumber\\\\ 3072/2 &amp; =1536 \\nonumber \\nonumber \\\\ \\nonumber \\\\ \\textit{Sample work 3} \\nonumber\\\\ 32768-16384&amp;=16384 \\nonumber\\\\ 32768-16384&amp;=16384 \\nonumber\\\\ 16384*2&amp;=32768 \\nonumber\\\\ 16384/2&amp;=8192 \\nonumber\\\\ 8192+16384&amp;=24576. \\nonumber \\end{align}\\] In particular, Sample work 3 demonstrates the participant processing the logarithmic base two mapping as they repeatedly calculate the distance between two grid lines by subtraction and multiplication; they however then go on to estimate halfway between the two grid lines by equating spatial and quantitative distances. This indicates a lack of understanding of logarithmic mapping where the spatial equivalence does not correspond to numeric equivalence; in other words, spatially half-way between two grid lines does not result in a numeric value half-way between the quantitative grid line labels. Figure 4.7: Estimated Population: Common responses In conclusion, Elementary Q1 and the first level population estimates extracted from participant calculations and scratch work indicate that accuracy for low magnitudes are more accurate with lower variance in those estimates on the logarithmic scale than on the linear scale. Accuracy of population estimates made on the linear scale improve as the magnitude of the population increases. The results also provided support that participants have a strong tendency to anchor their estimates to both grid lines and a base ten framework with resistance to estimating between grid lines on the logarithmic scale in particular, leading to a sacrifice in accuracy for larger magnitudes. Participant calculations and scratch work reveal a lack of understanding of logarithm as they equated spatial distance to numerical distance. 4.4.3 Elementary Q2: Estimation of time In addition to estimating the population from a given year, participants were asked, . This requires literal reading of the data by mapping a value given on the \\(y\\)-axis to its corresponding value on the \\(x\\)-axis. The true estimated year based on the underlying equation in which the population reaches exactly 4000 is 28.45. Unlike the previous question, there is no exact simulated point that aligns with the quantity to be estimated; the closest points for data set one occur at years 24 (population 3774.9) and 30 (population 5174.12) and for data set two at years 27 (population 3859.22) and 28 (population 4099.69). The median year estimated by participants for data set 1 was 24 on both scales with innerquartile ranges of 1 and 3 for the linear and log scale respectively; the median for data set two occured at 27 for both data sets with innerquartile ranges of 2 and 1 for the linear and log scale respectively. Figure 4.8: Elementary Q2 Sketch While a small portion of participants provided estimates of years 5, 10 and 15, the density plots in focus on reasonable participant estimates between years 20 and 35. A population of 4000 occurs around a medium magnitude and is thus distinguishable on the linear scale, making the estimated location more visible. Participant accuracy was consistent across both the linear and log scales with an larger variance for data set one when estimates were made on the log scale. One possible explanation for the difference in variation between data sets might be that some participants were first visually fitting a trend on on the log scale (results in a visually linear trend) while some participants were basing their estimates off the closest point (year 24). This is apparent by participants overestimation of the closest point, with some estimates drawing closer to the true value based on the underlying equation. On the logarithmic scale, participants were able to strongly anchor their estimates to the grid line break of 4096 and provide accurate year estimates by counting between grid lines on the \\(x\\)-axis with few participants making estimates between years (for example, 27.5). However, participants still had a tendency to anchor to a base ten framework as indicated by an increase in the density of estimates occurring at year 30. Figure 4.9: Elementary Q2 Density Results form Elementary Q2 provide support that participants accurately estimated the year in which the population reaches 4000 on both scales. The accuracy on the linear scale can be explained by the visibility of a medium magnitude along with participant ability to make accurate estimates between grid lines on a linear scale. The population given aligned closely with grid line 4096 on the logarithmic scale, allowing participants to strongly anchor to the grid line for their estimation. In particular, for data set one, participants were slightly more likely to base their estimates off the underlying trend line on the logarithmic scale than on the linear scale. Estimated years were often provided in whole numbers and few participants showed an understanding that the population of interest could occur between years. 4.4.4 Intermediate Q1: Additive increase in population Intermediate level questions require participants to read between the data and make comparisons between points. Participants were asked, . The questioning was selected carefully to prompt participants to make an additive comparison of populations between two years. In order to make this comparison, participants must make accurate first level estimates in both years and subtract the two estimates. Sample participant work below shows correct logic on both the linear and logarithmic scales: \\[\\begin{align} \\textit{Sample work 4: correct logic (linear)} \\nonumber\\\\ 15000 - 2500 &amp; = 12500\\nonumber\\\\ \\text{Scratchpad: } &amp;\\text{In 20 ABY the population of Ewoks was 2500,}\\nonumber\\\\ &amp;\\text{in 40 ABY the population was 15 000,}\\nonumber\\\\ &amp;\\text{i would make a substraction}\\nonumber \\\\ \\nonumber \\\\ \\textit{Sample work 5: correct logic (log)} \\nonumber\\\\ 2048 - 1024 &amp; = 1024 \\nonumber\\\\ 1024 - 512 &amp; = 512 \\nonumber\\\\ 1024 + 512 &amp; = 1536 \\nonumber\\\\ 16384 - 1536 &amp; = 14848 \\nonumber\\\\ \\text{Scratchpad: } &amp; \\text{20 aby 1536} \\nonumber\\\\ &amp; \\text{40 16384.} \\nonumber \\end{align}\\] The true estimated increase in population from year 20 to 40 based on the underlying equation is 14363.34 (15846.35 - 1483.01) with increases based on the closest points of 15517.75 (17046.94 - 1529.18) and 22897.45 (24186.34 - 1288.91) for data sets one and two respectively. The median estimated increase for data set one was 15000 (IQR = 3000) for the linear scale and 14784 (IQR = 2000) for the logarithmic scale while data set two resulted in larger estimates and variability with a median increase of 17500 (IQR = 10625) and 16500 (IQR = 8952) for the linear and logarithmic scale respectively. The discrepancy in the summary between the two data sets provides further support that participants are inspecting the simulated data points in order to make their estimates. Figure 4.10: Intermediate Q1 Sketch Figure 4.11: Intermediate Q1 Density (Data set 1) Figure 4.12: Intermediate Q1 Density (Data set 2) and display the density for estimated increases in population as made by participants for data set one and two respectively. There were a considerable amount of estimated increases near zero indicating that some participants were misinterpreting the value they were asked to estimate. Sample participant work below shows both common incorrect logic on both the linear and logarithmic scales: \\[\\begin{align} \\textit{Sample work 6: incorrect logic (linear)} \\nonumber\\\\ 24000/2000&amp;=12 \\nonumber\\\\ \\nonumber \\\\ \\textit{Sample work 7: incorrect logic (log)} \\nonumber\\\\ 16380/1026&amp;=15.96\\nonumber\\\\ \\nonumber \\\\ \\textit{Sample work 8: changed logic (log)} \\nonumber\\\\ 2048-1024&amp;=1024\\nonumber\\\\ 1024+512&amp;=1536\\nonumber\\\\ 16384-1536&amp;=14848\\nonumber\\\\ 14848/1536&amp;=9.67.\\nonumber \\end{align}\\] Sample work 5 shows how the participant first estimated halfway between the logarithmic grid lines and correctly subtracted the populations for the two given years before changing their logic to divide the two populations. One potential source of misinterpretation of this questions might be the particular order in which participants were asked the questions. For example, if participants were asked to provide an estimated increase in population after having been asked Intermediate Q2 which prompts participants to provide a multiplicative change in population, they may be more likely to misinterpret Intermediate Q1. However, participants answering questions on the second scenario would have seen both questioning frameworks in the previous context. Estimates for the increase in population between year 20 and year 40 is distinctly more accurate for estimates made on the linear scale as indicated by the peak density occurring near the closest point and true value vertical lines. The slight shifts in the density on the logarithmic scale suggest participants are . Variance in estimates appears to be consistent across both scales for data set two with a smaller variance on the logarithmic scale for data set one. One explanation might be that participants are anchoring to the grid lines much stronger on the logarithmic scale as opposed to being more likely to adjust their estimates between grid lines on the linear scale. Common responses on the logarithmic scale come from anchoring to grid lines (16384 - 1024 = 15360), halfway numerically between grid lines (16384 - 1536 = 14848; 24576 - 1536 = 2340), and base ten (16384 - 2000 = 14784) while participants on the linear scale anchor to multiples of 500 and 1000. This is dependent on the location of simulated points in relation to the grid lines and leads to an underestimation in difference for data set one and an overestimation in difference for data set two. Figure 4.13: Intermediate Q1 Common Responses Responses from Intermediate Q1 reqiured participants to use their first level estimates in order to make an additive comparison of populations between two years. Some participants misinterpretted the question, making a multiplicative comparison, thus providing estimates closer to zero. This is supported by examining select participant calculation and scratchpad work. The estimated increase in population was more accurate on the linear scale with the lack of accuracy on the logarithmic scale affected by participant resistance to and misunderstanding of making estimates between logrithmic grid lines. 4.4.5 Intermediate Q2: Multiplicative change in population Previously, we explored how participants made an additive comparison of populations between two years. In addition, participants were asked, . The questioning was selected carefully to prompt participants to make a multiplicative comparison between two years. Similar to Intermediate Q1, in order to make this comparison, participants must have made accurate first level estimates in both years and divide the two estimates. Participants may also have made this comparison on the logarithmic scale by understanding the multiplicative nature of the grid lines. Sample participant work below shows correct logic on both the linear and logarithmic scales: \\[\\begin{align} \\textit{Sample work 9: correct logic (linear)} \\nonumber\\\\ 17500/1400&amp;=12.5 \\nonumber\\\\ \\text{Scratchpad: } &amp; \\text{same as before, but a division} \\nonumber\\\\ \\nonumber \\\\ \\textit{Sample work 11: correct logic (linear)} \\nonumber\\\\ 17000-1000&amp;=16000 \\nonumber\\\\ 17000/1000&amp;=17 \\nonumber\\\\ \\nonumber \\\\ \\textit{Sample work 12: correct logic (log)} \\nonumber\\\\ 24/1.4&amp;=17.14 \\nonumber\\\\ \\text{Scratchpad: } &amp; \\text{around 24k tribbles were at 4540, and} \\nonumber\\\\ &amp; \\text{1.4k at 4520, make a division and thats}\\nonumber\\\\ &amp; \\text{how many times (without the k)}\\nonumber \\nonumber \\\\ \\textit{Sample work 13: correct logic (log)} \\nonumber\\\\ 2048*5&amp;=10240 \\nonumber\\\\ 2048*6&amp;=12288 \\nonumber\\\\ 2048*7&amp;=14336 \\nonumber\\\\ 2048*8&amp;=16384 \\nonumber\\\\ 2048*8&amp;=16384. \\nonumber \\end{align}\\] Figure 4.14: Intermediate Q2 Sketch The scratch work from participants gives insight about the process participants followed when determining the estimated change in population. For instance, Sample work 11 shows the participant first incorrectly calculated the additive increase in population before correcting their calculation through division while Sample work 13 shows how the participant used a trial and error method. The true change in population based on the underlying equation is 10.69 times as many (15846.35/1483.01) with changes based on the closest points of 11.1 (17046.94/1529.18) and 18.8 (24186.34/1288.91) for data sets one and two respectively. The median estimated change for data set one was 11.7 (IQR = 8.5) for the linear scale and 10.7 (IQR = 6) for the logarithmic scale while data set two resulted in larger estimates and variability with a median change of 15.3 (IQR = 14) and 16 (IQR = 8.5) for the linear and logarithmic scale respectively. The inconsistency between the two data sets aligns with previous evidence that participants are making estimates by reading the simulated data rather than based on the underlying trend. Figure 4.15: Intermediate Q2 Observed Plot As seen in the results for Intermediate Q1, some participants struggled to understand the value they were being asked to estimate. Similarly, illustrates a substantial number of participants provided estimates that more closely reflect that of the additive increase in population rather than the multiplicative change. highlights that 15000 was still a common participant response. Sample work 14 and 15 below demonstrate common calculations conducted by participants: \\[\\begin{align} \\textit{Sample work 14: incorrect logic (linear)} \\nonumber\\\\ 23800-1100&amp;=22700 \\nonumber\\\\ \\nonumber \\\\ \\textit{Sample work 15: incorrect logic (log)} \\nonumber\\\\ 16384-1536&amp;=14848. \\nonumber \\end{align}\\] Evaluating reasonable participant responses for the change in population between 0 times as many and 35 times as many, indicates participants tended to be make more accurate and less variable estimates on the logarithmic scale than on the linear scale. shows common responses provided by participants. Figure 4.16: Intermediate Q2 Density Figure 4.17: Intermediate Q2 Common Responses Overall, responses for Intermediate Q2 provide further support that participants tend to misinterpret the quantity they are being asked to estimate. The density plots of responses suggest that the logarithmic scale has a slight advantage over the linear scale for estimating the multiplicative change in population. While anchoring to grid lines and base ten values for first level estimates still occurred, many participants further anchored their responses to whole values. 4.4.6 Intermediate Q3: Time until population doubles An alternative multiplicative comparison between two points is to determine the amount of time it takes for a value to double. Participants were asked, . In order to accurately evaluate this comparison, participants must have made a first level estimate for the population in year 10, asked in Elementary Q1. Participants then must double their first level estimate in order to extract the year in which this value occurs and finally subtract year 10. Alternately, on the logarithmic scale, participants could have made this comparison without actually extracting the numeric values and instead relied on their spatial distance equating one increase in grid lines to a double in population. This requires keen understanding of the logarithmic base two scale. Making a judgement based on participant calculations and scratch work, most participants selected the former approach when they estimated the time it took for the population to double. One participant stated, 4510 has 512 Tribble Population and to double it it needs to have 1,024 so it would take approximately 5 years. In 4515 they would have double the population. while another participant indicated 10Aby near to 460.8 so double is 921.6; aprox. 5 years. Figure 4.18: Intermediate Q3 Sketch Based on the true underlying equation, the population in year 10 (481.62) doubles to 963.24 in year 16.25, thus it takes 6.25 years for the population to double. Closest simulated points result in the population doubling in 4 years (445 x 2 = 890.97 in 14) for data set one and 6 years (466.90 x 2 = 933.79 in 16) for data set two. The median participant response for data set one was 5 (IQR = 5) and 5 (IQR = 2) for the linear and log scale respectively with a median for data set two of 8 (IQR = 6) and 6 (IQR = 2) for the linear and log scale respectively. illustrates the estimated number of years until the population in year 10 doubles. While there appears to be similar accuracy across both scales, the variance in estimates is considerably smaller for the log scale. The large variance in the linear scale may be explained by the location of reference year 10 which results in a population of low magnitude and is visually difficult to estimate. As indicated by peaks in density, there is strong anchoring which occurs at multiples of five. Figure 4.19: Intermediate Q3 Density In summary, as indicated by participant scratch work, they tended to make a first level estimates for the reference year 10 rather than visually judging the distance between grid lines on the log scale. The estimated time until the population doubles from a reference year of 10 results in lower variability on the log scale as opposed to a larger variability on the linear scale. One explanation might be the low magnitude of population in year ten and further exploration would be needed to justify for other reference years. Common responses strongly suggest participants are anchoring to multiples of 5 years. 4.5 Discussion and Conclusion This study was intended to help inform and aid in understanding the cognitive implications of displaying exponentially increasing data on a log scale. I evaluated graph comprehension as it relates to two contextual scenarios by asking participants a series of six questions (one open ended, two elementary level, and three intermediate) which required them to quantitatively transform information in the chart. Results provided an understanding of the advantages and disadvantages of the log scale. In general, results suggest that understanding logarithmic logic is difficult as indicated by the misunderstanding in Intermediate Q1 and Q2. This is also supported in following participants estimation strategy for making first level estimates in Intermediate Q3 rather than relying on their spatial awareness between grid lines. The accuracy of estimates greatly depends on the location of the value being estimated in relation to the magnitude. For example, accuracy and variability of population estimates made on the linear scale improved as the year of interest increased and thus the magnitude of population increased, making the point more visible. Alternatively, there was a slight sacrifice in the accuracy of population estimates on the log scale as the year of interest increased. This is due to participant resistance to estimate between grid lines on the log scale and inaccurate representation of equating spatial distance to quantitative difference. As the magnitude of population increases, there is a more noticeable effect of the resistance and lack of understanding. Density plots showed an advantage of the linear scale on when estimating an additive increase in population and a slight advantage of the log scale when estimated a multiplicative change in population. It was also found that estimates were subjective to the simulated data set as shown by the discrepancy between data set one and data set two. This implies a large portion of participants were reading the actual simulated data points as opposed to basing estimates on the underlying visual trend of the data. Common responses revealed participants bias to anchoring their estimates to the grid lines, particularly on the log scale, as well as to base ten values. The strong tendency to anchor to grid lines on the log scale resulted in a sacrifice in accuracy as quantitative differences between grid lines increased. This also implies that accuracy strongly depends on the location of the simulated point in relation to the grid lines. Understanding graph comprehension is a complex process and requires long-term interaction with the chart and information being presented. With a fixed number of estimates and assessment of participant scratch work, I outlined a variety of situations in which displaying exponentially increasing data on a log scale results in both advantages and disadvantages, thus providing a better understanding of the cognitive implications of the use of log scales. "],["5-conclusion.html", "CHAPTER 5 Conclusion", " CHAPTER 5 Conclusion If you feel it necessary to include an appendix, it goes here. --> "],["A-youdrawit-with-shiny.html", "A You Draw It Setup with Shiny", " A You Draw It Setup with Shiny Interactive plots for the you draw it study were created using the r2d3 package and integrating D3 source code with an R shiny application. provides a visual aid of the process of creating the you draw it experimental study in Chapter 3. Figure A.1: Interactive plot development "],["B-exponential-prediction-plots.html", "B Exponential Prediction Interactive Plots", " B Exponential Prediction Interactive Plots The figures below illustrate the 8 interactive plots used to test exponential prediction. Two data sets were simulated with low and high exponential growth rates and shown four times each by truncating the points shown at both 50% and 75% of the domain as well as on both the log and linear scales following a 2 x 2 x 2 factorial treatment design. Figure B.1: Exponential Prediction: low growth rate, points truncated at 50%, linear scale Figure B.2: Exponential Prediction: low growth rate, points truncated at 50%, log scale Figure B.3: Exponential Prediction: low growth rate, points truncated at 75%, linear scale Figure B.4: Exponential Prediction: low growth rate, points truncated at 75%, log scale Figure B.5: Exponential Prediction: high growth rate, points truncated at 50%, linear scale Figure B.6: Exponential Prediction: high growth rate, points truncated at 50%, log scale Figure B.7: Exponential Prediction: high growth rate, points truncated at 75%, linear scale Figure B.8: Exponential Prediction: high growth rate, points truncated at 75%, log scale "],["C-estimation-1.html", "C Estimation C.1 Scratchwork participant comparison", " C Estimation C.1 Scratchwork participant comparison Data set Scale Showed work N dataset1 linear no 73 dataset1 linear yes 72 dataset1 log2 no 79 dataset1 log2 yes 78 dataset2 linear no 79 dataset2 linear yes 78 dataset2 log2 no 73 dataset2 log2 yes 72 Warning: Removed 13 rows containing non-finite values (stat_density). "],["references.html", "References", " References Aisch, G., Cohn, N., Cox, A., Katz, J., Pearce, A., &amp; Quealy, K. (2016). Live presidential forecast. The New York Times. The New York Times. Retrieved from https://www.nytimes.com/elections/2016/forecast/president Aisch, G., Cox, A., &amp; Quealy, K. (2015, May). You draw it: How family income predicts childrens college chances. The New York Times. The New York Times. Retrieved from https://www.nytimes.com/interactive/2015/05/28/upshot/you-draw-it-how-family-income-affects-childrens-college-chances.html Allison Horst. Artwork by allison horst. Creative Commons. Retrieved from https://github.com/allisonhorst/stats-illustrations Amer, T. (2005). Bias due to visual illusion in the graphical presentation of accounting information. Journal of Information Systems, 19(1), 118. Barry, D., Buchanan, L., Cargill, C., Daniel, A., Delaquérière, A., Gamio, L.,  al., et. (2020, May). Remembering the 100,000 lives lost to coronavirus in america. The New York Times. The New York Times. Retrieved from https://www.nytimes.com/interactive/2020/05/24/us/us-coronavirus-deaths-100000.html Bates, D., Mächler, M., Bolker, B., &amp; Walker, S. (2015). Fitting linear mixed-effects models using lme4. Journal of Statistical Software, 67(1), 148. http://doi.org/10.18637/jss.v067.i01 Bavel, J. J. V., Baicker, K., Boggio, P. S., Capraro, V., Cichocka, A., Cikara, M.,  Willer, R. (2020). Using social and behavioural science to support COVID-19 pandemic response. Nature Human Behaviour, 4(5), 460471. http://doi.org/10.1038/s41562-020-0884-z Becker, G., Moore, S. E., &amp; Lawrence, M. (2019). Trackr: A framework for enhancing discoverability and reproducibility of data visualizations and other artifacts in r. Journal of Computational and Graphical Statistics, 28(3), 644658. Best, L. A., Smith, L. D., &amp; Stubbs, D. A. (2007). Perception of Linear and Nonlinear Trends: Using Slope and Curvature Information to Make Trend Discriminations. Perceptual and Motor Skills, 104(3), 707721. http://doi.org/10.2466/pms.104.3.707-721 Broersma, H., &amp; Molenaar, I. (1985). Graphical perception of distributional aspects of data. Computational Statistics Quarterly, 2(1), 5372. Buchanan, L., Park, H., &amp; Pearce, A. (2017, January). You draw it: What got better or worse during obamas presidency. The New York Times. The New York Times. Retrieved from https://www.nytimes.com/interactive/2017/01/15/us/politics/you-draw-obama-legacy.html Buja, A., Cook, D., Hofmann, H., Lawrence, M., Lee, E.-K., Swayne, D. F., &amp; Wickham, H. (2009a). Statistical inference for exploratory data analysis and model diagnostics. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 367(1906), 43614383. http://doi.org/10.1098/rsta.2009.0120 Buja, A., Cook, D., Hofmann, H., Lawrence, M., Lee, E., Swayne, D. F., &amp; Wickham, H. (2009b). Statistical inference for exploratory data analysis and model diagnostics. Royal Society Philosophical Transactions A, 367(1906), 43614383. Retrieved from http://rsta.royalsocietypublishing.org/content/367/1906/4361.article-info Burn-Murdoch, J., Nevitt, C., Tilford, C., Rininsland, A., Kao, J. S., Elliott, O.,  Stabe, M. (2020). Coronavirus tracked: Has the epidemic peaked near you? Coronavirus chart: see how your country compares. Financial Times. Retrieved from https://ig.ft.com/coronavirus-chart/?areas=eur Carlson, N. R. (2010). Psychology: The science of behaviour. Pearson Education. Chandar, N., Collier, D., &amp; Miranti, P. (2012). Graph standardization and management accounting at AT&amp;t during the 1920s. Accounting History, 17(1), 3562. Cleveland, W. S., &amp; McGill, R. (1984). Graphical Perception: Theory, Experimentation, and Application to the Development of Graphical Methods. Retrieved from http://euclid.psych.yorku.ca/www/psy6135/papers/ClevelandMcGill1984.pdf Cleveland, W. S., &amp; McGill, R. (1985). Graphical Perception and Graphical Methods for Analyzing Scientific Data. Science, New Series, 229(4716), 828833. Retrieved from http://www.jstor.org/stable/1695272 Croxton, F. E., &amp; Stein, H. (1932). Graphic comparisons by bars, squares, circles, and cubes. Journal of the American Statistical Association, 27(177), 5460. Croxton, F. E., &amp; Stryker, R. E. (1927). Bar charts versus circle diagrams. Journal of the American Statistical Association, 22(160), 473482. Curcio, F. R. (1987). Comprehension of mathematical relationships expressed in graphs. Journal for Research in Mathematics Education, 18(5), 382393. Dehaene, S., Izard, V., Spelke, E., &amp; Pica, P. (2008). Log or linear? Distinct intuitions of the number scale in western and amazonian indigene cultures. Science, 320(5880), 12171220. Dunn, R. (1988). Framed rectangle charts or statistical maps with shading: An experiment in graphical perception. The American Statistician, 42(2), 123129. Eells, W. C. (1926). The relative merits of circles and bars for representing component parts. Journal of the American Statistical Association, 21(154), 119132. Fagen-Ulmschneider, W. (2020). 91-DIVOC. An interactive visualization of COVID-19. University of Illinois. Retrieved from https://91-divoc.com/pages/covid-visualization/ Fechner, G. T. (1860). Elemente der psychophysik (Vol. 2). Breitkopf u. Härtel. Fellows, I. (2018). Wordcloud: Word clouds. Retrieved from https://CRAN.R-project.org/package=wordcloud Finney, D. J. (1951). Subjective Judgment in Statistical Analysis: An Experimental Study. Journal of the Royal Statistical Society. Series B (Methodological), 13(2), 284297. Retrieved from https://www.jstor.org/stable/2984070 Finney, David John, &amp; Stevens, W. (1948). A table for the calculation of working probits and weights in probit analysis. Biometrika, 35(1/2), 191201. Friel, S. N., Curcio, F. R., &amp; Bright, G. W. (2001). Making sense of graphs: Critical factors influencing comprehension and instructional implications. Journal for Research in Mathematics Education, 32(2), 124158. Furmanski, C. S., &amp; Engel, S. A. (2000). An oblique effect in human primary visual cortex. Nature Neuroscience, 3(6), 535536. Giesbrecht, F. G., &amp; Gumpertz, M. L. (2004). Planning, construction, and statistical analysis of comparative experiments (Vol. 405, pp. 118157). John Wiley &amp; Sons. Glazer, N. (2011). Challenges with graph interpretation: A review of the literature. Studies in Science Education, 47(2), 183210. Goldstein, E. B., &amp; Brockmole, J. R. (2017). Sensation and Perception, 480. Gordon, I., &amp; Finch, S. (2015). Statistician Heal Thyself: Have We Lost the Plot? Journal of Computational and Graphical Statistics, 24(4), 12101229. http://doi.org/10.1080/10618600.2014.989324 Gouretski, V., &amp; Koltermann, K. P. (2007). How much is the ocean really warming? Geophysical Research Letters, 34(1). Graesser, A. C., Swamer, S. S., Baggett, W. B., &amp; Sell, M. A. (2014). New models of deep comprehension. In Models of understanding text (pp. 940). Psychology Press. Green, T. M., &amp; Fisher, B. (2009). The personal equation of complex individual cognition during visual interface interaction. In Workshop on human-computer interaction and visualization (pp. 3857). Springer. Haemer, K. W., &amp; Kelley, T. L. (1949). Presentation Problems: Suiting the Chart to the Audience: Common Graphic Devices Classified According to Ease of Reading. The American Statistician, 3(5), 1111. http://doi.org/10.1080/00031305.1949.10501608 Harms, H. (1991). August friedrich wilhelm crome (1753-1833) autor begehrter wirtschaftskarten. Cartographica Helvetica, 3, 3338. Heckler, A. F., Mikula, B., &amp; Rosenblatt, R. (2013). Student accuracy in reading logarithmic plots: The problem and how to fix it. In 2013 IEEE Frontiers in Education Conference (FIE) (pp. 10661071). http://doi.org/10.1109/FIE.2013.6684990 Hofmann, H., Follett, L., Majumder, M., &amp; Cook, D. (2012). Graphical Tests for Power Comparison of Competing Designs. IEEE Transactions on Visualization and Computer Graphics, 18(12), 24412448. http://doi.org/10.1109/TVCG.2012.230 Jerne, N. K., &amp; Wood, E. C. (1949). The validity and meaning of the results of biological assays. Biometrics, 5(4), 273299. Jolliffe, F. R. (1991). Assessment of the understanding of statistical concepts. In Proceedings of the third international conference on teaching statistics (Vol. 1, pp. 461466). Jones, G. V. (1977). Polynomial perception of exponential growth. Perception &amp; Psychophysics, 21(2), 197198. http://doi.org/10.3758/BF03198726 Katz, J. (2017, April). You draw it: Just how bad is the drug overdose epidemic? The New York Times. The New York Times. Retrieved from https://www.nytimes.com/interactive/2017/04/14/upshot/drug-overdose-epidemic-you-draw-it.html Kruskal, W. (1975). Visions of maps and graphs. In Proceedings of the international symposium on computer-assisted cartography, auto-carto II (pp. 2736). Citeseer. Lenth, R. V. (2021). Emmeans: Estimated marginal means, aka least-squares means. Retrieved from https://CRAN.R-project.org/package=emmeans Lewandowsky, S., &amp; Spence, I. (1989). The Perception of Statistical Graphs. Sociological Methods &amp; Research, 18(2-3), 200242. http://doi.org/10.1177/0049124189018002002 Loy, A., Follett, L., &amp; Hofmann, H. (2016). Variations of q  q Plots: The Power of Our Eyes! The American Statistician, 70(2), 202214. http://doi.org/10.1080/00031305.2015.1077728 Loy, A., Hofmann, H., &amp; Cook, D. (2017). Model Choice and Diagnostics for Linear Mixed-Effects Models Using Statistics on Street Corners. Journal of Computational and Graphical Statistics, 26(3), 478492. http://doi.org/10.1080/10618600.2017.1330207 Luraschi, J., &amp; Allaire, J. (2018). r2d3: Interface to D3 visualizations. Retrieved from https://CRAN.R-project.org/package=r2d3 Macdonald-Ross, M. (1977). How numbers are shown. AV Communication Review, 25(4), 359409. Mackinnon, A. J., &amp; Wearing, A. J. (1991). Feedback and the forecasting of exponential change. Acta Psychologica, 76(2), 177191. http://doi.org/10.1016/0001-6918(91)90045-2 Majumder, M., Hofmann, H., &amp; Cook, D. (2013). Validation of Visual Statistical Inference, Applied to Linear Models. Journal of the American Statistical Association, 108(503), 942956. http://doi.org/10.1080/01621459.2013.808157 Marquand, R. (Director). (1983). Star wars: Episode VI - return of the jedi. 20th Century Fox: Lucasfilm Ltd. Menge, D. N. L., MacPherson, A. C., Bytnerowicz, T. A., Quebbeman, A. W., Schwartz, N. B., Taylor, B. N., &amp; Wolf, A. A. (2018). Logarithmic scales in ecological data presentation may cause misinterpretation. Nature Ecology &amp; Evolution, 2(9), 13931402. http://doi.org/10.1038/s41559-018-0610-7 Mosteller, F., Siegel, A. F., Trapido, E., &amp; Youtz, C. (1981). Eye fitting straight lines. The American Statistician, 35(3), 150152. Munroe, R. (2005). Log scale. xkcd: A webcomic of romance, sarcasm, math, and language. Creative Commons. Retrieved from https://xkcd.com/1162/ Myers, D. G., &amp; DeWall, C. N. (2021). Psychology. Worth Publishers. Nieder, A., &amp; Miller, E. K. (2003). Coding of cognitive magnitude: Compressed scaling of numerical information in the primate prefrontal cortex. Neuron, 37(1), 149157. Perry, P. O. (2021). Corpus: Text corpus analysis. Retrieved from https://CRAN.R-project.org/package=corpus Peterson, L. V., &amp; Schramm, W. (1954). How accurately are different kinds of graphs read? Audio Visual Communication Review, 178189. Peterson, M. A. (1994). Object recognition processes can and do operate before figureground organization. Current Directions in Psychological Science, 3(4), 105111. Playfair, W. (1801). The statistical breviary; shewing, on a principle entirely new, the resources of every state and kingdom in europe, wallis, londres. Press, Chicago. Risk levels. (2021, March). Global Epidemics. Brown School of Public Health. Retrieved from https://globalepidemics.org/key-metrics-for-covid-suppression/ Romano, A., Sotis, C., Dominioni, G., &amp; Guidi, S. (2020). The Scale of COVID-19 Graphs Affects Understanding, Attitudes, and Policy Preferences ({SSRN} {Scholarly} {Paper} No. ID 3588511). Rochester, NY: Social Science Research Network. Retrieved from https://papers.ssrn.com/abstract=3588511 Rost, L. C. (2020, December). Youve informed the public with visualizations about the coronavirus. Thank you. Datawrapper Blog. Retrieved from https://blog.datawrapper.de/coronavirus-data-visualization-effect-datawrapper/ Ryanabest. (2021, June). Build an NBA contender with our roster-shuffling machine. FiveThirtyEight. Retrieved from https://projects.fivethirtyeight.com/nba-trades-2021/ Shah, P., &amp; Carpenter, P. A. (1995). Conceptual limitations in comprehending line graphs. Journal of Experimental Psychology: General, 124(1), 43. Shah, P., Mayer, R. E., &amp; Hegarty, M. (1999). Graphs as aids to knowledge construction: Signaling techniques for guiding the process of graph comprehension. Journal of Educational Psychology, 91(4), 690. Siegler, R. S., &amp; Braithwaite, D. W. (2017). Numerical Development. Annual Review of Psychology, 68(1), 187213. http://doi.org/10.1146/annurev-psych-010416-044101 Silge, J., &amp; Robinson, D. (2016). Tidytext: Text mining and analysis using tidy data principles in r. JOSS, 1(3). http://doi.org/10.21105/joss.00037 Silver, N. (2020, November). 2020 election forecast. FiveThirtyEight. Retrieved from https://projects.fivethirtyeight.com/2020-election-forecast/ Spence, I. (1990). Visual psychophysics of simple graphical elements. Journal of Experimental Psychology: Human Perception and Performance, 16(4), 683692. http://doi.org/10.1037/0096-1523.16.4.683 Star trek: The original series. \"The trouble with tribbles\". Season 2. Episode 15. (1967). Broadcast on NBC. Star wars: Episode IV  a new hope. (1977). 20th Century Fox. Sun, J. Z., Wang, G. I., Goyal, V. K., &amp; Varshney, L. R. (2012). A framework for Bayesian optimality of psychophysical laws. Journal of Mathematical Psychology, 56(6), 495501. http://doi.org/10.1016/j.jmp.2012.08.002 Tan, J. K. (1994). Human processing of two-dimensional graphics: Information-volume concepts and effects in graph-task fit anchoring frameworks. International Journal of Human-Computer Interaction, 6(4), 414456. Teghtsoonian, M. (1965). The judgment of size. The American Journal of Psychology, 78(3), 392402. Tol. (2021). Bitmap VS SVG. Retrieved from https://commons.wikimedia.org/wiki/File:Bitmap_VS_SVG.svg Unwin, A. (2020). Why is Data Visualization Important? What is Important in Data Visualization? Harvard Data Science Review. http://doi.org/10.1162/99608f92.8ae4d525 Vanderplas, S., Cook, D., &amp; Hofmann, H. (2020). Testing Statistical Charts: What Makes a Good Graph? Annual Review of Statistics and Its Application, 7(1), 6188. http://doi.org/10.1146/annurev-statistics-031219-041252 VanderPlas, S., &amp; Hofmann, H. (2015). Spatial reasoning and data displays. IEEE Transactions on Visualization and Computer Graphics, 22(1), 459468. VanderPlas, S., &amp; Hofmann, H. (2017). Clusters Beat Trend!? Testing Feature Hierarchy in Statistical Graphics. Journal of Computational and Graphical Statistics, 26(2), 231242. http://doi.org/10.1080/10618600.2016.1209116 Varshney, L. R., &amp; Sun, J. Z. (2013). Why do we perceive logarithmically? Significance, 10(1), 2831. http://doi.org/10.1111/j.1740-9713.2013.00636.x von Bergmann, J. (2021, March). Xkcd_exponential: Public health vs scientists. mountainMath. GitHub. Retrieved from https://github.com/mountainMath/xkcd_exponential Waddell, W. J. (2005). Comparisons of thresholds for carcinogenicity on linear and logarithmic dosage scales. Human &amp; Experimental Toxicology, 24(6), 325332. Wagenaar, W. A., &amp; Sagaria, S. D. (1975). Misperception of exponential growth. Perception &amp; Psychophysics, 18(6), 416422. http://doi.org/10.3758/BF03204114 Walker, F. A.others. (2013). Statistical atlas of the united states based on the results of the ninth census 1870 with contributions from many eminent men of science and several departments of the government. Wickham, Hadley. (2011). ggplot2. Wiley Interdisciplinary Reviews: Computational Statistics, 3(2), 180185. Wickham, Hadley. (2016). ggplot2: Elegant graphics for data analysis. Springer-Verlag New York. Retrieved from https://ggplot2.tidyverse.org Wickham, H., Cook, D., Hofmann, H., &amp; Buja, A. (2010). Graphical inference for infovis. IEEE Transactions on Visualization and Computer Graphics, 16(6), 973979. http://doi.org/10.1109/TVCG.2010.161 Wickham, Hadley, &amp; Grolemund, G. (2016). R for data science: Import, tidy, transform, visualize, and model data. \" OReilly Media, Inc.\". Wilkinson, L. (2012). The grammar of graphics. In Handbook of computational statistics (pp. 375414). Springer. Wood, R. (1968). Objectives in the teaching of mathematics. Educational Research, 10(2), 8398. Wood, S. N. (2003). Thin-plate regression splines. Journal of the Royal Statistical Society (B), 65(1), 95114. Wood, S. N. (2004). Stable and efficient multiple smoothing parameter estimation for generalized additive models. Journal of the American Statistical Association, 99(467), 673686. Wood, S. N. (2011). Fast stable restricted maximum likelihood and marginal likelihood estimation of semiparametric generalized linear models. Journal of the Royal Statistical Society (B), 73(1), 336. Wood, S. N. (2017). Generalized additive models: An introduction with r (2nd ed.). Chapman; Hall/CRC. Wood, S. N., N., Pya, &amp; Saefken, B. (2016). Smoothing parameter and model selection for general smooth models (with discussion). Journal of the American Statistical Association, 111, 15481575. Yates, J. (1985). Graphs as a managerial tool: A case study of du ponts use of graphs in the early twentieth century. The Journal of Business Communication (1973), 22(1), 533. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
